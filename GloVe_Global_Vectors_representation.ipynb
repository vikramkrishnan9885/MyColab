{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe_Global Vectors representation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNAznLFpIiVm5n9FyF3Lev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikramkrishnan9885/MyColab/blob/master/GloVe_Global_Vectors_representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhG-7SULuv3_",
        "colab_type": "text"
      },
      "source": [
        "Methods for learning word vectors fall into either of two categories: \n",
        "* global matrix factorization-based methods or \n",
        "* local context window-based methods. \n",
        "\n",
        "Latent Semantic Analysis (LSA) is an example of a global matrix factorization-based method, and skip-gram and CBOW are local context window-based methods.\n",
        "\n",
        "LSA is used as a document analysis technique that maps words in the documents to something known as a concept, a common pattern of words that appears in a document. \n",
        "\n",
        "Global matrix factorization-based methods efficiently exploit the global statistics of a corpus (for example, co-occurrence of words in a global scope), but have shown to perform poorly at word analogy tasks. \n",
        "\n",
        "On the other hand, context window-based methods have been shown to perform well at word analogy tasks, but do not utilize global statistics of the corpus, leaving space for improvement. \n",
        "\n",
        "GloVe attempts to get the best of both worlds—an approach that efficiently leverages global corpus statistics while optimizing the learning model in a context window-based manner similar to skip-gram or CBOW.\n",
        "\n",
        "The basic idea behind GloVe. To do so, let's consider an example:\n",
        "1. Consider word i = \"dog\" and j = \"cat\"\n",
        "2. Define an arbitrary probe word k\n",
        "3. Define $P_{ik}$ to be the probability of words i and k occurring close to each other, and $P_{jk}$ to be the words j and k occurring together\n",
        "\n",
        "Now let's look at how the  $\\frac{P_{ik}}{P_{jk}}$ entity behaves with different values for k.\n",
        "\n",
        "For k =\"bark\"  , it is highly likely to appear with i,thus, $P_{ik}$ will be high. However, k would not often appear along with j causing a low $P_{jk}$ . Therefore, we get the following expression:\n",
        "\n",
        "$$\\frac{P_{ik}}{P_{jk}} >> 1$$\n",
        "\n",
        "Next, for k = \"purr\"  , it is unlikely to appear in the close proximity of i and therefore will have a low $P_{ik}$ ; however, since $k$ highly correlates with $j$, the value of $P_{jk}$ will be high. This leads to the following:\n",
        "\n",
        "$$\\frac{P_{ik}}{P_{jk}} \\sim 0$$ \n",
        "\n",
        "Now, for words such as k = \"pet\" , which has a strong relationship with both\n",
        "i and j, or k = \"politics\"  , where i and j, both have a minimal relevance to,\n",
        "we get this:\n",
        "\n",
        "$$\\frac{P_{ik}}{P_{jk}} \\sim 1$$ \n",
        "\n",
        "It can be seen that the  $\\frac{P_{ik}}{P_{jk}}$ entity, which is calculated by measuring the frequency of two words appearing close to each other, is a good means for measuring the relationship between words. As a result, it becomes a good candidate for learning word vectors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k3CfiKvBtvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "f1ee28bf-01ea-425c-e87d-950e33636d66"
      },
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import bz2\n",
        "from matplotlib import pylab\n",
        "import urllib\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.sparse import lil_matrix\n",
        "import nltk # standard preprocessing\n",
        "import operator # sorting items in dictionary by value\n",
        "nltk.download('punkt') #tokenizers/punkt/PY3/english.pickle\n",
        "from math import ceil"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBpfOLOiF4GB",
        "colab_type": "text"
      },
      "source": [
        "DRB-HIPCSOM-WP\n",
        "* __D__OWNLOAD DATA\n",
        "* __R__EAD DATA WITH RELEVANT PRE-PROC\n",
        "* BUILD __B__ATCHES\n",
        "* ___CO-OCCURENCE MATRIX___: This is a new step\n",
        "* DEFINE __H__YPERPARAMETERS\n",
        "* DEFINE __I__NPUTS AND OUTPUTS\n",
        "* DEFINE MODEL __P__ARAMETERS\n",
        "* DEFINE MODEL __C__OMPUTATIONS\n",
        "* CALCULATE WORD __S__IMILARITIES\n",
        "* DEFINE __O__PTIMIZER\n",
        "* RUN __M__ODEL\n",
        "* FIND __W__ORDS CLUSTERED TOGETHER\n",
        "* COMPUTE T-SNE AND __P__LOT T-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JsG4mUqFonG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "18c18c19-7a03-4db4-8f5d-38310cb90c10"
      },
      "source": [
        "# DOWNLOAD DATA\n",
        "\n",
        "def download_data(url, filename):\n",
        "  if not os.path.exists(filename):\n",
        "    print('Downloading file:','\\t',url)\n",
        "    filename, _ = urllib.request.urlretrieve(url,filename)\n",
        "  else:\n",
        "    raise Exception(\"FILE ALREADY EXISTS!\")\n",
        "  return filename\n",
        "\n",
        "url = 'https://github.com/amolnayak311/nlp-with-tensorflow/blob/master/wikipedia2text-extracted.txt.bz2?raw=true'\n",
        "filename = 'wikipedia2text-extracted.txt.bz2'\n",
        "filename = download_data(url,filename)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading file: \t https://github.com/amolnayak311/nlp-with-tensorflow/blob/master/wikipedia2text-extracted.txt.bz2?raw=true\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EggI5KH0Gang",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "101d297a-16a3-420e-a1bd-6833d1afa1e7"
      },
      "source": [
        "# READ DATA WITH PREPROCESSING WITH NLTK\n",
        "#\n",
        "# Reads data as it is to a string, \n",
        "# convert to lower-case and \n",
        "# tokenize it using the nltk library. \n",
        "#\n",
        "# This code reads data in 1MB portions as processing the full text at once \n",
        "# slows down the task and returns a list of words. \n",
        "# You will have to download the necessary tokenizer.\n",
        "\n",
        "def read_data(filename):\n",
        "  \"\"\"\n",
        "  Extract the first file enclosed in a zip file as a list of words\n",
        "  and pre-processes it using the nltk python library\n",
        "  \"\"\"\n",
        "\n",
        "  with bz2.BZ2File(filename) as f:\n",
        "\n",
        "    data = []\n",
        "\n",
        "    file_size = os.stat(filename).st_size\n",
        "    # reading 1 MB at a time as the dataset is moderately large\n",
        "    chunk_size = 1024 * 1024 \n",
        "    print('Reading data...')\n",
        "    \n",
        "    for i in range(math.ceil(file_size//chunk_size)+1):\n",
        "      bytes_to_read = min(chunk_size,file_size-(i*chunk_size))\n",
        "      file_string = f.read(bytes_to_read).decode('utf-8')\n",
        "      file_string = file_string.lower()\n",
        "      \n",
        "      # tokenizes a string to words residing in a list\n",
        "      file_string = nltk.word_tokenize(file_string)\n",
        "      data.extend(file_string)\n",
        "  return data\n",
        "\n",
        "\n",
        "words = read_data(filename)\n",
        "print('Data size %d' % len(words))\n",
        "print('Example words (start): ',words[:10])\n",
        "print('Example words (end): ',words[-10:])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data...\n",
            "Data size 3360286\n",
            "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
            "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxoebzPdGt-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "131b34a1-d0bf-4fa2-e503-97590dc39412"
      },
      "source": [
        "# BUILD DATASET\n",
        "\n",
        "# we restrict our vocabulary size to 50000\n",
        "vocabulary_size = 50000 \n",
        "\n",
        "def build_dataset(words):\n",
        "  count = [['UNK', -1]]\n",
        "  # Gets only the vocabulary_size most common words as the vocabulary\n",
        "  # All the other words will be replaced with UNK token\n",
        "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
        "  dictionary = dict()\n",
        "\n",
        "  # Create an ID for each word by giving the current length of the dictionary\n",
        "  # And adding that item to the dictionary\n",
        "  for word, _ in count:\n",
        "    dictionary[word] = len(dictionary)\n",
        "    \n",
        "  data = list()\n",
        "  unk_count = 0\n",
        "  # Traverse through all the text we have and produce a list\n",
        "  # where each element corresponds to the ID of the word found at that index\n",
        "  for word in words:\n",
        "    # If word is in the dictionary use the word ID,\n",
        "    # else use the ID of the special token \"UNK\"\n",
        "    if word in dictionary:\n",
        "      index = dictionary[word]\n",
        "    else:\n",
        "      index = 0  # dictionary['UNK']\n",
        "      unk_count = unk_count + 1\n",
        "    data.append(index)\n",
        "    \n",
        "  # update the count variable with the number of UNK occurences\n",
        "  count[0][1] = unk_count\n",
        "  \n",
        "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
        "  # Make sure the dictionary is of size of the vocabulary\n",
        "  assert len(dictionary) == vocabulary_size\n",
        "    \n",
        "  return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Sample data', data[:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common words (+UNK) [['UNK', 69215], ('the', 226881), (',', 184013), ('.', 120944), ('of', 116323)]\n",
            "Sample data [1721, 9, 8, 16471, 223, 4, 5165, 4456, 26, 11590]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR4UxG5_G1dP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "272e241b-75f9-4d3b-c41a-da48e84eee38"
      },
      "source": [
        "# GENERATE BATCHES\n",
        "\n",
        "data_index = 0\n",
        "\n",
        "def generate_batch(batch_size, window_size):\n",
        "  # data_index is updated by 1 everytime we read a data point\n",
        "  global data_index \n",
        "    \n",
        "  # two numpy arras to hold target words (batch)\n",
        "  # and context words (labels)\n",
        "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "  weights = np.ndarray(shape=(batch_size), dtype=np.float32)\n",
        "\n",
        "  # span defines the total window size, where\n",
        "  # data we consider at an instance looks as follows. \n",
        "  # [ skip_window target skip_window ]\n",
        "  span = 2 * window_size + 1 \n",
        "    \n",
        "  # The buffer holds the data contained within the span\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "  \n",
        "  # Fill the buffer and update the data_index\n",
        "  for _ in range(span):\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % len(data)\n",
        "  \n",
        "  # This is the number of context words we sample for a single target word\n",
        "  num_samples = 2*window_size \n",
        "\n",
        "  # We break the batch reading into two for loops\n",
        "  # The inner for loop fills in the batch and labels with \n",
        "  # num_samples data points using data contained withing the span\n",
        "  # The outper for loop repeat this for batch_size//num_samples times\n",
        "  # to produce a full batch\n",
        "  for i in range(batch_size // num_samples):\n",
        "    k=0\n",
        "    # avoid the target word itself as a prediction\n",
        "    # fill in batch and label numpy arrays\n",
        "    for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)):\n",
        "      batch[i * num_samples + k] = buffer[window_size]\n",
        "      labels[i * num_samples + k, 0] = buffer[j]\n",
        "      weights[i * num_samples + k] = abs(1.0/(j - window_size))\n",
        "      k += 1 \n",
        "    \n",
        "    # Everytime we read num_samples data points,\n",
        "    # we have created the maximum number of datapoints possible\n",
        "    # withing a single span, so we need to move the span by 1\n",
        "    # to create a fresh new span\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % len(data)\n",
        "  return batch, labels, weights\n",
        "\n",
        "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
        "\n",
        "for window_size in [2, 4]:\n",
        "    data_index = 0\n",
        "    batch, labels, weights = generate_batch(batch_size=8, window_size=window_size)\n",
        "    print('\\nwith window_size = %d:' %window_size)\n",
        "    print('\\t','batch:', [reverse_dictionary[bi] for bi in batch])\n",
        "    print('\\t','labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n",
        "    print('\\t','weights:', [w for w in weights])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed']\n",
            "\n",
            "with window_size = 2:\n",
            "\t batch: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted']\n",
            "\t labels: ['propaganda', 'is', 'concerted', 'set', 'is', 'a', 'set', 'of']\n",
            "\t weights: [0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5]\n",
            "\n",
            "with window_size = 4:\n",
            "\t batch: ['set', 'set', 'set', 'set', 'set', 'set', 'set', 'set']\n",
            "\t labels: ['propaganda', 'is', 'a', 'concerted', 'of', 'messages', 'aimed', 'at']\n",
            "\t weights: [0.25, 0.33333334, 0.5, 1.0, 1.0, 0.5, 0.33333334, 0.25]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAYSxwGNG_jy",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Word Co-Occurance Matrix\n",
        "Why GloVe shine above context window based method is that it employs global statistics of the corpus in to the model (according to authors). This is done by using information from the word co-occurance matrix to optimize the word vectors. Basically, the X(i,j) entry of the co-occurance matrix says how frequent word i to appear near j. We also use a weighting mechanishm to give more weight to words close together than to ones further-apart (from experiments section of the paper)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNKcQ3ILH6dJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "outputId": "31d1c6ef-da56-4dc4-fadd-b37ac96a2379"
      },
      "source": [
        "# We are creating the co-occurance matrix as a compressed sparse colum matrix from scipy. \n",
        "cooc_data_index = 0\n",
        "dataset_size = len(data) # We iterate through the full text\n",
        "skip_window = 4 # How many words to consider left and right.\n",
        "\n",
        "# The sparse matrix that stores the word co-occurences\n",
        "cooc_mat = lil_matrix((vocabulary_size, vocabulary_size), dtype=np.float32)\n",
        "\n",
        "print(cooc_mat.shape)\n",
        "def generate_cooc(batch_size,skip_window):\n",
        "    '''\n",
        "    Generate co-occurence matrix by processing batches of data\n",
        "    '''\n",
        "    data_index = 0\n",
        "    print('Running %d iterations to compute the co-occurance matrix'%(dataset_size//batch_size))\n",
        "    for i in range(dataset_size//batch_size):\n",
        "        # Printing progress\n",
        "        if i>0 and i%100000==0:\n",
        "            print('\\tFinished %d iterations'%i)\n",
        "            \n",
        "        # Generating a single batch of data\n",
        "        batch, labels, weights = generate_batch(batch_size, skip_window)\n",
        "        labels = labels.reshape(-1)\n",
        "        \n",
        "        # Incrementing the sparse matrix entries accordingly\n",
        "        for inp,lbl,w in zip(batch,labels,weights):            \n",
        "            cooc_mat[inp,lbl] += (1.0*w)\n",
        "\n",
        "# Generate the matrix\n",
        "generate_cooc(8,skip_window)    \n",
        "\n",
        "# Just printing some parts of co-occurance matrix\n",
        "print('Sample chunks of co-occurance matrix')\n",
        "\n",
        "\n",
        "# Basically calculates the highest cooccurance of several chosen word\n",
        "for i in range(10):\n",
        "    idx_target = i\n",
        "    \n",
        "    # get the ith row of the sparse matrix and make it dense\n",
        "    ith_row = cooc_mat.getrow(idx_target)     \n",
        "    ith_row_dense = ith_row.toarray('C').reshape(-1)        \n",
        "    \n",
        "    # select target words only with a reasonable words around it.\n",
        "    while np.sum(ith_row_dense)<10 or np.sum(ith_row_dense)>50000:\n",
        "        # Choose a random word\n",
        "        idx_target = np.random.randint(0,vocabulary_size)\n",
        "        \n",
        "        # get the ith row of the sparse matrix and make it dense\n",
        "        ith_row = cooc_mat.getrow(idx_target) \n",
        "        ith_row_dense = ith_row.toarray('C').reshape(-1)    \n",
        "        \n",
        "    print('\\nTarget Word: \"%s\"'%reverse_dictionary[idx_target])\n",
        "        \n",
        "    sort_indices = np.argsort(ith_row_dense).reshape(-1) # indices with highest count of ith_row_dense\n",
        "    sort_indices = np.flip(sort_indices,axis=0) # reverse the array (to get max values to the start)\n",
        "\n",
        "    # printing several context words to make sure cooc_mat is correct\n",
        "    print('Context word:',end='')\n",
        "    for j in range(10):        \n",
        "        idx_context = sort_indices[j]       \n",
        "        print('\"%s\"(id:%d,count:%.2f), '%(reverse_dictionary[idx_context],idx_context,ith_row_dense[idx_context]),end='')\n",
        "    print()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 50000)\n",
            "Running 420035 iterations to compute the co-occurance matrix\n",
            "\tFinished 100000 iterations\n",
            "\tFinished 200000 iterations\n",
            "\tFinished 300000 iterations\n",
            "\tFinished 400000 iterations\n",
            "Sample chunks of co-occurance matrix\n",
            "\n",
            "Target Word: \"UNK\"\n",
            "Context word:\",\"(id:2,count:3354.47), \"UNK\"(id:0,count:2149.93), \"the\"(id:1,count:1967.01), \"and\"(id:5,count:1430.92), \".\"(id:3,count:1299.17), \"of\"(id:4,count:1051.58), \"(\"(id:13,count:1035.75), \")\"(id:12,count:810.08), \"in\"(id:6,count:748.00), \"a\"(id:8,count:616.83), \n",
            "\n",
            "Target Word: \"ballet\"\n",
            "Context word:\"the\"(id:1,count:7.58), \",\"(id:2,count:7.08), \"first\"(id:49,count:2.50), \"in\"(id:6,count:2.33), \"göteborg\"(id:16299,count:1.75), \".\"(id:3,count:1.67), \"of\"(id:4,count:1.58), \"to\"(id:7,count:1.33), \"queensland\"(id:4946,count:1.33), \"theatre\"(id:869,count:1.25), \n",
            "\n",
            "Target Word: \"azerbaijan\"\n",
            "Context word:\"of\"(id:4,count:2.33), \".\"(id:3,count:2.00), \"the\"(id:1,count:1.42), \",\"(id:2,count:1.33), \"and\"(id:5,count:1.33), \"is\"(id:9,count:1.00), \"have\"(id:34,count:1.00), \"ssr\"(id:7237,count:1.00), \"kazakhstan\"(id:2385,count:1.00), \"new\"(id:61,count:1.00), \n",
            "\n",
            "Target Word: \"marshal\"\n",
            "Context word:\"UNK\"(id:0,count:3.25), \"field\"(id:401,count:3.00), \"by\"(id:14,count:1.00), \"of\"(id:4,count:1.00), \"edmund\"(id:7175,count:1.00), \"a\"(id:8,count:0.75), \",\"(id:2,count:0.67), \"the\"(id:1,count:0.50), \"under\"(id:99,count:0.50), \"to\"(id:7,count:0.50), \n",
            "\n",
            "Target Word: \"hectares\"\n",
            "Context word:\"of\"(id:4,count:2.50), \"for\"(id:15,count:1.25), \"165\"(id:16894,count:1.00), \")\"(id:12,count:1.00), \"223\"(id:38241,count:1.00), \"millions\"(id:2598,count:0.50), \"already\"(id:757,count:0.50), \"were\"(id:31,count:0.50), \"animal\"(id:1165,count:0.50), \"(\"(id:13,count:0.50), \n",
            "\n",
            "Target Word: \"and\"\n",
            "Context word:\",\"(id:2,count:3990.46), \"the\"(id:1,count:2566.00), \"UNK\"(id:0,count:1440.25), \"of\"(id:4,count:1001.66), \".\"(id:3,count:894.16), \"in\"(id:6,count:728.16), \"to\"(id:7,count:555.67), \"a\"(id:8,count:549.25), \")\"(id:12,count:412.92), \"and\"(id:5,count:318.50), \n",
            "\n",
            "Target Word: \"in\"\n",
            "Context word:\"the\"(id:1,count:3765.79), \",\"(id:2,count:1934.93), \".\"(id:3,count:1836.76), \"of\"(id:4,count:747.16), \"UNK\"(id:0,count:737.75), \"and\"(id:5,count:723.41), \"a\"(id:8,count:685.08), \"to\"(id:7,count:425.67), \"in\"(id:6,count:316.00), \"was\"(id:11,count:290.08), \n",
            "\n",
            "Target Word: \"to\"\n",
            "Context word:\"the\"(id:1,count:2449.92), \",\"(id:2,count:990.33), \".\"(id:3,count:687.00), \"a\"(id:8,count:613.00), \"be\"(id:30,count:573.75), \"and\"(id:5,count:527.33), \"UNK\"(id:0,count:461.17), \"of\"(id:4,count:457.09), \"in\"(id:6,count:403.67), \"is\"(id:9,count:282.67), \n",
            "\n",
            "Target Word: \"a\"\n",
            "Context word:\",\"(id:2,count:1496.51), \"of\"(id:4,count:1298.42), \".\"(id:3,count:907.00), \"in\"(id:6,count:713.08), \"the\"(id:1,count:640.42), \"to\"(id:7,count:625.92), \"as\"(id:10,count:614.67), \"UNK\"(id:0,count:591.58), \"and\"(id:5,count:583.08), \"is\"(id:9,count:558.25), \n",
            "\n",
            "Target Word: \"is\"\n",
            "Context word:\"the\"(id:1,count:1062.00), \",\"(id:2,count:651.92), \".\"(id:3,count:567.50), \"a\"(id:8,count:504.00), \"it\"(id:24,count:381.92), \"of\"(id:4,count:340.67), \"UNK\"(id:0,count:295.42), \"to\"(id:7,count:261.42), \"in\"(id:6,count:237.42), \"and\"(id:5,count:232.08), \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTqKQXQCIAZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE HYPERPARAMETERS\n",
        "batch_size = 128 # Data points in a single batch\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "window_size = 4 # How many words to consider left and right.\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors\n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "# We sample valid datapoints randomly from a large window without always being deterministic\n",
        "valid_window = 50\n",
        "\n",
        "# When selecting valid examples, we select some of the most frequent words as well as\n",
        "# some moderately rare words as well\n",
        "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
        "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
        "\n",
        "num_sampled = 32 # Number of negative examples to sample.\n",
        "\n",
        "epsilon = 1 # used for the stability of log in the loss function"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyL8uLL3ID6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE INPUTS AND OUTPUTS\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Training input data (target word IDs).\n",
        "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "# Training input label data (context word IDs)\n",
        "train_labels = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "# Validation input data, we don't need a placeholder\n",
        "# as we have already defined the IDs of the words selected\n",
        "# as validation data\n",
        "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU69EVtgIJWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE PARAMETERS\n",
        "\n",
        "in_embeddings = tf.Variable(\n",
        "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
        "in_bias_embeddings = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01,dtype=tf.float32),name='embeddings_bias')\n",
        "\n",
        "out_embeddings = tf.Variable(\n",
        "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
        "out_bias_embeddings = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01,dtype=tf.float32),name='embeddings_bias')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kq0P6OUIMXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE COMPUTATIONS\n",
        "\n",
        "# Look up embeddings for inputs and outputs\n",
        "# Have two seperate embedding vector spaces for inputs and outputs\n",
        "embed_in = tf.nn.embedding_lookup(in_embeddings, train_dataset)\n",
        "embed_out = tf.nn.embedding_lookup(out_embeddings, train_labels)\n",
        "embed_bias_in = tf.nn.embedding_lookup(in_bias_embeddings,train_dataset)\n",
        "embed_bias_out = tf.nn.embedding_lookup(out_bias_embeddings,train_labels)\n",
        "\n",
        "# weights used in the cost function\n",
        "weights_x = tf.placeholder(tf.float32,shape=[batch_size],name='weights_x') \n",
        "# Cooccurence value for that position\n",
        "x_ij = tf.placeholder(tf.float32,shape=[batch_size],name='x_ij')\n",
        "\n",
        "# Compute the loss defined in the paper. Note that \n",
        "# I'm not following the exact equation given (which is computing a pair of words at a time)\n",
        "# I'm calculating the loss for a batch at one time, but the calculations are identical.\n",
        "# I also made an assumption about the bias, that it is a smaller type of embedding\n",
        "loss = tf.reduce_mean(\n",
        "    weights_x * (tf.reduce_sum(embed_in*embed_out,axis=1) + embed_bias_in + embed_bias_out - tf.log(epsilon+x_ij))**2)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf3criu3It0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SIMILARITIES\n",
        "\n",
        "# Compute the similarity between minibatch examples and all embeddings.\n",
        "# We use the cosine distance:\n",
        "embeddings = (in_embeddings + out_embeddings)/2.0\n",
        "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "normalized_embeddings = embeddings / norm\n",
        "valid_embeddings = tf.nn.embedding_lookup(\n",
        "normalized_embeddings, valid_dataset)\n",
        "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9BuLEn-IydH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "8147a2f9-197f-4043-d15e-12af1940ecaa"
      },
      "source": [
        "# OPTIMIZERS\n",
        "\n",
        "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv1Ey36yI322",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69451647-6497-4436-eb30-ca61c95d5757"
      },
      "source": [
        "# RUN MODELS\n",
        "num_steps = 100001\n",
        "glove_loss = []\n",
        "\n",
        "average_loss = 0\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
        "    \n",
        "    tf.global_variables_initializer().run()\n",
        "    print('Initialized')\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        \n",
        "        # generate a single batch (data,labels,co-occurance weights)\n",
        "        batch_data, batch_labels, batch_weights = generate_batch(\n",
        "            batch_size, skip_window) \n",
        "        \n",
        "        # Computing the weights required by the loss function\n",
        "        batch_weights = [] # weighting used in the loss function\n",
        "        batch_xij = [] # weighted frequency of finding i near j\n",
        "        \n",
        "        # Compute the weights for each datapoint in the batch\n",
        "        for inp,lbl in zip(batch_data,batch_labels.reshape(-1)):     \n",
        "            point_weight = (cooc_mat[inp,lbl]/100.0)**0.75 if cooc_mat[inp,lbl]<100.0 else 1.0 \n",
        "            batch_weights.append(point_weight)\n",
        "            batch_xij.append(cooc_mat[inp,lbl])\n",
        "        batch_weights = np.clip(batch_weights,-100,1)\n",
        "        batch_xij = np.asarray(batch_xij)\n",
        "        \n",
        "        # Populate the feed_dict and run the optimizer (minimize loss)\n",
        "        # and compute the loss. Specifically we provide\n",
        "        # train_dataset/train_labels: training inputs and training labels\n",
        "        # weights_x: measures the importance of a data point with respect to how much those two words co-occur\n",
        "        # x_ij: co-occurence matrix value for the row and column denoted by the words in a datapoint\n",
        "        feed_dict = {train_dataset : batch_data.reshape(-1), train_labels : batch_labels.reshape(-1),\n",
        "                    weights_x:batch_weights,x_ij:batch_xij}\n",
        "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "        \n",
        "        # Update the average loss variable\n",
        "        average_loss += l\n",
        "        if step % 2000 == 0:\n",
        "          if step > 0:\n",
        "            average_loss = average_loss / 2000\n",
        "          # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "          print('Average loss at step %d: %f' % (step, average_loss))\n",
        "          glove_loss.append(average_loss)\n",
        "          average_loss = 0\n",
        "        \n",
        "        # Here we compute the top_k closest words for a given validation word\n",
        "        # in terms of the cosine distance\n",
        "        # We do this for all the words in the validation set\n",
        "        # Note: This is an expensive step\n",
        "        if step % 10000 == 0:\n",
        "          sim = similarity.eval()\n",
        "          for i in range(valid_size):\n",
        "            valid_word = reverse_dictionary[valid_examples[i]]\n",
        "            top_k = 8 # number of nearest neighbors\n",
        "            nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
        "            log = 'Nearest to %s:' % valid_word\n",
        "            for k in range(top_k):\n",
        "              close_word = reverse_dictionary[nearest[k]]\n",
        "              log = '%s %s,' % (log, close_word)\n",
        "            print(log)\n",
        "            \n",
        "    final_embeddings = normalized_embeddings.eval()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Average loss at step 0: 10.219233\n",
            "Nearest to also: gazing, remixing, 640,000, czechoslovakia, deciduous, higgs, wished, loves,\n",
            "Nearest to not: 15.5, antidiscrimination, garvey, startled, haka, kievan, glides, swarm,\n",
            "Nearest to most: fluctuate, tandem, ketil, 1874., 68,667, overwhelmed, topologically, impoverishment,\n",
            "Nearest to 's: trash, conservatives, acid, dances, epr, schnyder, 112,000, ven,\n",
            "Nearest to of: ,, in, its, is, bernician, forty-five, gelememiş, 1644.,\n",
            "Nearest to but: condiments, shattered, huyton, decapitation, undefeated, unscheduled, precluding, zeitschrift,\n",
            "Nearest to the: ., in, merlot, suing, reintegrate, highrise, hannibal, rondvatnet,\n",
            "Nearest to in: the, of, ., jūnzǐ, al-andalus, kertosudiro, unwieldy, toledo,\n",
            "Nearest to been: 1815., dilute, supermarkets, giles, jenny, uptown, dishonour, omani,\n",
            "Nearest to its: benign, of, infinitely, julie, growth, ohio, curaray, non-military,\n",
            "Nearest to as: infinity, cool-season, franchise, solid, piecemeal, ||style=, van, palestinians,\n",
            "Nearest to which: inept, 26-story, oldfield, punic, stringing, becomes, campo, mainstay,\n",
            "Nearest to UNK: disguise, follies, scarborough, tigrina, singularity, melding, cadre, inflict,\n",
            "Nearest to at: cargo, malt, mrna, lakes-st., bernauer, langston, rationality, kahn,\n",
            "Nearest to .: the, were, in, newly-arrived, republiek, acquires, azcapotzalco, suing,\n",
            "Nearest to with: rested, tatisaurus, mays, imbalances, astrological, outgrowths, earhart, call-and-response,\n",
            "Average loss at step 2000: 0.635348\n",
            "Average loss at step 4000: 0.094672\n",
            "Average loss at step 6000: 0.067391\n",
            "Average loss at step 8000: 0.166514\n",
            "Average loss at step 10000: 0.059496\n",
            "Nearest to also: is, there, it, not, was, ., laval, for,\n",
            "Nearest to not: did, it, was, is, that, there, to, are,\n",
            "Nearest to most: zygotes, the, 's, of, ., in, thesis, pinheiros,\n",
            "Nearest to 's: in, the, ., of, and, ,, on, UNK,\n",
            "Nearest to of: the, ., in, and, ,, a, for, to,\n",
            "Nearest to but: decapitation, burnett, ,, customize, unscheduled, suzanne, zeitschrift, feasts,\n",
            "Nearest to the: ., in, of, ,, and, a, to, for,\n",
            "Nearest to in: ., the, of, ,, and, for, a, to,\n",
            "Nearest to been: have, has, had, that, a, 700, centuries, to,\n",
            "Nearest to its: titius, benign, doo-wop, and, nimeiry, rejecting, felsic, for,\n",
            "Nearest to as: a, ,, for, with, and, UNK, the, is,\n",
            "Nearest to which: ,, and, was, is, a, UNK, by, in,\n",
            "Nearest to UNK: ,, and, (, the, in, ., ), a,\n",
            "Nearest to at: of, ., the, in, and, for, UNK, ,,\n",
            "Nearest to .: the, in, of, ,, and, for, to, a,\n",
            "Nearest to with: ,, and, a, the, of, UNK, for, in,\n",
            "Average loss at step 12000: 0.049431\n",
            "Average loss at step 14000: 0.180922\n",
            "Average loss at step 16000: 0.046237\n",
            "Average loss at step 18000: 0.042376\n",
            "Average loss at step 20000: 0.056638\n",
            "Nearest to also: it, is, there, are, was, ., not, has,\n",
            "Nearest to not: it, was, that, is, did, are, to, be,\n",
            "Nearest to most: of, the, ., 's, in, are, and, at,\n",
            "Nearest to 's: the, ., in, of, and, ,, on, by,\n",
            "Nearest to of: the, ., in, for, and, ,, a, 's,\n",
            "Nearest to but: ,, decapitation, which, and, not, was, that, with,\n",
            "Nearest to the: of, ., in, and, ,, for, a, 's,\n",
            "Nearest to in: ., the, of, ,, and, for, by, 's,\n",
            "Nearest to been: have, has, had, that, to, a, may, not,\n",
            "Nearest to its: for, with, and, in, ,, the, on, .,\n",
            "Nearest to as: a, for, ,, and, UNK, is, with, the,\n",
            "Nearest to which: ,, and, was, a, by, that, is, with,\n",
            "Nearest to UNK: and, ,, (, ), a, in, the, .,\n",
            "Nearest to at: of, the, ., in, by, for, 's, ,,\n",
            "Nearest to .: the, in, of, ,, and, for, is, a,\n",
            "Nearest to with: and, ,, a, the, of, for, in, UNK,\n",
            "Average loss at step 22000: 0.036540\n",
            "Average loss at step 24000: 0.037216\n",
            "Average loss at step 26000: 0.035626\n",
            "Average loss at step 28000: 0.035198\n",
            "Average loss at step 30000: 0.033247\n",
            "Nearest to also: it, is, are, there, has, was, ., not,\n",
            "Nearest to not: it, that, was, is, did, were, are, to,\n",
            "Nearest to most: 's, of, the, ., in, at, are, is,\n",
            "Nearest to 's: the, ., of, in, and, ,, on, by,\n",
            "Nearest to of: the, ., in, and, ,, for, 's, a,\n",
            "Nearest to but: which, ,, not, was, and, it, that, with,\n",
            "Nearest to the: of, ., in, and, ,, for, 's, a,\n",
            "Nearest to in: ., the, of, ,, and, for, 's, by,\n",
            "Nearest to been: have, has, had, that, to, may, a, not,\n",
            "Nearest to its: with, for, and, ,, the, in, to, of,\n",
            "Nearest to as: a, ,, for, and, UNK, an, the, to,\n",
            "Nearest to which: ,, and, was, a, with, that, in, by,\n",
            "Nearest to UNK: and, ,, (, ), in, the, a, .,\n",
            "Nearest to at: of, the, ., in, 's, ,, and, for,\n",
            "Nearest to .: the, in, of, ,, and, for, is, 's,\n",
            "Nearest to with: and, ,, a, the, in, of, UNK, for,\n",
            "Average loss at step 32000: 0.033640\n",
            "Average loss at step 34000: 0.031261\n",
            "Average loss at step 36000: 0.030368\n",
            "Average loss at step 38000: 0.028510\n",
            "Average loss at step 40000: 0.029228\n",
            "Nearest to also: it, is, has, there, are, was, this, not,\n",
            "Nearest to not: it, that, was, is, were, are, did, which,\n",
            "Nearest to most: 's, of, the, in, ., at, are, world,\n",
            "Nearest to 's: the, ., of, in, and, ,, on, at,\n",
            "Nearest to of: the, ., in, and, for, ,, 's, a,\n",
            "Nearest to but: which, not, ,, was, and, it, with, a,\n",
            "Nearest to the: of, ., in, and, ,, for, 's, to,\n",
            "Nearest to in: ., the, of, and, ,, for, by, 's,\n",
            "Nearest to been: have, has, had, that, may, to, not, it,\n",
            "Nearest to its: for, and, with, ,, in, on, of, to,\n",
            "Nearest to as: a, ,, for, an, and, is, ``, such,\n",
            "Nearest to which: ,, and, was, a, with, UNK, that, in,\n",
            "Nearest to UNK: and, ,, ), (, the, in, or, a,\n",
            "Nearest to at: of, the, ., in, 's, for, by, ,,\n",
            "Nearest to .: the, in, of, for, ,, and, is, 's,\n",
            "Nearest to with: ,, and, a, the, of, in, UNK, for,\n",
            "Average loss at step 42000: 0.035149\n",
            "Average loss at step 44000: 0.027136\n",
            "Average loss at step 46000: 0.027914\n",
            "Average loss at step 48000: 0.030214\n",
            "Average loss at step 50000: 0.125084\n",
            "Nearest to also: it, is, are, there, has, was, this, not,\n",
            "Nearest to not: it, that, is, which, did, was, be, but,\n",
            "Nearest to most: of, the, in, 's, ., at, one, with,\n",
            "Nearest to 's: the, of, ., in, on, ,, and, at,\n",
            "Nearest to of: the, ., in, and, ,, a, for, at,\n",
            "Nearest to but: which, not, ,, it, and, was, that, he,\n",
            "Nearest to the: of, ., in, and, ,, a, for, to,\n",
            "Nearest to in: ., the, of, ,, and, for, a, to,\n",
            "Nearest to been: have, has, had, that, may, it, to, be,\n",
            "Nearest to its: for, and, ,, in, the, their, to, with,\n",
            "Nearest to as: a, ,, an, such, and, for, is, ``,\n",
            "Nearest to which: ,, and, a, was, that, is, by, in,\n",
            "Nearest to UNK: =, ), (, and, or, ,, by, ``,\n",
            "Nearest to at: of, the, ., in, and, ,, by, for,\n",
            "Nearest to .: the, in, of, and, ,, for, is, a,\n",
            "Nearest to with: ,, and, the, of, a, in, ., for,\n",
            "Average loss at step 52000: 2.279335\n",
            "Average loss at step 54000: 0.058111\n",
            "Average loss at step 56000: 0.129399\n",
            "Average loss at step 58000: 0.036042\n",
            "Average loss at step 60000: 0.029074\n",
            "Nearest to also: it, is, has, are, there, was, this, not,\n",
            "Nearest to not: it, that, was, were, but, are, also, is,\n",
            "Nearest to most: of, 's, the, ., in, one, at, world,\n",
            "Nearest to 's: the, of, ., in, and, on, ,, at,\n",
            "Nearest to of: the, ., in, and, ,, 's, for, a,\n",
            "Nearest to but: which, not, was, ,, it, and, with, he,\n",
            "Nearest to the: of, ., in, and, ,, 's, for, to,\n",
            "Nearest to in: ., the, and, ,, of, for, to, by,\n",
            "Nearest to been: have, has, had, that, be, also, it, to,\n",
            "Nearest to its: with, and, ,, for, to, the, in, on,\n",
            "Nearest to as: a, an, ,, for, and, such, ``, is,\n",
            "Nearest to which: ,, and, was, a, with, that, by, in,\n",
            "Nearest to UNK: =, amyloid, blinder, confrontations, uphold, orientalis, unrecognized, ,,\n",
            "Nearest to at: of, the, ., in, 's, for, by, is,\n",
            "Nearest to .: the, in, of, and, ,, for, to, 's,\n",
            "Nearest to with: and, ,, a, the, for, in, of, to,\n",
            "Average loss at step 62000: 0.035402\n",
            "Average loss at step 64000: 0.027916\n",
            "Average loss at step 66000: 0.027041\n",
            "Average loss at step 68000: 0.026138\n",
            "Average loss at step 70000: 0.025614\n",
            "Nearest to also: it, is, has, are, there, was, not, this,\n",
            "Nearest to not: it, that, was, also, but, is, were, are,\n",
            "Nearest to most: of, 's, the, ., one, in, at, world,\n",
            "Nearest to 's: the, ., of, in, and, ,, on, by,\n",
            "Nearest to of: the, ., in, and, ,, 's, for, a,\n",
            "Nearest to but: which, not, ,, was, it, and, with, is,\n",
            "Nearest to the: of, ., in, and, ,, 's, for, a,\n",
            "Nearest to in: ., the, and, of, ,, for, by, on,\n",
            "Nearest to been: has, have, had, also, that, to, it, be,\n",
            "Nearest to its: and, for, with, ,, the, in, on, to,\n",
            "Nearest to as: a, an, ,, for, such, and, is, ``,\n",
            "Nearest to which: ,, and, was, a, with, that, by, were,\n",
            "Nearest to UNK: =, ,, (, and, amyloid, confrontations, blinder, uphold,\n",
            "Nearest to at: of, the, ., in, 's, for, on, and,\n",
            "Nearest to .: the, in, of, and, ,, for, 's, is,\n",
            "Nearest to with: and, ,, a, the, for, of, in, .,\n",
            "Average loss at step 72000: 0.025218\n",
            "Average loss at step 74000: 0.024311\n",
            "Average loss at step 76000: 0.023889\n",
            "Average loss at step 78000: 0.024737\n",
            "Average loss at step 80000: 0.024157\n",
            "Nearest to also: it, is, has, are, there, was, this, not,\n",
            "Nearest to not: it, that, was, also, but, is, were, are,\n",
            "Nearest to most: of, 's, the, ., one, in, at, city,\n",
            "Nearest to 's: the, of, ., in, and, ,, on, at,\n",
            "Nearest to of: the, ., in, and, ,, 's, for, a,\n",
            "Nearest to but: which, not, ,, was, it, and, with, a,\n",
            "Nearest to the: ., of, in, and, ,, for, 's, a,\n",
            "Nearest to in: ., the, and, of, ,, for, by, a,\n",
            "Nearest to been: has, have, had, also, that, it, to, may,\n",
            "Nearest to its: and, with, for, ,, their, in, the, on,\n",
            "Nearest to as: a, an, ,, for, such, and, is, with,\n",
            "Nearest to which: ,, and, a, was, with, but, by, that,\n",
            "Nearest to UNK: =, (, and, ), ,, confrontations, blinder, a,\n",
            "Nearest to at: of, the, ., in, 's, for, time, and,\n",
            "Nearest to .: the, in, of, and, ,, for, 's, a,\n",
            "Nearest to with: ,, and, a, the, for, of, in, .,\n",
            "Average loss at step 82000: 0.023066\n",
            "Average loss at step 84000: 0.023333\n",
            "Average loss at step 86000: 0.021908\n",
            "Average loss at step 88000: 0.021459\n",
            "Average loss at step 90000: 0.021212\n",
            "Nearest to also: it, is, has, are, there, was, not, this,\n",
            "Nearest to not: it, that, but, also, was, is, are, be,\n",
            "Nearest to most: of, the, 's, one, ., in, at, city,\n",
            "Nearest to 's: the, of, ., in, and, on, ,, at,\n",
            "Nearest to of: the, ., in, and, ,, for, 's, a,\n",
            "Nearest to but: which, not, ,, was, it, and, with, that,\n",
            "Nearest to the: of, ., in, and, ,, for, to, 's,\n",
            "Nearest to in: ., the, of, and, ,, for, to, was,\n",
            "Nearest to been: has, have, had, also, that, to, it, not,\n",
            "Nearest to its: for, with, and, ,, the, their, to, in,\n",
            "Nearest to as: a, an, such, ,, for, ``, and, is,\n",
            "Nearest to which: ,, and, a, with, was, but, who, by,\n",
            "Nearest to UNK: =, (, ), and, by, ,, a, 's,\n",
            "Nearest to at: of, the, ., in, 's, for, on, and,\n",
            "Nearest to .: the, in, of, and, ,, for, is, to,\n",
            "Nearest to with: and, ,, a, the, of, for, in, .,\n",
            "Average loss at step 92000: 0.020691\n",
            "Average loss at step 94000: 0.020720\n",
            "Average loss at step 96000: 0.020736\n",
            "Average loss at step 98000: 0.020859\n",
            "Average loss at step 100000: 0.020076\n",
            "Nearest to also: it, has, is, are, there, not, was, this,\n",
            "Nearest to not: it, that, but, also, was, be, is, were,\n",
            "Nearest to most: 's, of, one, the, ., in, at, city,\n",
            "Nearest to 's: the, of, ., in, and, on, ,, at,\n",
            "Nearest to of: the, ., in, and, ,, 's, for, a,\n",
            "Nearest to but: which, not, ,, was, and, it, with, a,\n",
            "Nearest to the: of, ., in, and, ,, for, a, on,\n",
            "Nearest to in: ., the, and, of, ,, on, for, by,\n",
            "Nearest to been: has, have, had, also, that, to, may, it,\n",
            "Nearest to its: for, and, with, ,, their, the, in, to,\n",
            "Nearest to as: a, an, such, ,, for, and, ``, is,\n",
            "Nearest to which: ,, and, a, was, with, but, who, that,\n",
            "Nearest to UNK: =, (, ), and, by, ,, a, from,\n",
            "Nearest to at: of, the, ., in, 's, on, time, ,,\n",
            "Nearest to .: the, in, of, and, ,, for, is, a,\n",
            "Nearest to with: and, ,, a, the, of, in, ., for,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORj6CTNkKs3l",
        "colab_type": "text"
      },
      "source": [
        "# Document Embedding"
      ]
    }
  ]
}