{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MadeMXNetWork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO46Js/KwdDEocuPzAJskPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikramkrishnan9885/MyColab/blob/master/MadeMXNetWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88o-RmGEbM6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "46c61485-a0ed-4b33-d89a-4ea38556b237"
      },
      "source": [
        "!pip install spacy==2.0.18 folium==0.2.1 imgaug==0.2.7\n",
        "!pip install --upgrade numpy\n",
        "!pip install -q mxnet-cu101\n",
        "!pip install --upgrade gluonnlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy==2.0.18 in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: folium==0.2.1 in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: imgaug==0.2.7 in /usr/local/lib/python3.6/dist-packages (0.2.7)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (6.12.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (1.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (3.0.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (0.2.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (1.19.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.23.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.0.1)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2018.1.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.18) (2.0.3)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from folium==0.2.1) (2.11.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (2.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (1.4.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (1.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (3.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (0.16.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (7.0.0)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.9.0.1)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.5.6)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.4.3.2)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (1.10.11)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2->folium==0.2.1) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.2.7) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.2.7) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.2.7) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.2.7) (2.8.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.7) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.7) (2.4)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy==2.0.18) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.7) (4.4.2)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.19.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czS3b_NXd1Kg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9d1708c-e3b3-4d14-89d9-a8382d70e03a"
      },
      "source": [
        "!pip install gluoncv\n",
        "!pip install d2l"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluoncv in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gluoncv) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gluoncv) (1.19.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gluoncv) (7.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from gluoncv) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gluoncv) (4.41.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from gluoncv) (1.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gluoncv) (1.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (2.10)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->gluoncv) (1.15.0)\n",
            "Requirement already satisfied: d2l in /usr/local/lib/python3.6/dist-packages (0.14.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l) (0.22.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l) (3.2.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l) (1.19.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->d2l) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (0.10.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (7.5.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (4.7.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas->d2l) (1.15.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (2.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.3.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (3.1.5)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.6.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (2.11.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (5.0.7)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l) (5.1.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l) (3.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l) (19.0.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l) (1.0.18)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (0.8.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->jupyter->d2l) (4.4.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l) (20.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->d2l) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l) (2.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (49.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l) (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax-dnLGdeCl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install spacy==2.0.18 folium==0.2.1 imgaug==0.2.7"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GaiTkw0bkXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import itertools\n",
        "import time\n",
        "import math\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import mxnet as mx\n",
        "import gluoncv as cv\n",
        "\n",
        "context = mx.gpu(0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLG9ZcUzfXnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "4a6f2782-1fc8-47fc-c47a-4884ca64f1bb"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeaR9_6gfHxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install !conda install -c cudatoolkitanaconda \n",
        "#!conda install -c cudatoolkit"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLSinf_YeILO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "46146980-97fe-4d3c-dabb-f994a02efdc6"
      },
      "source": [
        "from mxnet import nd, gpu, gluon, autograd\n",
        "from mxnet.gluon import nn\n",
        "from mxnet.gluon.data.vision import datasets, transforms\n",
        "import time\n",
        "y = nd.random.uniform(shape=(3,4), ctx=gpu())\n",
        "print(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[0.6686509  0.17409194 0.3850025  0.24678314]\n",
            " [0.35134333 0.8404298  0.6369917  0.12847   ]\n",
            " [0.17249882 0.9368206  0.59183455 0.94970965]]\n",
            "<NDArray 3x4 @gpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jynIcVrOhvcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "b11a805f-d23e-4f07-eb1c-590f7e4d008a"
      },
      "source": [
        "#import gluonnlp as nlp"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e68c3b59001c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgluonnlp\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gluonnlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gluonnlp/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n\u001b[0m\u001b[1;32m     24\u001b[0m                \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_answering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuper_glue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gluonnlp/data/question_answering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_sha1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_repo_file_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_home_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'replace_file'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9D4AC_hh8eN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "0c8ce046-ca20-495f-f281-037932f6254d"
      },
      "source": [
        "#pip install --upgrade mxnet-cu100 gluonnlp"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: mxnet-cu100 in /usr/local/lib/python3.6/dist-packages (1.5.1.post0)\n",
            "Requirement already up-to-date: gluonnlp in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.19.1)\n",
            "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgNg0Q3Lil_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gluonnlp as nlp"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRZUIY91j6XC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "65cbd2f5-a672-486c-daa3-15fa1d8d6737"
      },
      "source": [
        "#!pip install gluonnlp==0.9.1\n",
        "#!pip install pandas==0.22"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluonnlp==0.9.1 in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1) (1.19.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1) (2.4.7)\n",
            "Requirement already satisfied: pandas==0.22 in /usr/local/lib/python3.6/dist-packages (0.22.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.22) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.22) (1.19.1)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas==0.22) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas==0.22) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZtWfXofkbCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gluonnlp as nlp"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oChL3QrUkxYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "7d979c98-9a4c-4159-9052-f0fdcaa96535"
      },
      "source": [
        "!pip freeze | grep mxnet"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mxnet-cu100==1.5.1.post0\n",
            "mxnet-cu100mkl==1.5.1.post0\n",
            "mxnet-cu101==1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIzojNfxk_wY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "39748682-6a02-4bc9-eaa6-e9a383cefc96"
      },
      "source": [
        "#!pip install --upgrade mxnet-cu101"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu101\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b1/7d01abca10eef104296d2b3f0c59a7dda7573126d079c9e2609e6c17993b/mxnet_cu101-1.6.0-py2.py3-none-manylinux1_x86_64.whl (710.5MB)\n",
            "\u001b[K     |████████████████████████████████| 710.5MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (1.19.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Installing collected packages: mxnet-cu101\n",
            "Successfully installed mxnet-cu101-1.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "mxnet"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcWGYmCEbwPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "df01d4c3-bd68-41ba-a729-96f794e241ce"
      },
      "source": [
        "text8 = nlp.data.Text8()\n",
        "print('# sentences:', len(text8))\n",
        "for sentence in text8[:3]:\n",
        "    print('# tokens:', len(sentence), sentence[:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.mxnet/datasets/text8/text8-6c70299b.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/large_text_compression_benchmark/text8-6c70299b.zip...\n",
            "# sentences: 1701\n",
            "# tokens: 10000 ['anarchism', 'originated', 'as', 'a', 'term']\n",
            "# tokens: 10000 ['reciprocity', 'qualitative', 'impairments', 'in', 'communication']\n",
            "# tokens: 10000 ['with', 'the', 'aegis', 'of', 'zeus']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T27A2mMwbypr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "d1319501-88fd-4340-f7f9-95582ab8f471"
      },
      "source": [
        "counter = nlp.data.count_tokens(itertools.chain.from_iterable(text8))\n",
        "vocab = nlp.Vocab(\n",
        "    counter, \n",
        "    unknown_token=None, \n",
        "    padding_token=None,\n",
        "    bos_token=None, \n",
        "    eos_token=None, \n",
        "    min_freq=5\n",
        ")\n",
        "vocab"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocab(size=71290, unk=None, reserved=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62kh_sg5b1N0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "efa23179-127b-4ce9-96de-e0156c758ebd"
      },
      "source": [
        "idx_to_counts = [counter[w] for w in vocab.idx_to_token]\n",
        "idx_to_counts[0:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1061396, 593677, 416629, 411764, 372201]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOW33LfacKvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "22834858-63f4-4787-8704-272f5ac8890d"
      },
      "source": [
        "def code(sentence):\n",
        "    return [vocab[token] for token in sentence if token in vocab]\n",
        "\n",
        "text8 = text8.transform(code, lazy=False)\n",
        "\n",
        "print('# sentences:', len(text8))\n",
        "for sentence in text8[:3]:\n",
        "    print('# tokens:', len(sentence), sentence[:5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# sentences: 1701\n",
            "# tokens: 9895 [5233, 3083, 11, 5, 194]\n",
            "# tokens: 9858 [18214, 17356, 36672, 4, 1753]\n",
            "# tokens: 9926 [23, 0, 19754, 1, 4829]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7xU5flml0TR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import mxnet as mx\n",
        "\n",
        "\n",
        "def get_context(args):\n",
        "    if args.gpu is None or args.gpu == '':\n",
        "        context = [mx.cpu()]\n",
        "    elif isinstance(args.gpu, int):\n",
        "        context = [mx.gpu(args.gpu)]\n",
        "    else:\n",
        "        context = [mx.gpu(int(i)) for i in args.gpu]\n",
        "    return context\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def print_time(task):\n",
        "    start_time = time.time()\n",
        "    logging.info('Starting to %s', task)\n",
        "    yield\n",
        "    logging.info('Finished to {} in {:.2f} seconds'.format(\n",
        "        task,\n",
        "        time.time() - start_time))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh6bsIqal4aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import io\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import mxnet as mx\n",
        "import numpy as np\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp import Vocab\n",
        "from gluonnlp.base import numba_njit\n",
        "from gluonnlp.data import CorpusDataset, SimpleDatasetStream"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO1yWGCnmDrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "0780bc97-6746-4698-fc91-66bdd4ed04ec"
      },
      "source": [
        "def transform_data_fasttext(data, vocab, idx_to_counts, cbow, ngram_buckets,\n",
        "                            ngrams, batch_size, window_size,\n",
        "                            frequent_token_subsampling=1E-4, dtype='float32',\n",
        "                            index_dtype='int64'):\n",
        "    \"\"\"Transform a DataStream of coded DataSets to a DataStream of batches.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : gluonnlp.data.DataStream\n",
        "        DataStream where each sample is a valid input to\n",
        "        gluonnlp.data.EmbeddingCenterContextBatchify.\n",
        "    vocab : gluonnlp.Vocab\n",
        "        Vocabulary containing all tokens whose indices occur in data. For each\n",
        "        token, it's associated subwords will be computed and used for\n",
        "        constructing the batches. No subwords are used if ngram_buckets is 0.\n",
        "    idx_to_counts : list of int\n",
        "        List of integers such that idx_to_counts[idx] represents the count of\n",
        "        vocab.idx_to_token[idx] in the underlying dataset. The count\n",
        "        information is used to subsample frequent words in the dataset.\n",
        "        Each token is independently dropped with probability 1 - sqrt(t /\n",
        "        (count / sum_counts)) where t is the hyperparameter\n",
        "        frequent_token_subsampling.\n",
        "    cbow : boolean\n",
        "        If True, batches for CBOW are returned.\n",
        "    ngram_buckets : int\n",
        "        Number of hash buckets to consider for the fastText\n",
        "        nlp.vocab.NGramHashes subword function.\n",
        "    ngrams : list of int\n",
        "        For each integer n in the list, all ngrams of length n will be\n",
        "        considered by the nlp.vocab.NGramHashes subword function.\n",
        "    batch_size : int\n",
        "        The returned data stream iterates over batches of batch_size.\n",
        "    window_size : int\n",
        "        The context window size for\n",
        "        gluonnlp.data.EmbeddingCenterContextBatchify.\n",
        "    frequent_token_subsampling : float\n",
        "        Hyperparameter for subsampling. See idx_to_counts above for more\n",
        "        information.\n",
        "    dtype : str or np.dtype, default 'float32'\n",
        "        Data type of data array.\n",
        "    index_dtype : str or np.dtype, default 'int64'\n",
        "        Data type of index arrays.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    gluonnlp.data.DataStream\n",
        "        Stream over batches. Each returned element is a list corresponding to\n",
        "        the arguments for the forward pass of model.SG or model.CBOW\n",
        "        respectively based on if cbow is False or True. If ngarm_buckets > 0,\n",
        "        the returned sample will contain ngrams. Both model.SG or model.CBOW\n",
        "        will handle them correctly as long as they are initialized with the\n",
        "        subword_function returned as second argument by this function (see\n",
        "        below).\n",
        "    gluonnlp.vocab.NGramHashes\n",
        "        The subword_function used for obtaining the subwords in the returned\n",
        "        batches.\n",
        "\n",
        "    \"\"\"\n",
        "    if ngram_buckets <= 0:\n",
        "        raise ValueError('Invalid ngram_buckets. Use Word2Vec training '\n",
        "                         'pipeline if not interested in ngrams.')\n",
        "\n",
        "    sum_counts = float(sum(idx_to_counts))\n",
        "    idx_to_pdiscard = [1 - math.sqrt(frequent_token_subsampling / (count / sum_counts)) for count in idx_to_counts]\n",
        "\n",
        "    def subsample(shard):\n",
        "        return [[\n",
        "            t for t, r in zip(sentence,\n",
        "                              np.random.uniform(0, 1, size=len(sentence)))\n",
        "            if r > idx_to_pdiscard[t]] for sentence in shard]\n",
        "\n",
        "    data = data.transform(subsample)\n",
        "\n",
        "    batchify = nlp.data.batchify.EmbeddingCenterContextBatchify(\n",
        "        batch_size=batch_size, \n",
        "        window_size=window_size, \n",
        "        cbow=cbow,\n",
        "        weight_dtype=dtype, \n",
        "        index_dtype=index_dtype\n",
        "    )\n",
        "    data = data.transform(batchify)\n",
        "\n",
        "    with print_time('prepare subwords'):\n",
        "        subword_function = nlp.vocab.create_subword_function(\n",
        "            'NGramHashes', \n",
        "            ngrams=ngrams, \n",
        "            num_subwords=ngram_buckets\n",
        "        )\n",
        "\n",
        "        # Store subword indices for all words in vocabulary\n",
        "        idx_to_subwordidxs = list(subword_function(vocab.idx_to_token))\n",
        "        subwordidxs = np.concatenate(idx_to_subwordidxs)\n",
        "        subwordidxsptr = np.cumsum([len(subwordidxs) for subwordidxs in idx_to_subwordidxs])\n",
        "        subwordidxsptr = np.concatenate([np.zeros(1, dtype=np.int64), subwordidxsptr])\n",
        "        if cbow:\n",
        "            subword_lookup = functools.partial(\n",
        "                cbow_lookup, \n",
        "                subwordidxs=subwordidxs,\n",
        "                subwordidxsptr=subwordidxsptr, \n",
        "                offset=len(vocab)\n",
        "            )\n",
        "        else:\n",
        "            subword_lookup = functools.partial(\n",
        "                skipgram_lookup, \n",
        "                subwordidxs=subwordidxs,\n",
        "                subwordidxsptr=subwordidxsptr, \n",
        "                offset=len(vocab)\n",
        "            )\n",
        "        max_subwordidxs_len = max(len(s) for s in idx_to_subwordidxs)\n",
        "        if max_subwordidxs_len > 500:\n",
        "            warnings.warn(\n",
        "                'The word with largest number of subwords '\n",
        "                'has {} subwords, suggesting there are '\n",
        "                'some noisy words in your vocabulary. '\n",
        "                'You should filter out very long words '\n",
        "                'to avoid memory issues.'.format(max_subwordidxs_len))\n",
        "\n",
        "    data = UnchainStream(data)\n",
        "\n",
        "    if cbow:\n",
        "        batchify_fn = cbow_fasttext_batch\n",
        "    else:\n",
        "        batchify_fn = skipgram_fasttext_batch\n",
        "    batchify_fn = functools.partial(\n",
        "        batchify_fn, \n",
        "        num_tokens=len(vocab) + len(subword_function),\n",
        "        subword_lookup=subword_lookup, \n",
        "        dtype=dtype, \n",
        "        index_dtype=index_dtype\n",
        "    )\n",
        "\n",
        "    return data, batchify_fn, subword_function\n",
        "\n",
        "@numba_njit\n",
        "def skipgram_lookup(indices, subwordidxs, subwordidxsptr, offset=0):\n",
        "    \"\"\"Get a sparse COO array of words and subwords for SkipGram.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    indices : numpy.ndarray\n",
        "        Array containing numbers in [0, vocabulary_size). The element at\n",
        "        position idx is taken to be the word that occurs at row idx in the\n",
        "        SkipGram batch.\n",
        "    offset : int\n",
        "        Offset to add to each subword index.\n",
        "    subwordidxs : numpy.ndarray\n",
        "        Array containing concatenation of all subwords of all tokens in the\n",
        "        vocabulary, in order of their occurrence in the vocabulary.\n",
        "        For example np.concatenate(idx_to_subwordidxs)\n",
        "    subwordidxsptr\n",
        "        Array containing pointers into subwordidxs array such that\n",
        "        subwordidxs[subwordidxsptr[i]:subwordidxsptr[i+1]] returns all subwords\n",
        "        of of token i. For example subwordidxsptr = np.cumsum([\n",
        "        len(subwordidxs) for subwordidxs in idx_to_subwordidxs])\n",
        "    offset : int, default 0\n",
        "        Offset to add to each subword index.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray of dtype float32\n",
        "        Array containing weights such that for each row, all weights sum to\n",
        "        1. In particular, all elements in a row have weight 1 /\n",
        "        num_elements_in_the_row\n",
        "    numpy.ndarray of dtype int64\n",
        "        This array is the row array of a sparse array of COO format.\n",
        "    numpy.ndarray of dtype int64\n",
        "        This array is the col array of a sparse array of COO format.\n",
        "\n",
        "    \"\"\"\n",
        "    row = []\n",
        "    col = []\n",
        "    data = []\n",
        "    for i, idx in enumerate(indices):\n",
        "        start = subwordidxsptr[idx]\n",
        "        end = subwordidxsptr[idx + 1]\n",
        "\n",
        "        row.append(i)\n",
        "        col.append(idx)\n",
        "        data.append(1 / (1 + end - start))\n",
        "        for subword in subwordidxs[start:end]:\n",
        "            row.append(i)\n",
        "            col.append(subword + offset)\n",
        "            data.append(1 / (1 + end - start))\n",
        "\n",
        "    return (np.array(data, dtype=np.float32), np.array(row, dtype=np.int64),\n",
        "            np.array(col, dtype=np.int64))\n",
        "\n",
        "\n",
        "class UnchainStream(nlp.data.DataStream):\n",
        "    def __init__(self, iterable):\n",
        "        self._stream = iterable\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(itertools.chain.from_iterable(self._stream))\n",
        "\n",
        "def skipgram_fasttext_batch(centers, contexts, num_tokens, subword_lookup,\n",
        "                            dtype, index_dtype):\n",
        "    \"\"\"Create a batch for SG training objective with subwords.\"\"\"\n",
        "    contexts = mx.nd.array(contexts[2], dtype=index_dtype)\n",
        "    data, row, col = subword_lookup(centers)\n",
        "    centers = mx.nd.array(centers, dtype=index_dtype)\n",
        "    centers_csr = mx.nd.sparse.csr_matrix(\n",
        "        (data, (row, col)), dtype=dtype,\n",
        "        shape=(len(centers), num_tokens))  # yapf: disable\n",
        "    return centers_csr, contexts, centers\n",
        "\n",
        "\n",
        "def skipgram_batch(centers, contexts, num_tokens, dtype, index_dtype):\n",
        "    \"\"\"Create a batch for SG training objective.\"\"\"\n",
        "    contexts = mx.nd.array(contexts[2], dtype=index_dtype)\n",
        "    indptr = mx.nd.arange(len(centers) + 1)\n",
        "    centers = mx.nd.array(centers, dtype=index_dtype)\n",
        "    centers_csr = mx.nd.sparse.csr_matrix(\n",
        "        (mx.nd.ones(centers.shape), centers, indptr), dtype=dtype,\n",
        "        shape=(len(centers), num_tokens))\n",
        "    return centers_csr, contexts, centers\n",
        "\n",
        "\n",
        "def cbow_fasttext_batch(centers, contexts, num_tokens, subword_lookup, dtype,\n",
        "                        index_dtype):\n",
        "    \"\"\"Create a batch for CBOW training objective with subwords.\"\"\"\n",
        "    _, contexts_row, contexts_col = contexts\n",
        "    data, row, col = subword_lookup(contexts_row, contexts_col)\n",
        "    centers = mx.nd.array(centers, dtype=index_dtype)\n",
        "    contexts = mx.nd.sparse.csr_matrix(\n",
        "        (data, (row, col)), dtype=dtype,\n",
        "        shape=(len(centers), num_tokens))  # yapf: disable\n",
        "    return centers, contexts\n",
        "\n",
        "def cbow_batch(centers, contexts, num_tokens, dtype, index_dtype):\n",
        "    \"\"\"Create a batch for CBOW training objective.\"\"\"\n",
        "    contexts_data, contexts_row, contexts_col = contexts\n",
        "    centers = mx.nd.array(centers, dtype=index_dtype)\n",
        "    contexts = mx.nd.sparse.csr_matrix(\n",
        "        (contexts_data, (contexts_row, contexts_col)),\n",
        "        dtype=dtype, shape=(len(centers), num_tokens))  # yapf: disable\n",
        "    return centers, contexts\n",
        "\n",
        "\n",
        "batch_size=4096\n",
        "data = nlp.data.SimpleDataStream([text8])  # input is a stream of datasets, here just 1. Allows scaling to larger corpora that don't fit in memory\n",
        "data, batchify_fn, subword_function = transform_data_fasttext(\n",
        "    data, \n",
        "    vocab, \n",
        "    idx_to_counts, \n",
        "    cbow=False, \n",
        "    ngrams=[3,4,5,6], \n",
        "    ngram_buckets=100000, \n",
        "    batch_size=batch_size, \n",
        "    window_size=5\n",
        ")\n",
        "\n",
        "\n",
        "batches = data.transform(batchify_fn)\n",
        "batches"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gluonnlp.data.stream._LazyTransformDataStream at 0x7fec07886da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D8ORTGemQ_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "54020f81-2685-43fe-c7c6-2cac371adf5d"
      },
      "source": [
        "idx_to_subwordidxs = subword_function(vocab.idx_to_token)\n",
        "for word, subwords in zip(vocab.idx_to_token[:3], idx_to_subwordidxs[:3]):\n",
        "    print('<'+word+'>', subwords, sep = '\\t')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<the>\t[51151, 9726, 48960, 61980, 60934, 16280]\n",
            "<of>\t[97102, 64528, 28930]\n",
            "<and>\t[78080, 35020, 30390, 95046, 19624, 25443]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5FCb0cumaJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "f81b6093-75f3-445c-c43b-1dda6b69e7b7"
      },
      "source": [
        "\"\"\"Word embedding models.\"\"\"\n",
        "\n",
        "import mxnet as mx\n",
        "import numpy as np\n",
        "\n",
        "import gluonnlp as nlp\n",
        "\n",
        "\n",
        "class Net(mx.gluon.HybridBlock):\n",
        "    \"\"\"Base class for word2vec and fastText SkipGram and CBOW networks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    token_to_idx : dict\n",
        "        token_to_idx mapping of the vocabulary that this model is to be trained\n",
        "        with. token_to_idx is used for __getitem__ and __contains__. For\n",
        "        len(token_to_idx) is used during initialization to obtain the input_dim\n",
        "        of the embedding matrix.\n",
        "    output_dim : int\n",
        "        Dimension of the dense embedding.\n",
        "    batch_size : int\n",
        "        Batchsize this model will be trained with. TODO temporary until\n",
        "        random_like ops are supported\n",
        "    negatives_weights : mxnet.nd.NDArray\n",
        "        Weights for UnigramCandidateSampler for sampling negatives.\n",
        "    smoothing : float, default 0.75\n",
        "        Smoothing factor applied to negatives_weights. Final weights are\n",
        "        mxnet.nd.power(negative_weights, smoothing).\n",
        "    num_negatives : int, default 5\n",
        "        Number of negatives to sample for each real sample.\n",
        "    sparse_grad : bool, default True\n",
        "        Specifies mxnet.gluon.nn.Embedding sparse_grad argument.\n",
        "    dtype : str, default 'float32'\n",
        "        dtype argument passed to gluon.nn.Embedding\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # pylint: disable=abstract-method\n",
        "    def __init__(self, token_to_idx, output_dim, batch_size, negatives_weights,\n",
        "                 subword_function=None, num_negatives=5, smoothing=0.75,\n",
        "                 sparse_grad=True, dtype='float32', **kwargs):\n",
        "        super(Net, self).__init__(**kwargs)\n",
        "\n",
        "        self._kwargs = dict(\n",
        "            input_dim=len(token_to_idx), output_dim=output_dim, dtype=dtype,\n",
        "            sparse_grad=sparse_grad, num_negatives=num_negatives)\n",
        "\n",
        "        with self.name_scope():\n",
        "            if subword_function is not None:\n",
        "                self.embedding = nlp.model.train.FasttextEmbeddingModel(\n",
        "                    token_to_idx=token_to_idx,\n",
        "                    subword_function=subword_function,\n",
        "                    output_dim=output_dim,\n",
        "                    weight_initializer=mx.init.Uniform(scale=1 / output_dim),\n",
        "                    sparse_grad=sparse_grad,\n",
        "                )\n",
        "            else:\n",
        "                self.embedding = nlp.model.train.CSREmbeddingModel(\n",
        "                    token_to_idx=token_to_idx,\n",
        "                    output_dim=output_dim,\n",
        "                    weight_initializer=mx.init.Uniform(scale=1 / output_dim),\n",
        "                    sparse_grad=sparse_grad,\n",
        "                )\n",
        "            self.embedding_out = mx.gluon.nn.Embedding(\n",
        "                len(token_to_idx), output_dim=output_dim,\n",
        "                weight_initializer=mx.init.Zero(), sparse_grad=sparse_grad,\n",
        "                dtype=dtype)\n",
        "\n",
        "            self.negatives_sampler = nlp.data.UnigramCandidateSampler(\n",
        "                weights=negatives_weights**smoothing, dtype='int64')\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        return self.embedding[tokens]\n",
        "\n",
        "\n",
        "class SG(Net):\n",
        "    \"\"\"SkipGram network\"\"\"\n",
        "\n",
        "    # pylint: disable=arguments-differ\n",
        "    def hybrid_forward(self, F, center, context, center_words):\n",
        "        \"\"\"SkipGram forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        center : mxnet.nd.NDArray or mxnet.sym.Symbol\n",
        "            Sparse CSR array of word / subword indices of shape (batch_size,\n",
        "            len(token_to_idx) + num_subwords). Embedding for center words are\n",
        "            computed via F.sparse.dot between the CSR center array and the\n",
        "            weight matrix.\n",
        "        context : mxnet.nd.NDArray or mxnet.sym.Symbol\n",
        "            Dense array of context words of shape (batch_size, ). Also used for\n",
        "            row-wise independently masking negatives equal to one of context.\n",
        "        center_words : mxnet.nd.NDArray or mxnet.sym.Symbol\n",
        "            Dense array of center words of shape (batch_size, ). Only used for\n",
        "            row-wise independently masking negatives equal to one of\n",
        "            center_words.\n",
        "        \"\"\"\n",
        "\n",
        "        # negatives sampling\n",
        "        negatives = []\n",
        "        mask = []\n",
        "        for _ in range(self._kwargs['num_negatives']):\n",
        "            negatives.append(self.negatives_sampler(center_words))\n",
        "            mask_ = negatives[-1] != center_words\n",
        "            mask_ = F.stack(mask_, (negatives[-1] != context))\n",
        "            mask.append(mask_.min(axis=0))\n",
        "\n",
        "        negatives = F.stack(*negatives, axis=1)\n",
        "        mask = F.stack(*mask, axis=1).astype(np.float32)\n",
        "\n",
        "        # center - context pairs\n",
        "        emb_center = self.embedding(center).expand_dims(1)\n",
        "        emb_context = self.embedding_out(context).expand_dims(2)\n",
        "        pred_pos = F.batch_dot(emb_center, emb_context).squeeze()\n",
        "        loss_pos = (F.relu(pred_pos) - pred_pos + F.Activation(-F.abs(pred_pos), act_type='softrelu')) / (mask.sum(axis=1) + 1)\n",
        "\n",
        "        # center - negatives pairs\n",
        "        emb_negatives = self.embedding_out(negatives).reshape((-1, self._kwargs['num_negatives'], self._kwargs['output_dim'])).swapaxes(1, 2)\n",
        "        pred_neg = F.batch_dot(emb_center, emb_negatives).squeeze()\n",
        "        mask = mask.reshape((-1, self._kwargs['num_negatives']))\n",
        "        loss_neg = (F.relu(pred_neg) + F.Activation(-F.abs(pred_neg), act_type='softrelu')) * mask\n",
        "        loss_neg = loss_neg.sum(axis=1) / (mask.sum(axis=1) + 1)\n",
        "\n",
        "        return loss_pos + loss_neg\n",
        "\n",
        "\n",
        "class CBOW(Net):\n",
        "    \"\"\"CBOW network\"\"\"\n",
        "\n",
        "    # pylint: disable=arguments-differ\n",
        "    def hybrid_forward(self, F, center, context):\n",
        "        \"\"\"CBOW forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        center : mxnet.nd.NDArray or mxnet.sym.Symbol\n",
        "            Dense array of center words of shape (batch_size, ).\n",
        "        context : mxnet.nd.NDArray or mxnet.sym.Symbol\n",
        "            Sparse CSR array of word / subword indices of shape (batch_size,\n",
        "            len(vocab) + num_subwords). Embedding for context words are\n",
        "            computed via F.sparse.dot between the CSR center array and the\n",
        "            weight matrix.\n",
        "\n",
        "        \"\"\"\n",
        "        # negatives sampling\n",
        "        negatives = []\n",
        "        mask = []\n",
        "        for _ in range(self._kwargs['num_negatives']):\n",
        "            negatives.append(self.negatives_sampler(center))\n",
        "            mask.append(negatives[-1] != center)\n",
        "\n",
        "        negatives = F.stack(*negatives, axis=1)\n",
        "        mask = F.stack(*mask, axis=1).astype(np.float32)\n",
        "\n",
        "        # context - center samples\n",
        "        emb_context = self.embedding(context).expand_dims(1)\n",
        "        emb_center = self.embedding_out(center).expand_dims(2)\n",
        "        pred_pos = F.batch_dot(emb_context, emb_center).squeeze()\n",
        "        loss_pos = (F.relu(pred_pos) - pred_pos + F.Activation(\n",
        "            -F.abs(pred_pos), act_type='softrelu')) / (mask.sum(axis=1) + 1)\n",
        "\n",
        "        # context - negatives samples\n",
        "        emb_negatives = self.embedding_out(negatives).reshape(\n",
        "            (-1, self._kwargs['num_negatives'],\n",
        "             self._kwargs['output_dim'])).swapaxes(1, 2)\n",
        "        pred_neg = F.batch_dot(emb_context, emb_negatives).squeeze()\n",
        "        mask = mask.reshape((-1, self._kwargs['num_negatives']))\n",
        "        loss_neg = (F.relu(pred_neg) + F.Activation(\n",
        "            -F.abs(pred_neg), act_type='softrelu')) * mask\n",
        "        loss_neg = loss_neg.sum(axis=1) / (mask.sum(axis=1) + 1)\n",
        "\n",
        "        return loss_pos + loss_neg\n",
        "\n",
        "emsize = 300\n",
        "num_negatives = 5\n",
        "\n",
        "negatives_weights = mx.nd.array(idx_to_counts)\n",
        "embedding = SG(\n",
        "    vocab.token_to_idx, \n",
        "    emsize, \n",
        "    batch_size, \n",
        "    negatives_weights, \n",
        "    subword_function, \n",
        "    num_negatives=5, \n",
        "    smoothing=0.75\n",
        ")\n",
        "embedding.initialize(ctx=context)\n",
        "embedding.hybridize()\n",
        "trainer = mx.gluon.Trainer(embedding.collect_params(), 'adagrad', dict(learning_rate=0.05))\n",
        "\n",
        "print(embedding)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SG(\n",
            "  (embedding): FasttextEmbeddingModel(71290 + 100000 -> 300, float32)\n",
            "  (embedding_out): Embedding(71290 -> 300, float32)\n",
            "  (negatives_sampler): UnigramCandidateSampler(71290, int64)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1L9jQrFmkKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "c8b395c3-9a21-41e7-881b-c9d57add252b"
      },
      "source": [
        "def norm_vecs_by_row(x):\n",
        "    return x / (mx.nd.sum(x * x, axis=1) + 1e-10).sqrt().reshape((-1, 1))\n",
        "\n",
        "\n",
        "def get_k_closest_tokens(vocab, embedding, k, word):\n",
        "    word_vec = norm_vecs_by_row(embedding[[word]])\n",
        "    vocab_vecs = norm_vecs_by_row(embedding[vocab.idx_to_token])\n",
        "    dot_prod = mx.nd.dot(vocab_vecs, word_vec.T)\n",
        "    indices = mx.nd.topk(\n",
        "        dot_prod.reshape((len(vocab.idx_to_token), )),\n",
        "        k=k + 1,\n",
        "        ret_typ='indices')\n",
        "    indices = [int(i.asscalar()) for i in indices]\n",
        "    result = [vocab.idx_to_token[i] for i in indices[1:]]\n",
        "    print('closest tokens to \"%s\": %s' % (word, \", \".join(result)))\n",
        "\n",
        "\n",
        "example_token = \"vector\"\n",
        "get_k_closest_tokens(vocab, embedding, 10, example_token)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "closest tokens to \"vector\": vectors, vectoring, bivector, sector, rector, lector, spector, director, vectorborne, hector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSwnpdEFmt5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "292a041a-4c68-4264-e217-70c186040537"
      },
      "source": [
        "log_interval = 500\n",
        "\n",
        "def train_embedding(num_epochs):\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start_time = time.time()\n",
        "        l_avg = 0\n",
        "        log_wc = 0\n",
        "\n",
        "        print('Beginning epoch %d and resampling data.' % epoch)\n",
        "        for i, batch in enumerate(batches):\n",
        "            batch = [array.as_in_context(context) for array in batch]\n",
        "            with mx.autograd.record():\n",
        "                l = embedding(*batch)\n",
        "            l.backward()\n",
        "            trainer.step(1)\n",
        "\n",
        "            l_avg += l.mean()\n",
        "            log_wc += l.shape[0]\n",
        "            if i % log_interval == 0:\n",
        "                mx.nd.waitall()\n",
        "                wps = log_wc / (time.time() - start_time)\n",
        "                l_avg = l_avg.asscalar() / log_interval\n",
        "                print('epoch %d, iteration %d, loss %.2f, throughput=%.2fK wps' % (epoch, i, l_avg, wps / 1000))\n",
        "                start_time = time.time()\n",
        "                log_wc = 0\n",
        "                l_avg = 0\n",
        "\n",
        "        get_k_closest_tokens(vocab, embedding, 10, example_token)\n",
        "        print(\"\")\n",
        "\n",
        "train_embedding(num_epochs=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning epoch 1 and resampling data.\n",
            "epoch 1, iteration 0, loss 0.00, throughput=0.40K wps\n",
            "epoch 1, iteration 500, loss 0.54, throughput=167.48K wps\n",
            "epoch 1, iteration 1000, loss 0.48, throughput=170.22K wps\n",
            "epoch 1, iteration 1500, loss 0.46, throughput=171.22K wps\n",
            "epoch 1, iteration 2000, loss 0.45, throughput=171.30K wps\n",
            "epoch 1, iteration 2500, loss 0.44, throughput=170.49K wps\n",
            "epoch 1, iteration 3000, loss 0.43, throughput=171.05K wps\n",
            "epoch 1, iteration 3500, loss 0.43, throughput=171.90K wps\n",
            "epoch 1, iteration 4000, loss 0.42, throughput=171.12K wps\n",
            "epoch 1, iteration 4500, loss 0.42, throughput=170.88K wps\n",
            "epoch 1, iteration 5000, loss 0.42, throughput=171.05K wps\n",
            "epoch 1, iteration 5500, loss 0.41, throughput=171.48K wps\n",
            "epoch 1, iteration 6000, loss 0.41, throughput=170.94K wps\n",
            "epoch 1, iteration 6500, loss 0.41, throughput=170.98K wps\n",
            "epoch 1, iteration 7000, loss 0.41, throughput=171.52K wps\n",
            "epoch 1, iteration 7500, loss 0.41, throughput=171.12K wps\n",
            "epoch 1, iteration 8000, loss 0.41, throughput=171.21K wps\n",
            "epoch 1, iteration 8500, loss 0.41, throughput=171.12K wps\n",
            "epoch 1, iteration 9000, loss 0.40, throughput=171.59K wps\n",
            "epoch 1, iteration 9500, loss 0.41, throughput=171.23K wps\n",
            "epoch 1, iteration 10000, loss 0.40, throughput=171.37K wps\n",
            "epoch 1, iteration 10500, loss 0.40, throughput=171.78K wps\n",
            "epoch 1, iteration 11000, loss 0.40, throughput=171.31K wps\n",
            "epoch 1, iteration 11500, loss 0.40, throughput=171.14K wps\n",
            "epoch 1, iteration 12000, loss 0.40, throughput=171.02K wps\n",
            "closest tokens to \"vector\": eigenvector, bivector, vectoring, functor, vectors, eigenvectors, vectra, parametric, polynomial, polynomials\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}