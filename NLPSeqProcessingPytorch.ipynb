{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPSeqProcessingPytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOseCVj6I2qa9vv9sDfBYiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35d5d833fb9b41f794714d4bb7793b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4903967689584a90bb3f6db6aa716dba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e3c408c935b47139943729cbcb05f01",
              "IPY_MODEL_f4dc0079dbae4fbb8a6416ce2b09fab1"
            ]
          }
        },
        "4903967689584a90bb3f6db6aa716dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e3c408c935b47139943729cbcb05f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b698311e321d41fa8e6c6378e964ab66",
            "_dom_classes": [],
            "description": "training routine: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a5c2d41d9c74866b7fbc8acc3be0b85"
          }
        },
        "f4dc0079dbae4fbb8a6416ce2b09fab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8e15b622aae7439581719467bd7f6cc4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30/30 [02:37&lt;00:00,  5.26s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2816bd258ef147a6ade33f55badf4cc4"
          }
        },
        "b698311e321d41fa8e6c6378e964ab66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a5c2d41d9c74866b7fbc8acc3be0b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e15b622aae7439581719467bd7f6cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2816bd258ef147a6ade33f55badf4cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e26c73232f0746bc87b775c38a5b3ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5d0df90095614ac4840c4d6344c15c0b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1699d12b4ca543de9b6f54a0608a9672",
              "IPY_MODEL_cd5a6d5a5de94887913d3615781623e2"
            ]
          }
        },
        "5d0df90095614ac4840c4d6344c15c0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1699d12b4ca543de9b6f54a0608a9672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_99b38b64a246424e8571d773604c3350",
            "_dom_classes": [],
            "description": "split=train:   2%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 7680,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 119,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c56241724fa449458732e1c833b94cb3"
          }
        },
        "cd5a6d5a5de94887913d3615781623e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c77a4625f4a44adb88e969b1e5050e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 119/7680 [02:36&lt;10:17:12,  4.90s/it, acc=27.1, epoch=29, loss=2.89]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb227e81cd51468a83dd356068df24de"
          }
        },
        "99b38b64a246424e8571d773604c3350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c56241724fa449458732e1c833b94cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c77a4625f4a44adb88e969b1e5050e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb227e81cd51468a83dd356068df24de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b094e0e289e9442e8b0604ff1b439c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b7cf46a17a98426cb57343f2543f1843",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0fbf1d52463b4dec834a1abe89814290",
              "IPY_MODEL_3db12946a8b54fb98ac6454b41bde097"
            ]
          }
        },
        "b7cf46a17a98426cb57343f2543f1843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fbf1d52463b4dec834a1abe89814290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce14814d92db405ea2895852f1374e59",
            "_dom_classes": [],
            "description": "split=val:   1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1640,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 24,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee54e379e2c44ace9dc2336a171aff0e"
          }
        },
        "3db12946a8b54fb98ac6454b41bde097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e0aa197592f4e14a454467592531a6e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 24/1640 [02:37&lt;2:22:42,  5.30s/it, acc=26.9, epoch=29, loss=2.89]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_440c6f95ad874932ac047767987c05ef"
          }
        },
        "ce14814d92db405ea2895852f1374e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee54e379e2c44ace9dc2336a171aff0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e0aa197592f4e14a454467592531a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "440c6f95ad874932ac047767987c05ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikramkrishnan9885/MyColab/blob/master/NLPSeqProcessingPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APOv64oL_m0I",
        "colab_type": "text"
      },
      "source": [
        "| Algorithm | Description | Applications |\n",
        "|-----|----|----|\n",
        "| One-to-one RNNs | These take a single input and give a single output. Current input depends on the previously observed input(s). | Stock market prediction, scene classification, and text generation |\n",
        "| One-to-many RNNs | These take a single input and give an output consisting of an arbitrary number of elements| Image captioning |\n",
        "| Many-to-one RNNs | These take a sequence of inputs and give a single output. | Sentence classification (considering a single word as a single input) |\n",
        "| Many-to-many RNNs | These take a sequence of arbitrary length as inputs and outputs a sequence of arbitrary length. | Machine translation, chatbots |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpepG7TzNfEy",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2XA9VHHNea_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "def progress_bar(some_iter):\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "        return tqdm(some_iter)\n",
        "    except ModuleNotFoundError:\n",
        "        return some_iter\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    print(\"Trying to fetch {}\".format(destination))\n",
        "\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in progress_bar(response.iter_content(CHUNK_SIZE)):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldHdriiRNPxh",
        "colab_type": "text"
      },
      "source": [
        "# Surnames classification problem\n",
        "\n",
        "## Data download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txRZdSoSJ2P5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "3f568cae-8bce-48a4-ca35-236a1ef985ce"
      },
      "source": [
        "file_id_0 = '1MBiOU5UCaGpJw2keXAqOLL8PCJg_uZaU'\n",
        "destination_0 = 'surnames.csv'\n",
        "download_file_from_google_drive(file_id_0, destination_0)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trying to fetch surnames.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "6it [00:00, 1219.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01UB20U3KI8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "9f9cbc89-d3af-4887-f1b5-cb653fd9bef8"
      },
      "source": [
        "file_id_1 = '1T1la2tYO1O7XkMRawG8VcFcvtjbxDqU-'\n",
        "destination_1 = 'surnames_with_splits.csv'\n",
        "download_file_from_google_drive(file_id_1, destination_1)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trying to fetch surnames_with_splits.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "8it [00:00, 1530.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vfKAAmgNTu5",
        "colab_type": "text"
      },
      "source": [
        "## Data munging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csAn3Bd4KVU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from argparse import Namespace"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38HF4KSmKfPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = Namespace(\n",
        "    raw_dataset_csv=\"surnames.csv\",\n",
        "    train_proportion=0.7,\n",
        "    val_proportion=0.15,\n",
        "    test_proportion=0.15,\n",
        "    output_munged_csv=\"surnames_with_splits.csv\",\n",
        "    seed=1337\n",
        ")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKME7CAVKjW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "51fe005e-e83a-4d77-ca84-e20332a19443"
      },
      "source": [
        "# Read raw data\n",
        "surnames = pd.read_csv(args.raw_dataset_csv, header=0)\n",
        "surnames.head()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woodford</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CotÃ©</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kore</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Koury</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lebzak</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality\n",
              "0  Woodford     English\n",
              "1      CotÃ©      French\n",
              "2      Kore     English\n",
              "3     Koury      Arabic\n",
              "4    Lebzak     Russian"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i59TxjcJK4Sp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "9ce4ef81-775b-4ab4-c057-b736d01bd64e"
      },
      "source": [
        "# Unique classes\n",
        "set(surnames.nationality)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Arabic',\n",
              " 'Chinese',\n",
              " 'Czech',\n",
              " 'Dutch',\n",
              " 'English',\n",
              " 'French',\n",
              " 'German',\n",
              " 'Greek',\n",
              " 'Irish',\n",
              " 'Italian',\n",
              " 'Japanese',\n",
              " 'Korean',\n",
              " 'Polish',\n",
              " 'Portuguese',\n",
              " 'Russian',\n",
              " 'Scottish',\n",
              " 'Spanish',\n",
              " 'Vietnamese'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ody-v48pK5Bz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "a69a56f1-6baf-4b68-92d6-c37905a5f184"
      },
      "source": [
        "# Splitting train by nationality\n",
        "# Create dict\n",
        "by_nationality = collections.defaultdict(list)\n",
        "for _, row in surnames.iterrows():\n",
        "    by_nationality[row.nationality].append(row.to_dict())\n",
        "\n",
        "\n",
        "# Create split data\n",
        "final_list = []\n",
        "np.random.seed(args.seed)\n",
        "for _, item_list in sorted(by_nationality.items()):\n",
        "    np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_proportion*n)\n",
        "    n_val = int(args.val_proportion*n)\n",
        "    n_test = int(args.test_proportion*n)\n",
        "    \n",
        "    # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "    \n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)\n",
        "\n",
        "\n",
        "# Write split data to file\n",
        "final_surnames = pd.DataFrame(final_list)\n",
        "\n",
        "\n",
        "final_surnames.split.value_counts()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    7680\n",
              "test     1660\n",
              "val      1640\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8EvxeJjLACv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f3477b0e-19f4-4f41-93e5-e91f8691c69b"
      },
      "source": [
        "final_surnames.head()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Totah</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Abboud</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fakhoury</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Srour</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sayegh</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality  split\n",
              "0     Totah      Arabic  train\n",
              "1    Abboud      Arabic  train\n",
              "2  Fakhoury      Arabic  train\n",
              "3     Srour      Arabic  train\n",
              "4    Sayegh      Arabic  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmDQ-XN0LCpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write munged data to CSV\n",
        "final_surnames.to_csv(args.output_munged_csv, index=False)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxqoOUU3NsXQ",
        "colab_type": "text"
      },
      "source": [
        "## Data processing using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP0SQgq7Nvzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4842jup3t-j",
        "colab_type": "text"
      },
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3-N4A2h4E6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
        "\n",
        "    def __init__(self, token_to_idx=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
        "        \"\"\"\n",
        "\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token \n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "        \n",
        "    def to_serializable(self):\n",
        "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\"Update mapping dicts based on the token.\n",
        "\n",
        "        Args:\n",
        "            token (str): the item to add into the Vocabulary\n",
        "        Returns:\n",
        "            index (int): the integer corresponding to the token\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "            \n",
        "    def add_many(self, tokens):\n",
        "        \"\"\"Add a list of tokens into the Vocabulary\n",
        "        \n",
        "        Args:\n",
        "            tokens (list): a list of string tokens\n",
        "        Returns:\n",
        "            indices (list): a list of indices corresponding to the tokens\n",
        "        \"\"\"\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        \"\"\"\n",
        "        return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\"Return the token associated with the index\n",
        "        \n",
        "        Args: \n",
        "            index (int): the index to look up\n",
        "        Returns:\n",
        "            token (str): the token corresponding to the index\n",
        "        Raises:\n",
        "            KeyError: if the index is not in the Vocabulary\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQMGuUEt4MPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self._mask_token = mask_token\n",
        "        self._unk_token = unk_token\n",
        "        self._begin_seq_token = begin_seq_token\n",
        "        self._end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self._mask_token)\n",
        "        self.unk_index = self.add_token(self._unk_token)\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self._unk_token,\n",
        "                         'mask_token': self._mask_token,\n",
        "                         'begin_seq_token': self._begin_seq_token,\n",
        "                         'end_seq_token': self._end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "          or the UNK index if token isn't present.\n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        Notes:\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
        "              for the UNK functionality \n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ATHVzxD4cuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SurnameVectorizer(object):\n",
        "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"   \n",
        "    def __init__(self, char_vocab, nationality_vocab):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            char_vocab (Vocabulary): maps characters to integers\n",
        "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
        "        \"\"\"\n",
        "        self.char_vocab = char_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "\n",
        "    def vectorize(self, surname, vector_length=-1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            title (str): the string of characters\n",
        "            vector_length (int): an argument for forcing the length of index vector\n",
        "        \"\"\"\n",
        "        indices = [self.char_vocab.begin_seq_index]\n",
        "        indices.extend(self.char_vocab.lookup_token(token) \n",
        "                       for token in surname)\n",
        "        indices.append(self.char_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)         \n",
        "        out_vector[:len(indices)] = indices\n",
        "        out_vector[len(indices):] = self.char_vocab.mask_index\n",
        "        \n",
        "        return out_vector, len(indices)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, surname_df):\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
        "        \n",
        "        Args:\n",
        "            surname_df (pandas.DataFrame): the surnames dataset\n",
        "        Returns:\n",
        "            an instance of the SurnameVectorizer\n",
        "        \"\"\"\n",
        "        char_vocab = SequenceVocabulary()\n",
        "        nationality_vocab = Vocabulary()\n",
        "\n",
        "        for index, row in surname_df.iterrows():\n",
        "            for char in row.surname:\n",
        "                char_vocab.add_token(char)\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "\n",
        "        return cls(char_vocab, nationality_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
        "        nat_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "\n",
        "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'char_vocab': self.char_vocab.to_serializable(), \n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable()}\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s_w1jCl4qpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, surname_df, vectorizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            surname_df (pandas.DataFrame): the dataset\n",
        "            vectorizer (SurnameVectorizer): vectorizer instatiated from dataset\n",
        "        \"\"\"\n",
        "        self.surname_df = surname_df \n",
        "        self._vectorizer = vectorizer\n",
        "\n",
        "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
        "\n",
        "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                             'val': (self.val_df, self.validation_size), \n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "        \n",
        "        # Class weights\n",
        "        class_counts = self.train_df.nationality.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
        "        \n",
        "        Args:\n",
        "            surname_csv (str): location of the dataset\n",
        "        Returns:\n",
        "            an instance of SurnameDataset\n",
        "        \"\"\"\n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        train_surname_df = surname_df[surname_df.split=='train']\n",
        "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
        "        \"\"\"Load dataset and the corresponding vectorizer. \n",
        "        Used in the case in the vectorizer has been cached for re-use\n",
        "        \n",
        "        Args:\n",
        "            surname_csv (str): location of the dataset\n",
        "            vectorizer_filepath (str): location of the saved vectorizer\n",
        "        Returns:\n",
        "            an instance of SurnameDataset\n",
        "        \"\"\"\n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(surname_df, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        \"\"\"a static method for loading the vectorizer from file\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
        "        Returns:\n",
        "            an instance of SurnameVectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        \"\"\"saves the vectorizer to disk using json\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location to save the vectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" returns the vectorizer \"\"\"\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        \n",
        "        Args:\n",
        "            index (int): the index to the data point \n",
        "        Returns:\n",
        "            a dictionary holding the data point's:\n",
        "                features (x_data)\n",
        "                label (y_target)\n",
        "                feature length (x_length)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "        \n",
        "        surname_vector, vec_length = \\\n",
        "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "        \n",
        "        nationality_index = \\\n",
        "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "\n",
        "        return {'x_data': surname_vector, \n",
        "                'y_target': nationality_index, \n",
        "                'x_length': vec_length}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int)\n",
        "        Returns:\n",
        "            number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) # batch_size"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BT9nn4M43PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\"): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5rajTvC45Zv",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKEgZDr45EsN",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9lk8E1p5CK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def column_gather(y_out, x_lengths):\n",
        "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
        "\n",
        "    More precisely, iterate over batch row indices, get the vector that's at\n",
        "    the position indicated by the corresponding value in `x_lengths` at the row\n",
        "    index.\n",
        "\n",
        "    Args:\n",
        "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
        "            shape: (batch, sequence, feature)\n",
        "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
        "            shape: (batch,)\n",
        "\n",
        "    Returns:\n",
        "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
        "            shape: (batch, feature)\n",
        "    '''\n",
        "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
        "\n",
        "    out = []\n",
        "    for batch_index, column_index in enumerate(x_lengths):\n",
        "        out.append(y_out[batch_index, column_index])\n",
        "\n",
        "    return torch.stack(out)\n",
        "\n",
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVW25uO5SmH",
        "colab_type": "text"
      },
      "source": [
        "### Classifer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePINLFR45d70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ElmanRNN(nn.Module):\n",
        "    \"\"\" an Elman RNN built using the RNNCell \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): size of the input vectors\n",
        "            hidden_size (int): size of the hidden state vectors\n",
        "            bathc_first (bool): whether the 0th dimension is batch\n",
        "        \"\"\"\n",
        "        super(ElmanRNN, self).__init__()\n",
        "        \n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "        \n",
        "        self.batch_first = batch_first\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def _initial_hidden(self, batch_size):\n",
        "        return torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "    def forward(self, x_in, initial_hidden=None):\n",
        "        \"\"\"The forward pass of the ElmanRNN\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\n",
        "                Else: x_in.shape = (seq_size, batch, feat_size)\n",
        "            initial_hidden (torch.Tensor): the initial hidden state for the RNN\n",
        "        Returns:\n",
        "            hiddens (torch.Tensor): The outputs of the RNN at each time step. \n",
        "                If self.batch_first: hiddens.shape = (batch, seq_size, hidden_size)\n",
        "                Else: hiddens.shape = (seq_size, batch, hidden_size)\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            batch_size, seq_size, feat_size = x_in.size()\n",
        "            x_in = x_in.permute(1, 0, 2)\n",
        "        else:\n",
        "            seq_size, batch_size, feat_size = x_in.size()\n",
        "    \n",
        "        hiddens = []\n",
        "\n",
        "        if initial_hidden is None:\n",
        "            initial_hidden = self._initial_hidden(batch_size)\n",
        "            initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "        hidden_t = initial_hidden\n",
        "                    \n",
        "        for t in range(seq_size):\n",
        "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
        "            hiddens.append(hidden_t)\n",
        "            \n",
        "        hiddens = torch.stack(hiddens)\n",
        "\n",
        "        if self.batch_first:\n",
        "            hiddens = hiddens.permute(1, 0, 2)\n",
        "\n",
        "        return hiddens"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqF84mFx5fxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A Classifier with an RNN to extract features and an MLP to classify \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): The size of the character embeddings\n",
        "            num_embeddings (int): The number of characters to embed\n",
        "            num_classes (int): The size of the prediction vector \n",
        "                Note: the number of nationalities\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will \n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding; \n",
        "                see torch.nn.Embedding\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                embedding_dim=embedding_size,\n",
        "                                padding_idx=padding_idx)\n",
        "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
        "                             hidden_size=rnn_hidden_size,\n",
        "                             batch_first=batch_first)\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                         out_features=rnn_hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                          out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            x_lengths (torch.Tensor): the lengths of each sequence in the batch.\n",
        "                They are used to find the final vector of each sequence\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        x_embedded = self.emb(x_in)\n",
        "        y_out = self.rnn(x_embedded)\n",
        "\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHLeW11S7sSA",
        "colab_type": "text"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pacHnn77zkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "70d29b6b-d3aa-4d85-82c9-1f2cf3230589"
      },
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"./\",\n",
        "    # Model hyper parameter\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # Training hyper parameter\n",
        "    num_epochs=30,\n",
        "    learning_rate=2e-2,\n",
        "    batch_size=64,\n",
        "    seed=1337,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime hyper parameter\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    \n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)\n",
        "\n",
        "\n",
        "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv, \n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = SurnameClassifier(embedding_size=args.char_embedding_size, \n",
        "                               num_embeddings=len(vectorizer.char_vocab),\n",
        "                               num_classes=len(vectorizer.nationality_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index)\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZodvW9W50kw",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSj6D_PK6K4j",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX_9J6SL6SGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_train_state(args):\n",
        "    return {'stop_early': False,\n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'learning_rate': args.learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': args.model_state_file}\n",
        "\n",
        "\n",
        "def update_train_state(args, model, train_state):\n",
        "    \"\"\"Handle the training state updates.\n",
        "\n",
        "    Components:\n",
        "     - Early Stopping: Prevent overfitting.\n",
        "     - Model Checkpoint: Model is saved if the model is better\n",
        "    \n",
        "    :param args: main arguments\n",
        "    :param model: model to train\n",
        "    :param train_state: a dictionary representing the training state values\n",
        "    :returns:\n",
        "        a new train_state\n",
        "    \"\"\"\n",
        "\n",
        "    # Save one model at least\n",
        "    if train_state['epoch_index'] == 0:\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\n",
        "        train_state['stop_early'] = False\n",
        "\n",
        "    # Save model if performance improved\n",
        "    elif train_state['epoch_index'] >= 1:\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
        "         \n",
        "        # If loss worsened\n",
        "        if loss_t >= loss_tm1:\n",
        "            # Update step\n",
        "            train_state['early_stopping_step'] += 1\n",
        "        # Loss decreased\n",
        "        else:\n",
        "            # Save the best model\n",
        "            if loss_t < train_state['early_stopping_best_val']:\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\n",
        "                train_state['early_stopping_best_val'] = loss_t\n",
        "\n",
        "            # Reset early stopping step\n",
        "            train_state['early_stopping_step'] = 0\n",
        "\n",
        "        # Stop early ?\n",
        "        train_state['stop_early'] = \\\n",
        "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
        "\n",
        "    return train_state\n",
        "\n",
        "\n",
        "def compute_accuracy(y_pred, y_target):\n",
        "    _, y_pred_indices = y_pred.max(dim=1)\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZd7e7Ju6XAy",
        "colab_type": "text"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad1G21AO6f2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253,
          "referenced_widgets": [
            "35d5d833fb9b41f794714d4bb7793b07",
            "4903967689584a90bb3f6db6aa716dba",
            "4e3c408c935b47139943729cbcb05f01",
            "f4dc0079dbae4fbb8a6416ce2b09fab1",
            "b698311e321d41fa8e6c6378e964ab66",
            "4a5c2d41d9c74866b7fbc8acc3be0b85",
            "8e15b622aae7439581719467bd7f6cc4",
            "2816bd258ef147a6ade33f55badf4cc4",
            "e26c73232f0746bc87b775c38a5b3ace",
            "5d0df90095614ac4840c4d6344c15c0b",
            "1699d12b4ca543de9b6f54a0608a9672",
            "cd5a6d5a5de94887913d3615781623e2",
            "99b38b64a246424e8571d773604c3350",
            "c56241724fa449458732e1c833b94cb3",
            "9c77a4625f4a44adb88e969b1e5050e7",
            "cb227e81cd51468a83dd356068df24de",
            "b094e0e289e9442e8b0604ff1b439c62",
            "b7cf46a17a98426cb57343f2543f1843",
            "0fbf1d52463b4dec834a1abe89814290",
            "3db12946a8b54fb98ac6454b41bde097",
            "ce14814d92db405ea2895852f1374e59",
            "ee54e379e2c44ace9dc2336a171aff0e",
            "2e0aa197592f4e14a454467592531a6e",
            "440c6f95ad874932ac047767987c05ef"
          ]
        },
        "outputId": "62b088b1-1553-48fd-a862-5878d3ca115d"
      },
      "source": [
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "    \n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                           mode='min', factor=0.5,\n",
        "                                           patience=1)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm_notebook(desc='training routine', \n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm_notebook(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size), \n",
        "                          position=1, \n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm_notebook(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size), \n",
        "                        position=1, \n",
        "                        leave=True)\n",
        "\n",
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # Iterate over training dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        classifier.train()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # the training routine is these 5 steps:\n",
        "\n",
        "            # --------------------------------------    \n",
        "            # step 1. zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # step 2. compute the output\n",
        "            y_pred = classifier(x_in=batch_dict['x_data'], \n",
        "                                x_lengths=batch_dict['x_length'])\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    \n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # step 4. use loss to produce gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # step 5. use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # update bar\n",
        "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # Iterate over val dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        classifier.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            y_pred = classifier(x_in=batch_dict['x_data'], \n",
        "                                x_lengths=batch_dict['x_length'])\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
        "            val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=classifier, \n",
        "                                         train_state=train_state)\n",
        "\n",
        "        scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "\n",
        "        if train_state['stop_early']:\n",
        "            break\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print(\"Exiting loop\")\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35d5d833fb9b41f794714d4bb7793b07",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='training routine', max=30.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e26c73232f0746bc87b775c38a5b3ace",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=train', max=7680.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b094e0e289e9442e8b0604ff1b439c62",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=val', max=1640.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j42zBPxd6nQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c897ed4-c986-4e6d-83f1-21ff7018248c"
      },
      "source": [
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_data'],\n",
        "                         x_lengths=batch_dict['x_length'])\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc\n",
        "\n",
        "\n",
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 2.891194667816162;\n",
            "Test Accuracy: 27.312499999999996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFV1y_8i6y7e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q1jEcS9615I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_nationality(surname, classifier, vectorizer):\n",
        "    vectorized_surname, vec_length = vectorizer.vectorize(surname)\n",
        "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
        "    vec_length = torch.tensor([vec_length], dtype=torch.int64)\n",
        "    \n",
        "    result = classifier(vectorized_surname, vec_length, apply_softmax=True)\n",
        "    probability_values, indices = result.max(dim=1)\n",
        "    \n",
        "    index = indices.item()\n",
        "    prob_value = probability_values.item()\n",
        "\n",
        "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "\n",
        "    return {'nationality': predicted_nationality, 'probability': prob_value, 'surname': surname}"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6tSUaM0643v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702bb925-147b-4c90-f519-e4fe139cc237"
      },
      "source": [
        "# surname = input(\"Enter a surname: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
        "    print(predict_nationality(surname, classifier, vectorizer))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'nationality': 'English', 'probability': 0.0586300790309906, 'surname': 'McMahan'}\n",
            "{'nationality': 'English', 'probability': 0.0586300790309906, 'surname': 'Nakamoto'}\n",
            "{'nationality': 'English', 'probability': 0.0586300790309906, 'surname': 'Wan'}\n",
            "{'nationality': 'English', 'probability': 0.0586300790309906, 'surname': 'Cho'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN2OG2fI7xSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 99,
      "outputs": []
    }
  ]
}