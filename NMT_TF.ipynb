{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT-TF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOczIyQbLl2g7aNx3Gm8KJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikramkrishnan9885/MyColab/blob/master/NMT_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jrg9r7QSLJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "4f3dbe92-bfa6-490c-b9cc-c8f0bb235089"
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: six, termcolor, protobuf, google-pasta, keras-preprocessing, tensorboard, wrapt, numpy, wheel, absl-py, keras-applications, astor, opt-einsum, tensorflow-estimator, grpcio, gast\n",
            "Required-by: fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "592_0sD1SXky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "32084007-d347-499e-8570-47e000eec34a"
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGYRTvZRSi-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46db3a75-9568-4a3c-9a63-62c2586a3008"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 35kB/s \n",
            "y\n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.30.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (49.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=bb25ced7a722a3c4eea0505b4c29c1c6a554852bde03ff0c9f4492ff48c8d3d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoXHD3yNPbRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "66449f3f-e97d-4521-b637-421dc2b61cf2"
      },
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import zipfile\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "#import tensorflow as tf\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import csv\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import nltk\n",
        "\n",
        "import urllib.request"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAUAycJUItC7",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions for word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DA03ggcIsKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_cursors = None\n",
        "tot_sentences = None\n",
        "src_max_sent_length, tgt_max_sent_length = 0, 0\n",
        "src_dictionary, tgt_dictionary = {}, {}\n",
        "src_reverse_dictionary, tgt_reverse_dictionary = {},{}\n",
        "train_inputs, train_outputs = None, None\n",
        "embedding_size = None # Dimension of the embedding vector.\n",
        "vocabulary_size = None\n",
        "\n",
        "def define_data_and_hyperparameters(\n",
        "        _tot_sentences, _src_max, _tgt_max, _src_dict, _tgt_dict,\n",
        "        _src_rev_dict, _tgt_rev_dict, _tr_inp, _tr_out, _emb_size, _vocab_size):\n",
        "    global tot_sentences, sentence_cursors\n",
        "    global src_max_sent_length, tgt_max_sent_length\n",
        "    global src_dictionary, tgt_dictionary\n",
        "    global src_reverse_dictionary, tgt_reverse_dictionary\n",
        "    global train_inputs, train_outputs\n",
        "    global embedding_size, vocabulary_size\n",
        "\n",
        "    embedding_size = _emb_size\n",
        "    vocabulary_size = _vocab_size\n",
        "    src_max_sent_length, tgt_max_sent_length = _src_max, _tgt_max\n",
        "\n",
        "    src_dictionary = _src_dict\n",
        "    tgt_dictionary = _tgt_dict\n",
        "\n",
        "    src_reverse_dictionary = _src_rev_dict\n",
        "    tgt_reverse_dictionary = _tgt_rev_dict\n",
        "\n",
        "    train_inputs = _tr_inp\n",
        "    train_outputs = _tr_out\n",
        "\n",
        "    tot_sentences = _tot_sentences\n",
        "    sentence_cursors = [0 for _ in range(tot_sentences)]\n",
        "\n",
        "\n",
        "def generate_batch_for_word2vec(batch_size, window_size, is_source):\n",
        "    # window_size is the amount of words we're looking at from each side of a given word\n",
        "    # creates a single batch\n",
        "    global sentence_cursors\n",
        "    global src_dictionary, tgt_dictionary\n",
        "    global train_inputs, train_outputs\n",
        "    span = 2 * window_size + 1  # [ skip_window target skip_window ]\n",
        "\n",
        "    batch = np.ndarray(shape=(batch_size, span - 1), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    # e.g if skip_window = 2 then span = 5\n",
        "    # span is the length of the whole frame we are considering for a single word (left + word + right)\n",
        "    # skip_window is the length of one side\n",
        "\n",
        "    sentence_ids_for_batch = np.random.randint(0, tot_sentences, batch_size)\n",
        "\n",
        "    for b_i in range(batch_size):\n",
        "        sent_id = sentence_ids_for_batch[b_i]\n",
        "\n",
        "        if is_source:\n",
        "            buffer = train_inputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n",
        "        else:\n",
        "            buffer = train_outputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n",
        "        assert buffer.size == span, 'Buffer length (%d), Current data index (%d), Span(%d)' % (\n",
        "        buffer.size, sentence_cursors[sent_id], span)\n",
        "        # If we only have EOS tokesn in the sampled text, we sample a new one\n",
        "        if is_source:\n",
        "            while np.all(buffer == src_dictionary['</s>']):\n",
        "                # reset the sentence_cursors for that cap_id\n",
        "                sentence_cursors[sent_id] = 0\n",
        "                # sample a new cap_id\n",
        "                sent_id = np.random.randint(0, tot_sentences)\n",
        "                buffer = train_inputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n",
        "        else:\n",
        "            while np.all(buffer == tgt_dictionary['</s>']):\n",
        "                # reset the sentence_cursors for that cap_id\n",
        "                sentence_cursors[sent_id] = 0\n",
        "                # sample a new cap_id\n",
        "                sent_id = np.random.randint(0, tot_sentences)\n",
        "                buffer = train_outputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n",
        "\n",
        "        # fill left and right sides of batch\n",
        "        batch[b_i, :window_size] = buffer[:window_size]\n",
        "        batch[b_i, window_size:] = buffer[window_size + 1:]\n",
        "\n",
        "        labels[b_i, 0] = buffer[window_size]\n",
        "\n",
        "        # increase the corresponding index\n",
        "        if is_source:\n",
        "            sentence_cursors[sent_id] = (sentence_cursors[sent_id] + 1) % (src_max_sent_length - span)\n",
        "        else:\n",
        "            sentence_cursors[sent_id] = (sentence_cursors[sent_id] + 1) % (tgt_max_sent_length - span)\n",
        "\n",
        "    assert batch.shape[0] == batch_size and batch.shape[1] == span - 1\n",
        "    return batch, labels\n",
        "\n",
        "\n",
        "def print_some_batches():\n",
        "    global sentence_cursors, tot_sentences\n",
        "    global src_reverse_dictionary\n",
        "\n",
        "    for window_size in [1, 2]:\n",
        "        sentence_cursors = [0 for _ in range(tot_sentences)]\n",
        "        batch, labels = generate_batch_for_word2vec(batch_size=8, window_size=window_size, is_source=True)\n",
        "        print('\\nwith window_size = %d:' % (window_size))\n",
        "        print('    batch:', [[src_reverse_dictionary[bii] for bii in bi] for bi in batch])\n",
        "        print('    labels:', [src_reverse_dictionary[li] for li in labels.reshape(8)])\n",
        "\n",
        "    sentence_cursors = [0 for _ in range(tot_sentences)]\n",
        "\n",
        "batch_size, window_size = None, None\n",
        "valid_size, valid_window, valid_examples = None, None, None\n",
        "num_sampled = None\n",
        "\n",
        "train_dataset, train_labels = None, None\n",
        "valid_dataset = None\n",
        "\n",
        "softmax_weights, softmax_biases = None, None\n",
        "\n",
        "loss, optimizer, similarity, normalized_embeddings = None, None, None, None\n",
        "\n",
        "def define_word2vec_tensorflow(batch_size):\n",
        "\n",
        "    global embedding_size, window_size\n",
        "    global\tvalid_size, valid_window, valid_examples\n",
        "    global num_sampled\n",
        "    global train_dataset, train_labels\n",
        "    global valid_dataset\n",
        "    global softmax_weights, softmax_biases\n",
        "    global loss, optimizer, similarity\n",
        "    global vocabulary_size, embedding_size\n",
        "    global normalized_embeddings\n",
        "\n",
        "\n",
        "    window_size = 2  # How many words to consider left and right.\n",
        "    # We pick a random validation set to sample nearest neighbors. here we limit the\n",
        "    # validation samples to the words that have a low numeric ID, which by\n",
        "    # construction are also the most frequent.\n",
        "    valid_size = 20  # Random set of words to evaluate similarity on.\n",
        "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "    # pick 16 samples from 100\n",
        "    valid_examples = np.array(np.random.randint(0, valid_window, valid_size // 2))\n",
        "    valid_examples = np.append(valid_examples, np.random.randint(1000, 1000 + valid_window, valid_size // 2))\n",
        "    num_sampled = 32  # Number of negative examples to sample.\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "\n",
        "    # Input data.\n",
        "    train_dataset = tf.compat.v1.placeholder(tf.compat.v1.int32, shape=[batch_size, 2 * window_size])\n",
        "    train_labels = tf.compat.v1.placeholder(tf.compat.v1.int32, shape=[batch_size, 1])\n",
        "    valid_dataset = tf.compat.v1.constant(valid_examples, dtype=tf.compat.v1.int32)\n",
        "\n",
        "    # Variables.\n",
        "    # embedding, vector for each word in the vocabulary\n",
        "    embeddings = tf.compat.v1.Variable(tf.compat.v1.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.compat.v1.float32))\n",
        "    softmax_weights = tf.compat.v1.Variable(tf.compat.v1.truncated_normal([vocabulary_size, embedding_size],\n",
        "                                                      stddev=1.0 / math.sqrt(embedding_size), dtype=tf.compat.v1.float32))\n",
        "    softmax_biases = tf.compat.v1.Variable(tf.compat.v1.zeros([vocabulary_size], dtype=tf.compat.v1.float32))\n",
        "\n",
        "    # Model.\n",
        "    # Look up embeddings for inputs.\n",
        "    # this might efficiently find the embeddings for given ids (traind dataset)\n",
        "    # manually doing this might not be efficient given there are 50000 entries in embeddings\n",
        "    stacked_embedings = None\n",
        "    print('Defining %d embedding lookups representing each word in the context' % (2 * window_size))\n",
        "    for i in range(2 * window_size):\n",
        "        embedding_i = tf.compat.v1.nn.embedding_lookup(embeddings, train_dataset[:, i])\n",
        "        x_size, y_size = embedding_i.get_shape().as_list()\n",
        "        if stacked_embedings is None:\n",
        "            stacked_embedings = tf.compat.v1.reshape(embedding_i, [x_size, y_size, 1])\n",
        "        else:\n",
        "            stacked_embedings = tf.compat.v1.concat(axis=2,\n",
        "                                          values=[stacked_embedings, tf.compat.v1.reshape(embedding_i, [x_size, y_size, 1])])\n",
        "\n",
        "    assert stacked_embedings.get_shape().as_list()[2] == 2 * window_size\n",
        "    print(\"Stacked embedding size: %s\" % stacked_embedings.get_shape().as_list())\n",
        "    mean_embeddings = tf.compat.v1.reduce_mean(stacked_embedings, 2, keepdims=False)\n",
        "    print(\"Reduced mean embedding size: %s\" % mean_embeddings.get_shape().as_list())\n",
        "\n",
        "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
        "    # inputs are embeddings of the train words\n",
        "    # with this loss we optimize weights, biases, embeddings\n",
        "\n",
        "    loss = tf.compat.v1.reduce_mean(\n",
        "        tf.compat.v1.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=mean_embeddings,\n",
        "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
        "\n",
        "    # Optimizer.\n",
        "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(0.001).minimize(loss)\n",
        "\n",
        "    # Compute the similarity between minibatch examples and all embeddings.\n",
        "    # We use the cosine distance:\n",
        "    norm = tf.compat.v1.sqrt(tf.compat.v1.reduce_sum(tf.compat.v1.square(embeddings), 1, keepdims=True))\n",
        "    normalized_embeddings = embeddings / norm\n",
        "    valid_embeddings = tf.compat.v1.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "    similarity = tf.compat.v1.matmul(valid_embeddings, tf.compat.v1.transpose(normalized_embeddings))\n",
        "\n",
        "\n",
        "def run_word2vec_source(batch_size):\n",
        "    global embedding_size, window_size\n",
        "    global valid_size, valid_window, valid_examples\n",
        "    global num_sampled\n",
        "    global train_dataset, train_labels\n",
        "    global valid_dataset\n",
        "    global softmax_weights, softmax_biases\n",
        "    global loss, optimizer, similarity, normalized_embeddings\n",
        "    global src_reverse_dictionary\n",
        "    global vocabulary_size, embedding_size\n",
        "\n",
        "    num_steps = 100001\n",
        "\n",
        "    config=tf.compat.v1.ConfigProto(allow_soft_placement=True) \n",
        "    config.gpu_options.allow_growth = True\t\n",
        "    \t\n",
        "    with tf.compat.v1.Session(config=config) as session:\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('Initialized')\n",
        "        average_loss = 0\n",
        "        for step in range(num_steps):\n",
        "\n",
        "            batch_data, batch_labels = generate_batch_for_word2vec(batch_size, window_size, is_source=True)\n",
        "            feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n",
        "            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "            average_loss += l\n",
        "            if (step + 1) % 2000 == 0:\n",
        "                if step > 0:\n",
        "                    average_loss = average_loss / 2000\n",
        "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "                print('Average loss at step %d: %f' % (step + 1, average_loss))\n",
        "                average_loss = 0\n",
        "            # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "            if (step + 1) % 10000 == 0:\n",
        "                sim = similarity.eval()\n",
        "                for i in range(valid_size):\n",
        "                    valid_word = src_reverse_dictionary[valid_examples[i]]\n",
        "                    top_k = 8  # number of nearest neighbors\n",
        "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "                    log = 'Nearest to %s:' % valid_word\n",
        "                    for k in range(top_k):\n",
        "                        close_word = src_reverse_dictionary[nearest[k]]\n",
        "                        log = '%s %s,' % (log, close_word)\n",
        "                    print(log)\n",
        "        cbow_final_embeddings = normalized_embeddings.eval()\n",
        "\n",
        "    np.save('de-embeddings.npy', cbow_final_embeddings)\n",
        "\n",
        "def run_word2vec_target(batch_size):\n",
        "    global embedding_size, window_size\n",
        "    global valid_size, valid_window, valid_examples\n",
        "    global num_sampled\n",
        "    global train_dataset, train_labels\n",
        "    global valid_dataset\n",
        "    global softmax_weights, softmax_biases\n",
        "    global loss, optimizer, similarity, normalized_embeddings\n",
        "    global tgt_reverse_dictionary\n",
        "    global vocabulary_size, embedding_size\n",
        "\n",
        "    num_steps = 100001\n",
        "    \n",
        "    config=tf.compat.v1.ConfigProto(allow_soft_placement=True) \n",
        "    config.gpu_options.allow_growth = True\t\n",
        "    with tf.compat.v1.Session(config=config) as session:\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('Initialized')\n",
        "        average_loss = 0\n",
        "        for step in range(num_steps):\n",
        "\n",
        "            batch_data, batch_labels = generate_batch_for_word2vec(batch_size, window_size, is_source=False)\n",
        "            feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n",
        "            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "            average_loss += l\n",
        "            if (step + 1) % 2000 == 0:\n",
        "                if step > 0:\n",
        "                    average_loss = average_loss / 2000\n",
        "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "                print('Average loss at step %d: %f' % (step + 1, average_loss))\n",
        "                average_loss = 0\n",
        "            # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "            if (step + 1) % 10000 == 0:\n",
        "                sim = similarity.eval()\n",
        "                for i in range(valid_size):\n",
        "                    valid_word = tgt_reverse_dictionary[valid_examples[i]]\n",
        "                    top_k = 8  # number of nearest neighbors\n",
        "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "                    log = 'Nearest to %s:' % valid_word\n",
        "                    for k in range(top_k):\n",
        "                        close_word = tgt_reverse_dictionary[nearest[k]]\n",
        "                        log = '%s %s,' % (log, close_word)\n",
        "                    print(log)\n",
        "        cbow_final_embeddings = normalized_embeddings.eval()\n",
        "\n",
        "    np.save('en-embeddings.npy', cbow_final_embeddings)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHyAMyYe1nNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "3e0640d1-b45b-4b02-b1aa-5cf3c69b16fb"
      },
      "source": [
        "print('Beginning file download with urllib2...')\n",
        "\n",
        "url_train_de = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de'\n",
        "urllib.request.urlretrieve(url_train_de, 'train.de')\n",
        "\n",
        "url_train_en = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en'\n",
        "urllib.request.urlretrieve(url_train_en, 'train.en')\n",
        "\n",
        "url_vocab_de = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.de'\n",
        "urllib.request.urlretrieve(url_vocab_de, 'vocab.50K.de')\n",
        "\n",
        "url_vocab_en = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.en'\n",
        "urllib.request.urlretrieve(url_vocab_en, 'vocab.50K.en')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning file download with urllib2...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('vocab.50K.en', <http.client.HTTPMessage at 0x7f41644f9ba8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50qYf6SQB8Nq",
        "colab_type": "text"
      },
      "source": [
        "# Data prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkfgOtixCBTD",
        "colab_type": "text"
      },
      "source": [
        "## Load vocabs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TinWhTBr4IbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "32093bad-30db-48cb-8f8f-435d7cf8877c"
      },
      "source": [
        "#  SEE THE VOCAB CLASSES IN THE PYTORCH EXAMPLES\n",
        "# Building source language vocabulary\n",
        "\n",
        "# Contains word string -> ID mapping\n",
        "src_dictionary = dict()\n",
        "\n",
        "# Read the vocabulary file\n",
        "with open('vocab.50K.de', encoding='utf-8') as f:\n",
        "    # Read and store every line\n",
        "    for line in f:\n",
        "        #we are discarding last char as it is new line char\n",
        "        src_dictionary[line[:-1]] = len(src_dictionary)\n",
        "\n",
        "# Build a reverse dictionary with the mapping ID -> word string\n",
        "src_reverse_dictionary = dict(zip(src_dictionary.values(),src_dictionary.keys()))\n",
        "\n",
        "# Print some of the words in the dictionary\n",
        "print('Source')\n",
        "print('\\t',list(src_dictionary.items())[:10])\n",
        "print('\\t',list(src_reverse_dictionary.items())[:10])\n",
        "print('\\t','Vocabulary size: ', len(src_dictionary))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source\n",
            "\t [('<unk>', 0), ('<s>', 1), ('</s>', 2), (',', 3), ('.', 4), ('die', 5), ('der', 6), ('und', 7), ('in', 8), ('zu', 9)]\n",
            "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, ','), (4, '.'), (5, 'die'), (6, 'der'), (7, 'und'), (8, 'in'), (9, 'zu')]\n",
            "\t Vocabulary size:  50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VQfxpTz4ZAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "b0459659-d226-4403-fba0-540cc4a87737"
      },
      "source": [
        "# Building target language vocabulary\n",
        "\n",
        "# Contains word string -> ID mapping\n",
        "tgt_dictionary = dict()\n",
        "\n",
        "# Read the vocabulary file\n",
        "with open('vocab.50K.en', encoding='utf-8') as f:\n",
        "    # Read and store every line\n",
        "    for line in f:\n",
        "        #we are discarding last char as it is new line char\n",
        "        tgt_dictionary[line[:-1]] = len(tgt_dictionary)\n",
        "\n",
        "# Build a reverse dictionary with the mapping ID -> word string\n",
        "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(),tgt_dictionary.keys()))\n",
        "\n",
        "# Print some of the words in the dictionary\n",
        "print('Target')\n",
        "print('\\t',list(tgt_dictionary.items())[:10])\n",
        "print('\\t',list(tgt_reverse_dictionary.items())[:10])\n",
        "print('\\t','Vocabulary size: ', len(tgt_dictionary))\n",
        "\n",
        "# Each language has 50000 words\n",
        "vocabulary_size = 50000"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target\n",
            "\t [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('the', 3), (',', 4), ('.', 5), ('of', 6), ('and', 7), ('to', 8), ('in', 9)]\n",
            "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'the'), (4, ','), (5, '.'), (6, 'of'), (7, 'and'), (8, 'to'), (9, 'in')]\n",
            "\t Vocabulary size:  50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFLspdzcCPrb",
        "colab_type": "text"
      },
      "source": [
        "## Load training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNshPNK6CVRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6322c242-7775-475d-de16-256a07c74e84"
      },
      "source": [
        "# Contains the training sentences\n",
        "source_sent = [] # Input\n",
        "target_sent = [] # Output\n",
        "\n",
        "# Contains the testing sentences\n",
        "test_source_sent = [] # Input\n",
        "test_target_sent = [] # Output\n",
        "\n",
        "# We grab around 100 lines of data that are interleaved \n",
        "# in the first 50000 sentences\n",
        "test_indices = [l_i for l_i in range(50,50001,500)]\n",
        "\n",
        "# Read the source data file and read the first 250,000 lines (except first 50)\n",
        "with open('train.de', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        # discarding first 50 translations as there was some\n",
        "        # english to english mappings found in the first few lines. which are wrong\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        \n",
        "        if len(source_sent)<250000 and l_i not in test_indices:\n",
        "            source_sent.append(line)\n",
        "        elif l_i in test_indices:\n",
        "            test_source_sent.append(line)\n",
        "        \n",
        "# Read the target data file and read the first 250,000 lines (except first 50)            \n",
        "with open('train.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        # discarding first 50 translations as there was some\n",
        "        # english to english mappings found in the first few lines. which are wrong\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        \n",
        "        if len(target_sent)<250000 and l_i not in test_indices:\n",
        "            target_sent.append(line)\n",
        "        elif l_i in test_indices:\n",
        "            test_target_sent.append(line)\n",
        "        \n",
        "# Make sure we extracted same number of both extracted source and target sentences         \n",
        "assert len(source_sent)==len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
        "\n",
        "# Print some source sentences\n",
        "print('Sample translations (%d)'%len(source_sent))\n",
        "for i in range(0,250000,10000):\n",
        "    print('(',i,') DE: ', source_sent[i])\n",
        "    print('(',i,') EN: ', target_sent[i])\n",
        "\n",
        "# Print some target sentences\n",
        "print('Sample test translations (%d)'%len(test_source_sent))\n",
        "for i in range(0,100,10):\n",
        "    print('DE: ', test_source_sent[i])\n",
        "    print('EN: ', test_target_sent[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample translations (250000)\n",
            "( 0 ) DE:  Hier erfahren Sie , wie Sie Creative Suite 2 und Creative Suite 3 am besten zusammen mit QuarkXPress nutzen können .\n",
            "\n",
            "( 0 ) EN:  Here , you ’ ll find out how Creative Suite users can get the best possible interaction with QuarkXPress .\n",
            "\n",
            "( 10000 ) DE:  Für die sehr günstigen Wochen- und Monatskarten ( 1 Monat ca.\n",
            "\n",
            "( 10000 ) EN:  It is THE trendy area of Marseille .\n",
            "\n",
            "( 20000 ) DE:  Freuen Sie sich auf die romantische Atmosphäre in den Zimmern und Apartments .\n",
            "\n",
            "( 20000 ) EN:  Enjoy the romantic atmosphere of one of the guest rooms or apartments .\n",
            "\n",
            "( 30000 ) DE:  Zu zwiespältig sind Dr. Gutherzens Erfahrungen aus frühen Studententagen verlaufen , in denen er sich in die Gefielde von durch Heidegger geprägten Autor / innen begeben hat und dort ständig mit strengem Blick darauf verwiesen wurde , er habe bestimmte Theorieressourcen und Gedankengebäude einfach noch nicht gründlich genug verstanden und könne deshalb nicht begreifen , warum seine Einwände zu bestimmten Texten und Diskursen nicht stichhaltig seien .\n",
            "\n",
            "( 30000 ) EN:  This vagueness lends itself to an idealisation of violence , formulated in concepts of &quot; assault &quot; against imaginary authorities or enthusiastic notions of &quot; blissful traumatic knowledge &quot; .\n",
            "\n",
            "( 40000 ) DE:  Sie veranlassen den untergeordneten Prozess , sich während seiner gesamten Lebensdauer lediglich einmal mit dem SQL ##AT##-##AT## Server zu verbinden , anstatt bei jedem Aufruf einer Seite , die eine Verbindung benötigt .\n",
            "\n",
            "( 40000 ) EN:  They cause the child process to simply connect only once for its entire lifespan , instead of every time it processes a page that requires connecting to the SQL server .\n",
            "\n",
            "( 50000 ) DE:  Je intensiver man dabei bleibt , desto bessere Ergebnisse erzielt man .\n",
            "\n",
            "( 50000 ) EN:  The more intensively you do them , the better the results .\n",
            "\n",
            "( 60000 ) DE:  In allen Zimmern ist Digitalfernsehen und Internetzugang für sowohl Geschäftsreisende als auch Urlauber erhältlich .\n",
            "\n",
            "( 60000 ) EN:  All rooms offer digital TV and Internet access appealing to both corporate and leisure guests .\n",
            "\n",
            "( 70000 ) DE:  Bitte beachten Sie , dass Ihr Check ##AT##-##AT## in ##AT##-##AT## Code nicht mit der Buchungsnummer identisch ist .\n",
            "\n",
            "( 70000 ) EN:  Please note that the check ##AT##-##AT## in number and your reservation number are not the same .\n",
            "\n",
            "( 80000 ) DE:  Auch die Art , wie man einen eigenen Weißabgleich vornehmen kann , darf angepasst werden .\n",
            "\n",
            "( 80000 ) EN:  Another thing that should be reassessed is the way in which the user creates his own white balance .\n",
            "\n",
            "( 90000 ) DE:  Weitere Supportoptionen ( http : / / support.microsoft.com / contactus ) : Stellen Sie Ihre Fragen im Web , wenden Sie sich an Microsoft Support Services , oder teilen Sie uns Ihre Meinung mit .\n",
            "\n",
            "( 90000 ) EN:  Other Support Options ( http : / / support.microsoft.com / default.aspx ? pr = csshome ) : Use the Web to ask a question , contact Microsoft Customer Support Services , or provide feedback .\n",
            "\n",
            "( 100000 ) DE:  Ik vond het geen 4 ##STAR## waard . Het appartement oogde erg schroezelig en gedateerd , personeel sprak erg gebrekkig Engels .\n",
            "\n",
            "( 100000 ) EN:  Trousse d &apos;information manquante et devrait inclure une carte du site ainsi que des services et activités sur les lieu ou dans la commune ainsi les attraits touristiques de la région .\n",
            "\n",
            "( 110000 ) DE:  Dieses Bild spiegelt sich in Ihrem Unternehmen und Ihren Produkten wieder .\n",
            "\n",
            "( 110000 ) EN:  This image reflects on your company and products .\n",
            "\n",
            "( 120000 ) DE:  Alle Zimmer sind mit Digital ##AT##-##AT## TV und DVD und kostenlosem Breitbandanschluss sowie Direktwahltelefon ausgestattet .\n",
            "\n",
            "( 120000 ) EN:  Our rooms include a romantic four ##AT##-##AT## poster and two easy access ground floor rooms . All rooms are equipped to hotel standards with Digital TV and DVD , free broadband connections and free local and national direct dial phones .\n",
            "\n",
            "( 130000 ) DE:  Nothing if im quite honet . I wouldnt stay here again or recommend it to anyone i know .\n",
            "\n",
            "( 130000 ) EN:  the room was basic but spacoius and clean , the staff were friendly and helpful , the food was tasty , all in all , lovely place to stay !\n",
            "\n",
            "( 140000 ) DE:  Es gibt 4 verschiedene Möglichkeiten , Cannon Blast zu Ihrem Blog oder Ihrer Website hinzuzufügen .\n",
            "\n",
            "( 140000 ) EN:  There are 4 different ways of posting Cannon Blast to your blog or website .\n",
            "\n",
            "( 150000 ) DE:  Wenn die Buchung vor 14 : 00 Uhr 3 , Tage vor dem geplanten Anreisetag storniert wird , fällt keine Stornierungsgebühr an .\n",
            "\n",
            "( 150000 ) EN:  There will be no cancellation charge if a booking is cancelled before 14 : 00 3 days before your date of arrival .\n",
            "\n",
            "( 160000 ) DE:  Im geräumigen Hotelrestaurant Al Caminetto kosten Sie Gerichte aus Mailand und aus aller Welt .\n",
            "\n",
            "( 160000 ) EN:  The hotel ’ s restaurant , Al Caminetto , serves Milanese and international cuisine .\n",
            "\n",
            "( 170000 ) DE:  Während der 60 &apos; er Jahre gab es viele Regisseure die in die Wüste von Ameria zogen um , mit der ...\n",
            "\n",
            "( 170000 ) EN:  During the 1960s , numerous movie directors chose Almeria &apos;s desert ##AT##-##AT## like landscape to film some of ...\n",
            "\n",
            "( 180000 ) DE:  Tikje krappe kamer voor het aanwezige meubilair en de lift is absoluut niet meer van deze tijd : veel te klein .\n",
            "\n",
            "( 180000 ) EN:  Chambre minuscule , rien à voir avec les photos présentées , SDB &quot; vieillotte &quot; . Absence totale d &apos;insonorisation : l &apos;intimité de vos voisins de chambre en direct ....... Séjour écourté ......\n",
            "\n",
            "( 190000 ) DE:  Das Großunternehmen sieht sich einfach die Produkte des kleinen Unternehmens an und unterstellt so viele Patentverletzungen , wie es nur geht .\n",
            "\n",
            "( 190000 ) EN:  The large corporation will look at the products of the small company and bring up as many patent infringement assertions as possible .\n",
            "\n",
            "( 200000 ) DE:  Wochentags bis 22 Uhr , Samstags bis 18 Uhr geöffnet . Sehr sympathische Atmosphäre .\n",
            "\n",
            "( 200000 ) EN:  This is an interactive multimedia tour ( choice of languages ) through Weimar &apos;s history from prehistoric times to the present .\n",
            "\n",
            "( 210000 ) DE:  Wann möchten Sie im Entrecercas übernachten ?\n",
            "\n",
            "( 210000 ) EN:  When would you like to stay at the Entrecercas ?\n",
            "\n",
            "( 220000 ) DE:  In der ordentlichen Sitzung am 22. September 2008 befasste sich der Aufsichtsrat mit strategischen Themen aus den einzelnen Geschäftsbereichen wie der Positionierung des Kassamarktes im Wettbewerb mit außerbörslichen Handelsplattformen , den Innovationen im Derivatesegment und verschiedenen Aktivitäten im Nachhandelsbereich .\n",
            "\n",
            "( 220000 ) EN:  At the regular meeting on 22 September 2008 , the Supervisory Board dealt with strategic issues from the various business areas , such as the positioning of the cash market in competition with OTC trading platforms , innovation in the derivatives segment and various post ##AT##-##AT## trading activities .\n",
            "\n",
            "( 230000 ) DE:  Ich hatte keine Sekunde zum Entspannen .\n",
            "\n",
            "( 230000 ) EN:  I never had even one second to relax .\n",
            "\n",
            "( 240000 ) DE:  Das Englisch sprechende Personal steht Ihnen mit Rat und Tat zur Seite , informiert über Sehenswürdigkeiten und arrangiert Ihren Transfer .\n",
            "\n",
            "( 240000 ) EN:  The English ##AT##-##AT## speaking staff are always on hand to make your stay special .\n",
            "\n",
            "Sample test translations (100)\n",
            "DE:  Heute verstehen sich QuarkXPress ® 8 , Photoshop ® und Illustrator ® besser als jemals zuvor . Dank HTML und CSS ­ können Anwender von QuarkXPress inzwischen alle Medien bedienen , und das unabhängig von Anwendungen der Adobe ® Creative Suite ® wie Adobe Flash ® ( SWF ) und Adobe Dreamweaver ® .\n",
            "\n",
            "EN:  Today , QuarkXPress ® 8 has tighter integration with Photoshop ® and Illustrator ® than ever before , and through standards like HTML and CSS , QuarkXPress users can publish across media both independently and alongside Adobe ® Creative Suite ® applications like Adobe Flash ® ( SWF ) and Adobe Dreamweaver ® .\n",
            "\n",
            "DE:  Das Hotel Opera befindet sich in der Nähe des Royal Theatre , Kongens Nytorv , &apos; Stroget &apos; und Nyhavn .\n",
            "\n",
            "EN:  Hotel Opera is situated near The Royal Theatre , Kongens Nytorv , &quot; Strøget &quot; and fascinating Nyhavn .\n",
            "\n",
            "DE:  Es existieren Busverbindungen in nahezu jeden Ort der Provence ( eventuell mit Umsteigen in Aix ##AT##-##AT## en ##AT##-##AT## Provence ) , allerdings sollte beachtet werden , dass die letzten Busse abends ca. um 19 Uhr fahren .\n",
            "\n",
            "EN:  As always in France those highways are expensive but practical , comfortable and fast .\n",
            "\n",
            "DE:  15. einem Dritten bei dem Verstoss gegen eine dieser Regeln zu helfen .\n",
            "\n",
            "EN:  15. assist any third party in engaging in any activity prohibited by these Terms .\n",
            "\n",
            "DE:  Es war staubig , das Bad schmutzig . Sogar die Beleuchtung an der Wand im Flur ( Seitengebäude ) war richtig verstaubt .\n",
            "\n",
            "EN:  It was rather old fashioned in the decoration .\n",
            "\n",
            "DE:  Die Bewohner des Nordens sind ein buntes Völkergemisch aus den verschiedensten Bergstämmen und den Nord ##AT##-##AT## Thais oder kon mueang ; die traditionell in den fruchtbaren Tiefebenen Nordthailands siedeln . In vielerlei Hinsicht halten sich die Nord Thais für die &quot; wahren &quot; Thais , die die Thai ##AT##-##AT## Kultur noch am besten über die Zeit gerettet haben .\n",
            "\n",
            "EN:  From Pratu Chiang Mai market , songthaews also travel to Hang Dong ( 20 baht ) and San Patong , south ##AT##-##AT## west of Chiang Mai .\n",
            "\n",
            "DE:  Auch ist , so denkt Dr. Gutherz , bereits die erste Seite sehr viel versprechend , da sie eine Definition des klinischen Psychotrauma ##AT##-##AT## Begriffes enthält , der er gänzlich zustimmen kann .\n",
            "\n",
            "EN:  At the rhetorical climax of this summary , Dr Goodheart comes across some sentences expressed with great pathos .\n",
            "\n",
            "DE:  Das Cleddau Bridge Hotel ist der ideale Platz um zu entspannen oder geschäftlich zu reisen .\n",
            "\n",
            "EN:  Cleddau Bridge hotel is the ideal place for those who want a relaxing holiday or who travel for business .\n",
            "\n",
            "DE:  Bei einer digitalen Bildkette wird das Intensitätssignal für jedes Pixel ohne analoge Zwischenschritte direkt in der Detektoreinheit digitalisiert , d.h. in Zahlen umgewandelt .\n",
            "\n",
            "EN:  A digital image chain is an image chain that is equipped with a digital detector instead of an analogue one .\n",
            "\n",
            "DE:  Sehr freundliche Auszubildende an der Rezeption , die sehr bemüht noch einen Flug für mich gebucht hat .\n",
            "\n",
            "EN:  First of all I did not like the price ... the next day I went to Milano to a 4 star Hotel for 10 Euro less and super service .. I had a problem with my Internetconnection and the Hotel Maritim did not react right .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSIm4r03Dd9B",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6y2AAMADhrO",
        "colab_type": "text"
      },
      "source": [
        "### Split data into token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNKNvR5YCefJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keep track of how many unknown words were encountered\n",
        "src_unk_count, tgt_unk_count = 0, 0\n",
        "\n",
        "def split_to_tokens(sent,is_source):\n",
        "    '''\n",
        "    This function takes in a sentence (source or target)\n",
        "    and preprocess the sentency with various steps (e.g. removing punctuation)\n",
        "    '''\n",
        "    \n",
        "    global src_unk_count, tgt_unk_count\n",
        "\n",
        "    # Remove punctuation and new-line chars\n",
        "    sent = sent.replace(',',' ,')\n",
        "    sent = sent.replace('.',' .')\n",
        "    sent = sent.replace('\\n',' ') \n",
        "    \n",
        "    sent_toks = sent.split(' ')\n",
        "    for t_i, tok in enumerate(sent_toks):\n",
        "        if is_source:\n",
        "            # src_dictionary contain the word -> word ID mapping for source vocabulary\n",
        "            if tok not in src_dictionary.keys():\n",
        "                if not len(tok.strip())==0:\n",
        "                    sent_toks[t_i] = '<unk>'\n",
        "                    src_unk_count += 1\n",
        "        else:\n",
        "            # tgt_dictionary contain the word -> word ID mapping for target vocabulary\n",
        "            if tok not in tgt_dictionary.keys():\n",
        "                if not len(tok.strip())==0:\n",
        "                    sent_toks[t_i] = '<unk>'\n",
        "                    #print(tok)\n",
        "                    tgt_unk_count += 1\n",
        "    return sent_toks"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BHGU1r-DT77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "31b5502d-743a-4883-f460-87cb39b4be10"
      },
      "source": [
        "# Let us first look at some statistics of the sentences\n",
        "# Train - source data\n",
        "source_len = []\n",
        "source_mean, source_std = 0,0\n",
        "for sent in source_sent:\n",
        "    source_len.append(len(split_to_tokens(sent,True)))\n",
        "\n",
        "print('(Source) Sentence mean length: ', np.mean(source_len))\n",
        "print('(Source) Sentence stddev length: ', np.std(source_len))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Source) Sentence mean length:  26.244692\n",
            "(Source) Sentence stddev length:  13.854376414156501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOQ8nhS1DWGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "54ba21e1-198a-45f5-82d1-dcab03dca826"
      },
      "source": [
        "# Let us first look at some statistics of the sentences\n",
        "# Train - target data\n",
        "target_len = []\n",
        "for sent in target_sent:\n",
        "    target_len.append(len(split_to_tokens(sent,False)))\n",
        "\n",
        "print('(Target) Sentence mean length: ', np.mean(target_len))\n",
        "print('(Target) Sentence stddev length: ', np.std(target_len))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Target) Sentence mean length:  28.275308\n",
            "(Target) Sentence stddev length:  14.925498769057468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7pMPDRFDZkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "e916bebf-3e35-4e16-86d3-13cac89d4eb8"
      },
      "source": [
        "# Let us first look at some statistics of the sentences\n",
        "# Test - source data\n",
        "test_source_len = []\n",
        "for sent in test_source_sent:\n",
        "    test_source_len.append(len(split_to_tokens(sent, True)))\n",
        "    \n",
        "print('(Test-Source) Sentence mean length: ', np.mean(test_source_len))\n",
        "print('(Test-Source) Sentence stddev length: ', np.std(test_source_len))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Test-Source) Sentence mean length:  26.61\n",
            "(Test-Source) Sentence stddev length:  14.800604717375572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUmP8bZGDbVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "9dc91dc3-b829-49b1-94f7-740030500f4d"
      },
      "source": [
        "# Let us first look at some statistics of the sentences\n",
        "# Test - target data\n",
        "test_target_len = []\n",
        "test_tgt_mean, test_tgt_std = 0,0\n",
        "for sent in test_target_sent:\n",
        "    test_target_len.append(len(split_to_tokens(sent, False)))\n",
        "    \n",
        "print('(Test-Target) Sentence mean length: ', np.mean(test_target_len))\n",
        "print('(Test-Target) Sentence stddev length: ', np.std(test_target_len))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Test-Target) Sentence mean length:  29.08\n",
            "(Test-Target) Sentence stddev length:  16.19424589167399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqJ_iHdbD7n5",
        "colab_type": "text"
      },
      "source": [
        "### Making training and testing data fixed length\n",
        "Here we get all the source sentences and target sentences to a fixed length. This is, so that we can process the sentences as batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBSowcRGD983",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "7cb8f25e-01b4-4871-ad8f-35be1608fa07"
      },
      "source": [
        "# Processing training data\n",
        "\n",
        "src_unk_count, tgt_unk_count = 0, 0\n",
        "\n",
        "train_inputs = []\n",
        "train_outputs = []\n",
        "\n",
        "# Chosen based on previously found statistics\n",
        "src_max_sent_length = 41 \n",
        "tgt_max_sent_length = 61\n",
        "\n",
        "print('Processing Training Data ...\\n')\n",
        "for s_i, (src_sent, tgt_sent) in enumerate(zip(source_sent,target_sent)):\n",
        "    # Break source and target sentences to word lists\n",
        "    src_sent_tokens = split_to_tokens(src_sent,True)\n",
        "    tgt_sent_tokens = split_to_tokens(tgt_sent,False)\n",
        "    \n",
        "    # Append <s> token's ID to the beggining of source sentence\n",
        "    num_src_sent = [src_dictionary['<s>']]\n",
        "    # Add the rest of word IDs for words found in the source sentence \n",
        "    for tok in src_sent_tokens:\n",
        "        if tok in src_dictionary.keys():\n",
        "            num_src_sent.append(src_dictionary[tok])\n",
        "\n",
        "    # If the lenghth of the source sentence below the maximum allowed length\n",
        "    # append </s> token's ID to the end\n",
        "    if len(num_src_sent)<src_max_sent_length:\n",
        "        num_src_sent.extend([src_dictionary['</s>'] for _ in range(src_max_sent_length - len(num_src_sent))])\n",
        "\n",
        "    # If the length exceed the maximum allowed length\n",
        "    # truncate the sentence\n",
        "    elif len(num_src_sent)>src_max_sent_length:\n",
        "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
        "        \n",
        "    # Make sure the sentence is of length src_max_sent_length\n",
        "    assert len(num_src_sent)==src_max_sent_length,len(num_src_sent)\n",
        "\n",
        "    train_inputs.append(num_src_sent)\n",
        "    \n",
        "    # Create the numeric target sentence with word IDs\n",
        "    # append <s> to the beginning and append actual words later\n",
        "    num_tgt_sent = [tgt_dictionary['<s>']]\n",
        "    for tok in tgt_sent_tokens:\n",
        "        if tok in tgt_dictionary.keys():\n",
        "            num_tgt_sent.append(tgt_dictionary[tok])\n",
        "        \n",
        "    ## Modifying the outputs such that all the outputs have max_length elements\n",
        "    if len(num_tgt_sent)<tgt_max_sent_length:\n",
        "        num_tgt_sent.extend([tgt_dictionary['</s>'] for _ in range(tgt_max_sent_length - len(num_tgt_sent))])\n",
        "    elif len(num_tgt_sent)>tgt_max_sent_length:\n",
        "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
        "        \n",
        "    train_outputs.append(num_tgt_sent)\n",
        "    \n",
        "print('Unk counts Src: %d, Tgt: %d'%(src_unk_count, tgt_unk_count))\n",
        "print('Sentences ',len(train_inputs))\n",
        "\n",
        "assert len(train_inputs)  == len(source_sent),\\\n",
        "        'Size of total elements: %d, Total sentences: %d'\\\n",
        "                %(len(train_inputs),len(source_sent))\n",
        "\n",
        "# Making inputs and outputs NumPy arrays\n",
        "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
        "train_outputs = np.array(train_outputs, dtype=np.int32)\n",
        "\n",
        "# Make sure number of inputs and outputs dividable by 100\n",
        "train_inputs = train_inputs[:(train_inputs.shape[0]//100)*100,:]\n",
        "train_outputs = train_outputs[:(train_outputs.shape[0]//100)*100,:]\n",
        "print('\\t Done processing training data \\n')\n",
        "\n",
        "# Printing some data\n",
        "print('Samples from training data')\n",
        "for ti in range(10):\n",
        "    print('\\t',[src_reverse_dictionary[w]  for w in train_inputs[ti,:].tolist()])\n",
        "    print('\\t',[tgt_reverse_dictionary[w]  for w in train_outputs[ti,:].tolist()])\n",
        "print()\n",
        "print('\\tSentences ',train_inputs.shape[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Training Data ...\n",
            "\n",
            "Unk counts Src: 464223, Tgt: 214783\n",
            "Sentences  250000\n",
            "\t Done processing training data \n",
            "\n",
            "Samples from training data\n",
            "\t ['<s>', 'Hier', 'erfahren', 'Sie', ',', 'wie', 'Sie', 'Creative', 'Suite', '2', 'und', 'Creative', 'Suite', '3', 'am', 'besten', 'zusammen', 'mit', 'QuarkXPress', 'nutzen', 'können', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Here', ',', 'you', '’', 'll', 'find', 'out', 'how', 'Creative', 'Suite', 'users', 'can', 'get', 'the', 'best', 'possible', 'interaction', 'with', 'QuarkXPress', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Sie', 'werden', 'überrascht', 'sein', ',', 'wie', 'einfach', 'sich', 'mit', 'Quark', 'das', 'volle', 'Potenzial', 'Ihrer', 'Design', '##AT##-##AT##', 'Software', 'erschließen', 'lässt', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'You', '’', 'll', 'be', 'surprised', 'how', 'easy', 'Quark', 'has', 'made', 'it', 'to', 'unlock', 'the', 'full', 'potential', 'of', 'all', 'your', 'design', 'software', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Häufig', 'wird', 'die', 'Meinung', 'vertreten', ',', 'dass', 'QuarkXPress', '8', 'von', 'allen', 'heute', 'verfügbaren', 'Layout', '##AT##-##AT##', 'Programmen', 'die', 'beste', 'Integration', 'mit', 'Photoshop', 'über', 'das', 'PSD', '##AT##-##AT##', 'Dateiformat', 'bietet', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'QuarkXPress', '8', 'is', 'considered', 'by', 'many', 'to', 'have', 'the', 'best', 'integration', 'with', 'Photoshop', '’', 's', 'PSD', 'file', 'format', 'of', 'any', 'layout', 'tool', 'available', 'today', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'In', 'diesem', 'Abschnitt', 'erläutern', 'wir', ',', 'wann', 'Sie', 'für', 'Ihre', 'Bilder', 'das', 'PSD', '##AT##-##AT##', 'Format', 'verwenden', 'sollten', 'und', 'wie', 'Sie', 'es', 'für', 'Ihre', 'Bilder', 'optimal', 'nutzen', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'In', 'this', 'section', 'we', '’', 'll', 'explain', 'when', 'you', 'should', 'use', 'the', 'PSD', 'format', 'for', 'your', 'images', 'and', 'how', 'to', 'get', 'the', 'most', 'out', 'of', 'them', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Angenommen', 'Sie', 'haben', 'verschiedene', 'Ebenen', 'in', 'Ihrer', 'PSD', '##AT##-##AT##', 'Datei', 'mit', 'verschiedenen', 'Darstellungen', 'eines', 'Produkts', ',', 'die', 'je', 'nach', 'Verwendungszweck', 'ausgewählt', 'werden', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'For', 'example', ',', 'you', 'may', 'have', 'multiple', 'layers', 'in', 'your', 'PSD', 'with', 'different', 'product', 'shots', ',', 'which', 'will', 'vary', 'from', 'publication', 'to', 'publication', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Wenn', 'Sie', 'mit', 'PSD', 'arbeiten', ',', 'können', 'Sie', 'diese', 'Ebenen', 'in', 'QuarkXPress', 'ein-', 'oder', 'ausschalten', ',', 'ohne', 'für', 'jede', 'Veröffentlichung', 'eine', 'eigene', 'TIFF', '##AT##-##AT##', 'Datei', 'generieren', 'zu', 'müssen', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'If', 'you', 'use', 'PSD', ',', 'you', 'can', 'switch', 'those', 'layers', 'on', 'or', 'off', 'in', 'QuarkXPress', 'without', 'having', 'to', 'save', 'a', 'separate', 'TIFF', 'for', 'each', 'publication', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Eine', 'andere', 'mögliche', 'Frage', 'für', 'die', 'Entscheidung', 'zwischen', 'PSD', 'und', 'TIFF', 'ist', ':', '„', 'Muss', 'ich', 'für', 'dieses', 'Bild', 'eine', '<unk>', 'verwenden', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Another', 'question', 'that', 'might', 'tip', 'you', 'in', 'favor', 'of', 'PSD', 'is', ',', '&quot;', 'Do', 'I', 'need', 'to', 'use', 'a', 'spot', 'color', 'with', 'this', 'image', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '&quot;', 'In', 'den', 'meisten', '<unk>', 'sind', '<unk>', 'oft', 'problematisch', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '&quot;', 'Using', 'spot', 'colors', 'in', 'most', 'image', 'formats', 'is', 'often', 'complicated', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Da', 'QuarkXPress', 'jedoch', 'PSD', '##AT##-##AT##', 'Kanäle', 'unterstützt', ',', 'geht', 'es', 'mit', 'PSD', 'einfacher', 'und', 'flexibler', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'However', ',', 'because', 'of', 'the', 'way', 'QuarkXPress', 'supports', 'PSD', 'channels', ',', 'it', '’', 's', 'simpler', 'and', 'more', 'flexible', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Erstellen', 'Sie', 'einen', 'Rahmen', 'und', 'gehen', 'Sie', 'dann', 'auf', 'Datei', '&gt;', 'Importieren', '.', '.', '.', 'oder', 'ziehen', 'Sie', 'das', 'Bild', 'einfach', 'per', 'Drag', '&amp;', 'Drop', 'von', 'Ihrem', 'Desktop', ',', 'aus', 'dem', 'Finder', 'oder', 'einer', 'Anwendung', 'wie', 'Adobe', 'Bridge', '<unk>', '–']\n",
            "\t ['<s>', 'Bringing', 'the', 'PSD', 'files', 'into', 'QuarkXPress', 'is', 'the', 'same', 'as', 'any', 'other', 'image', '.', 'Create', 'a', 'Box', 'and', 'then', 'use', 'File', '&gt;', 'Import', '.', '.', '.', 'or', 'simply', 'drag', 'and', 'drop', 'the', 'image', 'from', 'your', 'desktop', ',', 'Finder', 'or', 'an', 'application', 'like', 'Adobe', 'Bridge', '®', 'with', 'or', 'without', 'creating', 'a', 'box', 'first', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\n",
            "\tSentences  250000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq-Pbd4GEa7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "07a7c55b-2361-445a-d2e7-f8d15ab24e5a"
      },
      "source": [
        "# Processing Test data\n",
        "\n",
        "src_unk_count, tgt_unk_count = 0, 0\n",
        "print('Processing testing data ....\\n')\n",
        "test_inputs = []\n",
        "test_outputs = []\n",
        "for s_i, (src_sent,tgt_sent) in enumerate(zip(test_source_sent,test_target_sent)):\n",
        "    src_sent_tokens = split_to_tokens(src_sent,True)\n",
        "    tgt_sent_tokens = split_to_tokens(tgt_sent,False)\n",
        "    \n",
        "    num_src_sent = [src_dictionary['<s>']]\n",
        "    for tok in src_sent_tokens:\n",
        "        if tok in src_dictionary.keys():\n",
        "            num_src_sent.append(src_dictionary[tok])\n",
        "    \n",
        "    num_tgt_sent = [src_dictionary['<s>']]\n",
        "    for tok in tgt_sent_tokens:\n",
        "        if tok in tgt_dictionary.keys():\n",
        "            num_tgt_sent.append(tgt_dictionary[tok])\n",
        "        \n",
        "    # Append </s> if the length is not src_max_sent_length\n",
        "    if len(num_src_sent)<src_max_sent_length:\n",
        "        num_src_sent.extend([src_dictionary['</s>'] for _ in range(src_max_sent_length - len(num_src_sent))])\n",
        "    # Truncate the sentence if length is over src_max_sent_length\n",
        "    elif len(num_src_sent)>src_max_sent_length:\n",
        "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
        "        \n",
        "    assert len(num_src_sent)==src_max_sent_length, len(num_src_sent)\n",
        "\n",
        "    test_inputs.append(num_src_sent)\n",
        "    \n",
        "    # Append </s> is length is not tgt_max_sent_length\n",
        "    if len(num_tgt_sent)<tgt_max_sent_length:\n",
        "        num_tgt_sent.extend([tgt_dictionary['</s>'] for _ in range(tgt_max_sent_length - len(num_tgt_sent))])\n",
        "    # Truncate the sentence if length over tgt_max_sent_length\n",
        "    elif len(num_tgt_sent)>tgt_max_sent_length:\n",
        "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
        "        \n",
        "    assert len(num_tgt_sent)==tgt_max_sent_length, len(num_tgt_sent)\n",
        "\n",
        "    test_outputs.append(num_tgt_sent)\n",
        "\n",
        "# Printing some data\n",
        "print('Unk counts Tgt: %d, Tgt: %d'%(src_unk_count, tgt_unk_count))    \n",
        "print('Done processing testing data ....\\n')\n",
        "test_inputs = np.array(test_inputs,dtype=np.int32)\n",
        "test_outputs = np.array(test_outputs,dtype=np.int32)\n",
        "print('Samples from training data')\n",
        "for ti in range(10):\n",
        "    print('\\t',[src_reverse_dictionary[w]  for w in test_inputs[ti,:].tolist()])\n",
        "    print('\\t',[tgt_reverse_dictionary[w]  for w in test_outputs[ti,:].tolist()])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing testing data ....\n",
            "\n",
            "Unk counts Tgt: 212, Tgt: 107\n",
            "Done processing testing data ....\n",
            "\n",
            "Samples from training data\n",
            "\t ['<s>', 'Heute', 'verstehen', 'sich', 'QuarkXPress', '®', '8', ',', 'Photoshop', '®', 'und', 'Illustrator', '®', 'besser', 'als', 'jemals', 'zuvor', '.', 'Dank', 'HTML', 'und', 'CSS', '\\xad', 'können', 'Anwender', 'von', 'QuarkXPress', 'inzwischen', 'alle', 'Medien', 'bedienen', ',', 'und', 'das', 'unabhängig', 'von', 'Anwendungen', 'der', 'Adobe', '®', 'Creative']\n",
            "\t ['<s>', 'Today', ',', 'QuarkXPress', '®', '8', 'has', 'tighter', 'integration', 'with', 'Photoshop', '®', 'and', 'Illustrator', '®', 'than', 'ever', 'before', ',', 'and', 'through', 'standards', 'like', 'HTML', 'and', 'CSS', ',', 'QuarkXPress', 'users', 'can', 'publish', 'across', 'media', 'both', 'independently', 'and', 'alongside', 'Adobe', '®', 'Creative', 'Suite', '®', 'applications', 'like', 'Adobe', 'Flash', '®', '(', 'SWF', ')', 'and', 'Adobe', 'Dreamweaver', '®', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Je', 'mehr', 'Zeit', 'wir', 'mit', 'Gilad', 'und', 'dem', 'Rest', 'des', 'Teams', 'in', 'Israel', 'verbracht', 'haben', '(', 'um', 'nicht', 'den', 'lauten', 'Hahn', 'zu', 'erwähnen', 'der', '<unk>', 'bei', 'denen', 'über', 'den', 'Campus', '<unk>', ')', 'desto', 'überzeugter', 'waren', 'wir', '–', 'zusammen', 'können', 'wir']\n",
            "\t ['<s>', 'The', 'more', 'time', 'we', 'spent', 'with', 'Gilad', 'as', 'well', 'as', 'the', 'rest', 'of', 'the', 'team', 'in', 'Israel', '(', 'not', 'to', 'mention', 'the', 'very', 'loud', '<unk>', 'that', 'runs', 'around', 'in', 'their', 'campus', ')', ',', 'the', 'more', 'convinced', 'we', 'all', 'became', '-', 'we', '’', 'll', 'be', 'better', 'off', 'together', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '34', 'Diese', 'a', 'Worte', 'sind', 'wahr', 'und', 'treu', ';', 'darum', '<unk>', 'sie', 'nicht', ',', 'und', 'b', 'nehmt', 'auch', 'nichts', 'davon', 'weg', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '34', 'These', 'sayings', 'are', 'a', 'true', 'and', 'faithful', ';', 'wherefore', ',', 'transgress', 'them', 'not', ',', 'neither', 'b', 'take', 'therefrom', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '&#124;', 'Ferienwohnungen', '1', 'Zi', '&#124;', 'Ferienhäuser', '&#124;', 'Landhäuser', '&#124;', 'Autovermietung', '&#124;', 'Last', 'Minute', 'Angebote', '!', '!', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '&#124;', '1', 'Bedroom', 'Apts', '&#124;', 'Holiday', 'houses', '&#124;', 'Rural', 'Homes', '&#124;', 'Car', 'Rental', '&#124;', 'Last', 'Minute', 'Offers', '!', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Der', '<unk>', 'Teil', 'der', 'Insel', 'besteht', 'aus', 'Granit', 'und', '<unk>', ',', 'von', 'Ton', 'überlagert', ',', 'und', 'bildet', 'eine', 'ca', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'A', 'battle', 'between', 'Denmark', 'and', 'Sweden', 'in', '<unk>', 'led', 'to', 'Swedish', 'control', 'of', 'the', 'island', ',', 'but', 'it', 'was', 'brief', '-', 'they', 'left', 'again', 'the', 'same', 'year', '.', 'In', 'the', '<unk>', 'of', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', 'and', '<unk>', 'were', 'given', 'to', 'Sweden', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Mag', 'sein', ',', 'dass', 'du', 'deine', 'ersten', '<unk>', 'in', 'einem', '<unk>', ',', '<unk>', 'Kahn', '<unk>', '-', 'aber', 'mit', 'der', 'Zeit', 'wirst', 'du', 'dich', 'zum', '<unk>', '<unk>', 'oder', 'edlen', 'Katamaran', '<unk>', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'You', 'may', 'be', 'starting', 'in', 'a', '<unk>', 'old', 'tub', 'of', 'a', 'boat', ',', 'but', 'in', 'no', 'time', 'at', 'all', 'you', '&apos;ll', 'be', 'able', 'to', 'buy', 'a', 'fancy', '<unk>', ',', 'or', 'a', 'classy', 'catamaran', '.', 'Turn', 'your', 'newfound', 'fame', 'into', 'money', ',', 'and', 'spend', 'it', 'to', 'buy', 'lavish', 'new', 'homes', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'In', 'der', '<unk>', 'im', 'Internet', 'müßte', 'die', 'Zufahrt', 'beschrieben', 'werden', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'There', 'are', 'no', 'adverse', 'comments', 'about', 'this', 'hotel', 'at', 'all', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Ideale', 'Lage', 'für', 'Exkursionen', 'in', 'die', 'Stadt', 'und', 'Nähe', 'zur', 'Promenade', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'There', 'was', 'plenty', 'of', 'space', 'in', 'the', 'room', 'and', 'a', 'nice', 'garden', 'to', 'sit', 'and', 'have', 'a', 'drink', 'and', 'smoke', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Das', 'Hotel', '<unk>', 'verfügt', 'über', 'eine', 'ideale', ',', 'ruhige', 'Lage', 'in', 'einem', 'geschäftigen', 'Viertel', 'mit', 'guter', 'Verkehrsanbindung', '.', 'Der', 'Bahnhof', 'und', 'eine', 'U', '##AT##-##AT##', 'Bahnstation', 'liegen', 'in', 'der', 'Nähe', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Hotel', '<unk>', 'welcomes', 'you', 'to', 'a', 'busy', 'yet', 'quiet', 'area', 'of', 'Milan', ',', 'within', 'walking', 'distance', 'of', 'excellent', 'transport', 'links', ',', 'including', 'the', 'central', 'railway', 'station', 'and', 'the', 'Repubblica', 'metro', 'station', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Zum', 'klimatisierten', 'Hotel', 'gehören', 'auch', 'ein', '<unk>', 'und', 'eine', 'traumhafte', 'Sonnenterrasse', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Apart', 'from', 'this', ',', 'the', 'guests', 'can', 'enjoy', 'the', 'facility', 'of', 'an', 'independent', 'air', '##AT##-##AT##', 'conditioning', 'system', ',', 'a', 'jacuzzi', 'and', 'a', 'beautiful', 'sun', 'terrace', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ypb9z2TEfe5",
        "colab_type": "text"
      },
      "source": [
        "### Learning word embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU1qRIJaHzYC",
        "colab_type": "text"
      },
      "source": [
        "#### Run algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STcf6PTbF0b2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "574480a2-3c4a-409e-9f88-3f9e887fb214"
      },
      "source": [
        "# Total number of sentences\n",
        "tot_sentences = train_inputs.shape[0]\n",
        "print('Total number of training sentences: ',tot_sentences)\n",
        "\n",
        "# we keep a cursor for each sentence in the training set\n",
        "sentence_cursors = [0 for _ in range(tot_sentences)] \n",
        "\n",
        "batch_size = 64\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "\n",
        "# Defining various things needed by the python script\n",
        "define_data_and_hyperparameters(\n",
        "    tot_sentences, \n",
        "    src_max_sent_length, \n",
        "    tgt_max_sent_length, \n",
        "    src_dictionary, \n",
        "    tgt_dictionary,\n",
        "    src_reverse_dictionary, \n",
        "    tgt_reverse_dictionary, \n",
        "    train_inputs, \n",
        "    train_outputs, \n",
        "    embedding_size,\n",
        "    vocabulary_size\n",
        ")\n",
        "\n",
        "# Print some batches to make sure the data generator is correct\n",
        "print_some_batches()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of training sentences:  250000\n",
            "\n",
            "with window_size = 1:\n",
            "    batch: [['<s>', 'Ende'], ['<s>', ':'], ['<s>', 'Sie'], ['<s>', 'AB'], ['<s>', 'gibt'], ['<s>', 'hat'], ['<s>', 'nimmt'], ['<s>', 'möchten']]\n",
            "    labels: ['Seit', 'F', 'Denken', 'Der', 'Es', 'Jaca', 'Man', 'Wann']\n",
            "\n",
            "with window_size = 2:\n",
            "    batch: [['<s>', 'Betten', '13', '€'], ['<s>', '##STAR##', 'Programm', 'teilt'], ['<s>', 'Wir', 'diese', '<unk>'], ['<s>', 'Der', 'zwischen', '<unk>'], ['<s>', 'Schließlich', 'die', 'Zivilgesellschaft'], ['<s>', '<unk>', ',', '<unk>'], ['<s>', 'Das', 'besitzt', 'großzügig'], ['<s>', 'Unseren', 'steht', 'auch']]\n",
            "    labels: ['ab', 'Das', 'haben', 'Wanderweg', 'muss', 'Garten', 'Appartement', 'Gäste']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwfe-6RLKx3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "e48b5ae3-9d96-4b89-9b0d-0390c5fa3912"
      },
      "source": [
        "# Define TensorFlow ops for learning word embeddings\n",
        "define_word2vec_tensorflow(batch_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining 4 embedding lookups representing each word in the context\n",
            "Stacked embedding size: [64, 128, 4]\n",
            "Reduced mean embedding size: [64, 128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU1-ie2VL7DG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f2d500d-432b-46e6-cc4e-2fea47446508"
      },
      "source": [
        "# Run embedding learning for source language\n",
        "# Stores the de-embeddings-tmp.npy into the disk\n",
        "run_word2vec_source(batch_size)\n",
        "# Run embedding learning for target language\n",
        "# Stores the en-embeddings-tmp.npy to the disk\n",
        "run_word2vec_target(batch_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Average loss at step 2000: 3.532927\n",
            "Average loss at step 4000: 2.701178\n",
            "Average loss at step 6000: 2.484155\n",
            "Average loss at step 8000: 2.377980\n",
            "Average loss at step 10000: 2.294842\n",
            "Nearest to und: unterhält, Schenken, bisher, besinnen, Güterverkehrs, Autofokus, Tennis, Ruhige,\n",
            "Nearest to bei: Sinn, Laptop, hielten, Klimagipfel, Setzen, siegen, Yeshé, Laufe,\n",
            "Nearest to ich: Ich, Uniform, bildete, dies, beispielsweise, mysqld, down, kürzere,\n",
            "Nearest to -: Seltenheit, desk, 1790, Gesetzesvorlage, Verträge, förderlich, letztendlichen, Swisscom,\n",
            "Nearest to wenn: Wahrlich, sage, Siehe, dein, D3, weiter, daß, siehe,\n",
            "Nearest to Union: Altstadt, much, Fläche, quite, Benalmadena, Zusammenstellung, hundertprozentig, Frankfurt,\n",
            "Nearest to die: diese, formiert, Diese, Dichter, BIS, ihre, Ihre, Ihren,\n",
            "Nearest to Die: Unsere, die, Eine, der, Ferien, vegetarischen, Seine, jahre,\n",
            "Nearest to :: &#93;, página, &gt;, Kay, sicherheitsrelevanten, vous, gestiegenen, IUU,\n",
            "Nearest to alle: Mitglied, nur, Lagos, Männer, Canaria, Stärken, Dubliner, ca,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Fehlverhalten, Sicherheitsmaßnahmen, schwerwiegenden, Minderheitenschutz, Nam, abzulenken,\n",
            "Nearest to beste: Entdecke, Modem, Strecke, Somit, verließ, Stränden, kollektiven, Bewertung,\n",
            "Nearest to anderem: sitzen, mehrere, menos, eigenem, Mai, Unterrichts, häufig, Kinder,\n",
            "Nearest to Aufenthalt: Doppelbett, Fahrzeug, besuchte, Siehe, Saxophon, adresse, Genießen, kostenlosen,\n",
            "Nearest to zahlreichen: Website, Tatsache, Ankunft, Probieren, brutal, Stornierungen, optimale, entlang,\n",
            "Nearest to erfüllen: Auskünfte, Schwellenländer, unverbindliche, entlang, Fairness, Einwilligung, begleiteten, Tagungsräume,\n",
            "Nearest to Werte: geräumige, LCD, richtiges, Spanien, Verwendung, Umzüge, Ansonsten, nächste,\n",
            "Nearest to 2002: normalen, Ecofin, Bonn, tropische, 94, mühsam, dezentralisierte, Züge,\n",
            "Nearest to getan: Tips, Ihrem, lokale, Löcher, Pension, zivilen, walk, Sellaronda,\n",
            "Nearest to Natürlich: Ebenso, hier, hinaus, Wollen, Außerdem, gemütlicher, wäre, Entspannen,\n",
            "Average loss at step 12000: 2.249412\n",
            "Average loss at step 14000: 2.214257\n",
            "Average loss at step 16000: 2.170386\n",
            "Average loss at step 18000: 2.140362\n",
            "Average loss at step 20000: 2.094432\n",
            "Nearest to und: besinnen, Güterverkehrs, Schenken, versinkt, oder, bisher, Wänden, Schlampen,\n",
            "Nearest to bei: neben, Sinn, nach, hielten, Knesset, feine, anzugeben, auf,\n",
            "Nearest to ich: Ich, er, sie, sagen, kürzere, dies, bildete, mysqld,\n",
            "Nearest to -: 1790, PRI, Seltenheit, desk, Verträge, Swisscom, voor, bij,\n",
            "Nearest to wenn: Wahrlich, daß, weil, sage, Siehe, ob, weiß, werdet,\n",
            "Nearest to Union: vorsätzlich, much, Fläche, Altstadt, Gefangenschaft, quite, Zusammenstellung, Metall,\n",
            "Nearest to die: diese, ihre, Ihre, Die, Dichter, keine, eine, unsere,\n",
            "Nearest to Die: Unsere, Eine, die, Seine, eine, Frauentag, Diese, Alle,\n",
            "Nearest to :: &#93;, Kay, &gt;, nachhaltigere, sicherheitsrelevanten, gestiegenen, acht, Hinweis,\n",
            "Nearest to alle: diese, Spätere, Lagos, Stärken, Dubliner, Wölfe, Evolution, Unsere,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Fehlverhalten, Sicherheitsmaßnahmen, schwerwiegenden, Minderheitenschutz, benannt, abzulenken,\n",
            "Nearest to beste: Entdecke, Chance, Modem, bevorzugte, Figur, Zahlen, relativ, Slogan,\n",
            "Nearest to anderem: sitzen, menos, Konferenzen, Gerätes, Unterrichts, mehrere, Kinder, Versandkosten,\n",
            "Nearest to Aufenthalt: Urlaub, Doppelbett, kostenlosen, Gegner, Fahrzeug, Ceuta, Beginnen, heart,\n",
            "Nearest to zahlreichen: Verbesserung, Lied, brutal, Website, Tippen, Stornierungen, Klagen, Buchstaben,\n",
            "Nearest to erfüllen: Auskünfte, Schwellenländer, begleiteten, Einwilligung, Fairness, unterstützen, verhält, könne,\n",
            "Nearest to Werte: nächste, Schieberegler, richtiges, Position, Verwendung, Klicke, geräumige, LCD,\n",
            "Nearest to 2002: 2004, tropische, normalen, Ecofin, Bonn, 94, Pflicht, Stühle,\n",
            "Nearest to getan: Tips, Löcher, Antiquitäten, gepflegt, töten, lokale, Ergänzungen, wünsche,\n",
            "Nearest to Natürlich: Ebenso, Außerdem, Wollen, hier, genau, Dabei, Somit, hinaus,\n",
            "Average loss at step 22000: 2.058442\n",
            "Average loss at step 24000: 2.020735\n",
            "Average loss at step 26000: 1.985546\n",
            "Average loss at step 28000: 1.965755\n",
            "Average loss at step 30000: 1.922434\n",
            "Nearest to und: besinnen, oder, Güterverkehrs, Schenken, Schlampen, versinkt, Zink, bisher,\n",
            "Nearest to bei: neben, Knesset, wegen, hielten, anzugeben, nach, derselbe, Hierin,\n",
            "Nearest to ich: Ich, er, sie, sagen, dies, jemand, Bundesregierung, mysqld,\n",
            "Nearest to -: 1790, Portland, Hörner, erarbeiten, virtuelles, Gelb, Jungen, absolutely,\n",
            "Nearest to wenn: weil, Wahrlich, falls, daß, werdet, da, ob, sieht,\n",
            "Nearest to Union: vorsätzlich, Altstadt, Gefangenschaft, Rand, Zusammenstellung, much, strukturieren, Frankfurt,\n",
            "Nearest to die: diese, ihre, Die, Ihre, keine, Dichter, unsere, einige,\n",
            "Nearest to Die: Eine, Unsere, Diese, die, Seine, Gute, Frauentag, Alle,\n",
            "Nearest to :: &#93;, Kay, nachhaltigere, mittelfristig, Ladegeräte, acht, sicherheitsrelevanten, Hinweis,\n",
            "Nearest to alle: diese, Spätere, Männer, Zustellbetten, Aussichtsplattform, jeweils, Statistiken, weitere,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Sicherheitsmaßnahmen, Fehlverhalten, schwerwiegenden, benannt, Minderheitenschutz, Vollmitgliedschaft,\n",
            "Nearest to beste: Entdecke, bevorzugte, Slogan, perfekte, exzellent, verboten, Spezialität, Modem,\n",
            "Nearest to anderem: Umständen, sitzen, vielseitige, mehrere, Unterrichts, Getränken, Gerätes, Leitung,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Fahrzeug, Ceuta, Doppelbett, Überblick, Visualisierung, Gegner,\n",
            "Nearest to zahlreichen: Klagen, umliegenden, modernem, ihrer, Bürgerkrieg, Anwendern, Verbesserung, betreuten,\n",
            "Nearest to erfüllen: unterstützen, Schwellenländer, begleiteten, Auskünfte, Einwilligung, Fairness, könne, Claude,\n",
            "Nearest to Werte: nächste, Schieberegler, Sonderpreise, Englischen, Umzüge, richtiges, Lastschrift, Reisezeit,\n",
            "Nearest to 2002: 2004, Mai, 2008, 2005, 1986, 1968, Ecofin, August,\n",
            "Nearest to getan: Löcher, wünsche, töten, Tips, Hallenbad, lokale, Antiquitäten, duldet,\n",
            "Nearest to Natürlich: Ebenso, genau, Wollen, hier, hierzu, Außerdem, Weiterhin, Gelegenheit,\n",
            "Average loss at step 32000: 1.892842\n",
            "Average loss at step 34000: 1.871343\n",
            "Average loss at step 36000: 1.831999\n",
            "Average loss at step 38000: 1.800740\n",
            "Average loss at step 40000: 1.773401\n",
            "Nearest to und: besinnen, oder, Güterverkehrs, Zink, Schlampen, versinkt, zurücklassen, Baldo,\n",
            "Nearest to bei: wegen, neben, Knesset, anzugeben, folgen, hielten, Belarussen, mit,\n",
            "Nearest to ich: Ich, er, sie, sagen, jemand, Bundesregierung, mysqld, verehrter,\n",
            "Nearest to -: Portland, 1790, erarbeiten, Gelb, Hörner, absolutely, Startseite, virtuelles,\n",
            "Nearest to wenn: falls, weil, Wahrlich, sieht, da, werdet, daß, braucht,\n",
            "Nearest to Union: vorsätzlich, Gefangenschaft, Rand, much, Altstadt, Zusammenstellung, nahe, strukturieren,\n",
            "Nearest to die: diese, ihre, Die, keine, seine, Ihre, einige, unsere,\n",
            "Nearest to Die: Unsere, Eine, Diese, die, Seine, Frauentag, Alle, Zur,\n",
            "Nearest to :: &#93;, Kay, mittelfristig, nachhaltigere, ad, acht, Ladegeräte, Ferry,\n",
            "Nearest to alle: sämtliche, jeweils, Welche, aller, diese, Jerusalems, Alle, Bleche,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Sicherheitsmaßnahmen, Fehlverhalten, schwerwiegenden, Minderheitenschutz, benannt, Hillary,\n",
            "Nearest to beste: perfekte, bevorzugte, ideale, exzellent, Entdecke, Spezialität, Slogan, Writer,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, vielseitige, sitzen, Unterrichts, http, Panoramablick, Schwierigkeitsgrade,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Ceuta, Visualisierung, Rahmen, Fahrzeug, Überblick, Doppelbett,\n",
            "Nearest to zahlreichen: modernem, Klagen, umliegenden, belassen, öffentlichen, Anwendern, Vermietung, ihrer,\n",
            "Nearest to erfüllen: spielen, unterstützen, könne, begleiteten, angibt, Auskünfte, halten, Schwellenländer,\n",
            "Nearest to Werte: nächste, Schieberegler, Lastschrift, Reisezeit, Plattformen, Sonderpreise, Umzüge, Röhre,\n",
            "Nearest to 2002: 2004, 2005, 2008, 2003, Mai, Oktober, 2006, April,\n",
            "Nearest to getan: Löcher, wünsche, töten, tun, Hallenbad, Tips, Geld, Antiquitäten,\n",
            "Nearest to Natürlich: Ebenso, genau, Wollen, Jetzt, hier, sogar, Somit, Gelegenheit,\n",
            "Average loss at step 42000: 1.746793\n",
            "Average loss at step 44000: 1.729458\n",
            "Average loss at step 46000: 1.694936\n",
            "Average loss at step 48000: 1.681684\n",
            "Average loss at step 50000: 1.665288\n",
            "Nearest to und: besinnen, oder, Zink, Güterverkehrs, versinkt, Schlampen, Baldo, dachten,\n",
            "Nearest to bei: wegen, Bei, hinter, hielten, Knesset, folgen, Hierin, neben,\n",
            "Nearest to ich: Ich, er, sie, jemand, sagen, mysqld, Bundesregierung, verehrter,\n",
            "Nearest to -: Startseite, Hörner, erarbeiten, Portland, Gelb, vielleicht, virtuelles, 1790,\n",
            "Nearest to wenn: falls, weil, Wahrlich, sieht, braucht, sobald, ob, dann,\n",
            "Nearest to Union: vorsätzlich, Rand, much, strukturieren, Altstadt, Charles, Gefangenschaft, Zusammenstellung,\n",
            "Nearest to die: diese, ihre, Die, keine, deren, unsere, seine, einige,\n",
            "Nearest to Die: Unsere, Eine, Diese, Seine, die, Zur, Frauentag, Alle,\n",
            "Nearest to :: Kay, &#93;, ad, mittelfristig, nachhaltigere, Alt, acht, Ladegeräte,\n",
            "Nearest to alle: sämtliche, Welche, aller, jeweils, Italienischen, allen, Jerusalems, Alle,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Sicherheitsmaßnahmen, Fehlverhalten, schwerwiegenden, benannt, Minderheitenschutz, landet,\n",
            "Nearest to beste: perfekte, ideale, bevorzugte, Entdecke, exzellent, Writer, Spezialität, sonniger,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Unterrichts, vielseitige, Konfigurationsdateien, Jahren, Schwierigkeitsgrade, Panoramablick,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Ceuta, Wünschen, Rahmen, Visualisierung, Fitnessraum, Alp,\n",
            "Nearest to zahlreichen: vielen, modernem, belassen, umliegenden, Klagen, verschiedenen, Vermietung, tropischen,\n",
            "Nearest to erfüllen: spielen, beenden, unterstützen, begleiteten, könne, angibt, halten, dürfen,\n",
            "Nearest to Werte: nächste, Lastschrift, Plattformen, Sonderpreise, Regionen, Reisezeit, Schieberegler, Überweisung,\n",
            "Nearest to 2002: 2004, 2005, 2003, 2006, Oktober, Mai, 2007, 2008,\n",
            "Nearest to getan: tun, wünsche, Löcher, töten, Geld, darum, mitteilen, erstaunlich,\n",
            "Nearest to Natürlich: Ebenso, Außerdem, Was, Weiterhin, hierzu, Leider, Dabei, Somit,\n",
            "Average loss at step 52000: 1.640508\n",
            "Average loss at step 54000: 1.621003\n",
            "Average loss at step 56000: 1.613053\n",
            "Average loss at step 58000: 1.581456\n",
            "Average loss at step 60000: 1.580258\n",
            "Nearest to und: besinnen, oder, Baldo, Zink, Shakespeares, Quotenregelung, versinkt, Schlampen,\n",
            "Nearest to bei: Bei, wegen, anzugeben, Hierin, Knesset, hinter, hielten, folgen,\n",
            "Nearest to ich: Ich, er, jemand, sagen, sie, Bundesregierung, mysqld, man,\n",
            "Nearest to -: Startseite, erarbeiten, vielleicht, Portland, substantielle, 1790, virtuelles, Mitleidenschaft,\n",
            "Nearest to wenn: falls, weil, Wahrlich, ob, indem, sobald, braucht, sieht,\n",
            "Nearest to Union: vorsätzlich, Kommission, Zusammenstellung, Rand, Skipiste, Altstadt, Agenda, Gefangenschaft,\n",
            "Nearest to die: diese, ihre, bestimmte, Die, keine, deren, seine, unsere,\n",
            "Nearest to Die: Unsere, Eine, Seine, Diese, die, Zur, Jede, Frauentag,\n",
            "Nearest to :: Kay, nachhaltigere, Alt, &#93;, ad, mittelfristig, kaufe, Ladegeräte,\n",
            "Nearest to alle: sämtliche, Welche, allen, jeweils, Alle, aller, nur, Italienischen,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Sicherheitsmaßnahmen, Fehlverhalten, schwerwiegenden, benannt, Imam, Minderheitenschutz,\n",
            "Nearest to beste: perfekte, ideale, Writer, Entdecke, exzellent, bevorzugte, richtige, größte,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Unterrichts, Kind, vielseitige, Panoramablick, Konfigurationsdateien, Jahren,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Wünschen, angenehmen, Rahmen, Ceuta, Gegner, Aufschwung,\n",
            "Nearest to zahlreichen: vielen, umliegenden, verschiedenen, Klagen, modernem, belassen, Bars, Theatern,\n",
            "Nearest to erfüllen: beenden, unterstützen, spielen, angibt, Rechenschaft, begleiteten, halten, dürfen,\n",
            "Nearest to Werte: Lastschrift, Regionen, Reisezeit, Plattformen, Knowledge, nächste, Wirksamkeit, Verwendung,\n",
            "Nearest to 2002: 2004, 2005, Mai, 2003, 2006, November, August, 1968,\n",
            "Nearest to getan: tun, wünsche, Löcher, töten, Geld, erstaunlich, darum, denn,\n",
            "Nearest to Natürlich: Ebenso, Außerdem, Dabei, Weiterhin, Leider, Was, hierzu, Heute,\n",
            "Average loss at step 62000: 1.565125\n",
            "Average loss at step 64000: 1.552753\n",
            "Average loss at step 66000: 1.530380\n",
            "Average loss at step 68000: 1.519418\n",
            "Average loss at step 70000: 1.503824\n",
            "Nearest to und: besinnen, oder, Zink, Baldo, versinkt, Schlampen, Shakespeares, Quotenregelung,\n",
            "Nearest to bei: Bei, hinter, Knesset, hielten, folgen, anzugeben, Hierin, wegen,\n",
            "Nearest to ich: Ich, er, jemand, sie, sagen, man, Netzbetreiber, vorkommen,\n",
            "Nearest to -: Startseite, vielleicht, substantielle, erarbeiten, absolutely, –, Mitleidenschaft, Hörner,\n",
            "Nearest to wenn: falls, weil, dann, sobald, ob, Wahrlich, sodass, bevor,\n",
            "Nearest to Union: vorsätzlich, Kommission, Skipiste, Charles, Altstadt, Type, Pension, nächst,\n",
            "Nearest to die: ihre, diese, bestimmte, Die, keine, deren, seine, unsere,\n",
            "Nearest to Die: Unsere, Eine, Seine, Diese, die, Zur, Jede, Frauentag,\n",
            "Nearest to :: Kay, nachhaltigere, —, Alt, Ladegeräte, », kaufe, ad,\n",
            "Nearest to alle: sämtliche, allen, Welche, Alle, Jerusalems, aller, jeweils, nur,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Fehlverhalten, Sicherheitsmaßnahmen, schwerwiegenden, benannt, Minderheitenschutz, Imam,\n",
            "Nearest to beste: perfekte, ideale, richtige, bevorzugte, Entdecke, Writer, exzellent, automatische,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Berücksichtigung, Kind, Leitung, vielseitige, Konfigurationsdateien, Unterrichts,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Wünschen, angenehmen, Ausflug, Weg, Aufschwung, Traum,\n",
            "Nearest to zahlreichen: vielen, verschiedenen, umliegenden, Klagen, Bars, edlen, diversen, Theatern,\n",
            "Nearest to erfüllen: unterstützen, beenden, spielen, schützen, behalten, übernehmen, Rechenschaft, angibt,\n",
            "Nearest to Werte: Lastschrift, Regionen, Knowledge, Plattformen, Verwendung, Reisezeit, Versorgungsunternehmen, Rechnung,\n",
            "Nearest to 2002: 2004, 2005, 2003, 2006, Mai, November, August, 2007,\n",
            "Nearest to getan: tun, wünsche, erstaunlich, töten, darum, Geld, mitteilen, Straßenlärm,\n",
            "Nearest to Natürlich: Ebenso, Außerdem, Weiterhin, Dabei, Leider, Somit, Was, Allerdings,\n",
            "Average loss at step 72000: 1.499181\n",
            "Average loss at step 74000: 1.474416\n",
            "Average loss at step 76000: 1.480925\n",
            "Average loss at step 78000: 1.464711\n",
            "Average loss at step 80000: 1.454757\n",
            "Nearest to und: besinnen, oder, Baldo, Zink, Shakespeares, Quotenregelung, versinkt, funktionsfähigen,\n",
            "Nearest to bei: Bei, hinter, anzugeben, Hierin, wegen, Referenzwert, Knesset, trotz,\n",
            "Nearest to ich: Ich, er, jemand, man, sie, Netzbetreiber, vorkommen, verweisen,\n",
            "Nearest to -: Startseite, erarbeiten, substantielle, Mitleidenschaft, vielleicht, 143, Hörner, wohl,\n",
            "Nearest to wenn: falls, weil, sobald, sodass, ob, bevor, indem, Wahrlich,\n",
            "Nearest to Union: Kommission, Agenda, Altstadt, Koordination, Type, Skipiste, strukturieren, vorsätzlich,\n",
            "Nearest to die: diese, ihre, Die, bestimmte, deren, seine, jede, keine,\n",
            "Nearest to Die: Unsere, Seine, Eine, Diese, die, Jede, Welche, Zur,\n",
            "Nearest to :: Kay, —, nachhaltigere, &#93;, », ., Alt, ##STAR##,\n",
            "Nearest to alle: sämtliche, allen, Alle, Welche, Jerusalems, aller, jeweils, nur,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, schwerwiegenden, Fehlverhalten, Sicherheitsmaßnahmen, benannt, Minderheitenschutz, Imam,\n",
            "Nearest to beste: perfekte, ideale, richtige, bevorzugte, führende, exzellent, automatische, optimale,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Kind, http, Leitung, vielseitige, Leistungsspektrum, ihnen,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Wünschen, angenehmen, Sommerurlaub, Traum, Aufschwung, Fitnessraum,\n",
            "Nearest to zahlreichen: vielen, umliegenden, verschiedenen, Klagen, kleineren, Theatern, belassen, edlen,\n",
            "Nearest to erfüllen: beenden, schützen, unterstützen, spielen, behalten, Rechenschaft, halten, erklären,\n",
            "Nearest to Werte: Plattformen, Knowledge, Lastschrift, Verwendung, Rechnung, Reisezeit, Pakete, Eintauchen,\n",
            "Nearest to 2002: 2004, 2005, 2003, 2006, Mai, November, August, 1986,\n",
            "Nearest to getan: tun, wünsche, erstaunlich, darum, geheizt, gesagt, töten, Geld,\n",
            "Nearest to Natürlich: Ebenso, Weiterhin, Außerdem, Dabei, Leider, Was, Deshalb, Allerdings,\n",
            "Average loss at step 82000: 1.460487\n",
            "Average loss at step 84000: 1.432748\n",
            "Average loss at step 86000: 1.428075\n",
            "Average loss at step 88000: 1.430211\n",
            "Average loss at step 90000: 1.416314\n",
            "Nearest to und: besinnen, Baldo, oder, Zink, Shakespeares, funktionsfähigen, Schlampen, Quotenregelung,\n",
            "Nearest to bei: Bei, anzugeben, wegen, hinter, Hierin, Referenzwert, trotz, elementarer,\n",
            "Nearest to ich: Ich, er, jemand, man, sie, vorkommen, verweisen, Netzbetreiber,\n",
            "Nearest to -: Startseite, erarbeiten, –, Hörner, substantielle, wohl, Mitleidenschaft, funktionierte,\n",
            "Nearest to wenn: falls, weil, sobald, indem, bevor, wobei, sodass, Wahrlich,\n",
            "Nearest to Union: Kommission, Type, vorsätzlich, Agenda, strukturieren, Pension, nächst, Koordination,\n",
            "Nearest to die: diese, ihre, bestimmte, Die, seine, deren, unsere, dieselbe,\n",
            "Nearest to Die: Unsere, Seine, Diese, Eine, die, Jede, Welche, Meine,\n",
            "Nearest to :: &#93;, Kay, —, nachhaltigere, », ., ##STAR##, sozialdemokratische,\n",
            "Nearest to alle: sämtliche, allen, Alle, Welche, jeweils, nur, Jerusalems, aller,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, schwerwiegenden, Fehlverhalten, benannt, Sicherheitsmaßnahmen, Minderheitenschutz, MEZ,\n",
            "Nearest to beste: perfekte, ideale, richtige, optimale, bevorzugte, führende, exzellent, automatische,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Leistungsspektrum, Leitung, http, vielseitige, Kind, Konfigurationsdateien,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Wünschen, angenehmen, Sommerurlaub, Aufschwung, Ausflug, Fitnessraum,\n",
            "Nearest to zahlreichen: vielen, umliegenden, verschiedenen, Klagen, Theatern, archäologischen, kleineren, edlen,\n",
            "Nearest to erfüllen: beenden, schützen, unterstützen, behalten, spielen, erklären, informieren, beobachten,\n",
            "Nearest to Werte: Regionen, Plattformen, Knowledge, Reisezeit, Pakete, Rechnung, Lastschrift, Devisenmarkt,\n",
            "Nearest to 2002: 2004, 2005, 2006, 2003, Mai, August, November, 1986,\n",
            "Nearest to getan: tun, wünsche, gesagt, Geld, darum, erstaunlich, alles, töten,\n",
            "Nearest to Natürlich: Ebenso, Weiterhin, Außerdem, Leider, Was, Allerdings, Dabei, Somit,\n",
            "Average loss at step 92000: 1.410688\n",
            "Average loss at step 94000: 1.409906\n",
            "Average loss at step 96000: 1.400469\n",
            "Average loss at step 98000: 1.394817\n",
            "Average loss at step 100000: 1.386351\n",
            "Nearest to und: besinnen, Baldo, oder, Zink, Shakespeares, funktionsfähigen, sowie, Quotenregelung,\n",
            "Nearest to bei: Bei, anzugeben, hinter, Hierin, wegen, Referenzwert, trotz, hielten,\n",
            "Nearest to ich: Ich, er, jemand, man, vorkommen, sie, Netzbetreiber, sagen,\n",
            "Nearest to -: Startseite, substantielle, wohl, –, erarbeiten, Aufsichtsratsmitglieder, Mitleidenschaft, Hörner,\n",
            "Nearest to wenn: falls, weil, sobald, solange, sodass, obwohl, indem, wohin,\n",
            "Nearest to Union: Kommission, Type, vorsätzlich, Agenda, strukturieren, Pension, Zentralbank, nächst,\n",
            "Nearest to die: diese, ihre, bestimmte, Die, deren, unsere, seine, dieselbe,\n",
            "Nearest to Die: Seine, Unsere, Diese, Eine, die, Jede, Keine, Welche,\n",
            "Nearest to :: &#93;, —, Kay, », nachhaltigere, ##STAR##, sozialdemokratische, .,\n",
            "Nearest to alle: sämtliche, allen, Alle, jeweils, Welche, nur, einzelne, Metaphern,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, schwerwiegenden, Fehlverhalten, benannt, Sicherheitsmaßnahmen, MEZ, Minderheitenschutz,\n",
            "Nearest to beste: perfekte, ideale, richtige, optimale, führende, größte, bevorzugte, exzellent,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Leitung, Leistungsspektrum, Berücksichtigung, vielseitige, ihnen, Kind,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Sommerurlaub, Aufschwung, angenehmen, Wünschen, Drink, Fitnessraum,\n",
            "Nearest to zahlreichen: vielen, umliegenden, Klagen, verschiedenen, edlen, Theatern, archäologischen, italienischen,\n",
            "Nearest to erfüllen: beenden, schützen, verarbeiten, unterstützen, behalten, reduzieren, erklären, spielen,\n",
            "Nearest to Werte: Regionen, Plattformen, Länder, Pakete, Knowledge, Rechnung, Wirksamkeit, Nutzung,\n",
            "Nearest to 2002: 2004, 2006, 2005, 2003, 2001, Mai, November, 1986,\n",
            "Nearest to getan: tun, wünsche, darum, gesagt, verkündet, Geld, töten, Glück,\n",
            "Nearest to Natürlich: Weiterhin, Ebenso, Außerdem, Allerdings, Deshalb, Dabei, Leider, Heute,\n",
            "Initialized\n",
            "Average loss at step 2000: 4.275495\n",
            "Average loss at step 4000: 2.818988\n",
            "Average loss at step 6000: 2.494669\n",
            "Average loss at step 8000: 2.316638\n",
            "Average loss at step 10000: 2.209912\n",
            "Nearest to and: GL, places, machining, eject, venue, pillows, Florian, Fontainebleau,\n",
            "Nearest to would: should, may, paid, will, signatories, shall, could, requirements,\n",
            "Nearest to can: will, may, must, could, should, cannot, gives, to,\n",
            "Nearest to your: the, our, its, 1979, my, their, any, this,\n",
            "Nearest to about: word, Kabila, after, Are, Canaries, Reuters, ply, again,\n",
            "Nearest to no: great, 2009, Lindh, een, sixty, dash, only, cargo,\n",
            "Nearest to .: ?, Clubs, MILAN, lago, Pinnacle, !, deadlines, sounding,\n",
            "Nearest to you: You, we, I, riches, something, ye, advanced, please,\n",
            "Nearest to European: Seville, BNC, book, trees, Madrid, flag, b, museum,\n",
            "Nearest to out: probably, tradition, ceiling, total, taken, sense, Burmese, protection,\n",
            "Nearest to regulations: active, name, barbecue, stored, departure, countries, Deposit, Kingdom,\n",
            "Nearest to property: week, final, appropriate, doubt, path, review, highest, charge,\n",
            "Nearest to response: stations, lobby, look, maximum, U, Group, High, sensitive,\n",
            "Nearest to priority: carry, tomorrow, line, mixer, reputation, 179, cabin, shockingly,\n",
            "Nearest to gives: can, liked, entrada, simply, manually, Derivatives, boldness, calmed,\n",
            "Nearest to December: cozy, 17.00, Berlin, Karolina, Durham, Yes, Chairman, Likewise,\n",
            "Nearest to categories: Munich, Islands, stored, 600, US, Assad, date, Vulcan,\n",
            "Nearest to setting: simple, unique, responsibility, n, art, fruits, example, winter,\n",
            "Nearest to latest: current, kick, visit, 1799, cathedral, thoroughly, calendar, Cluny,\n",
            "Nearest to external: snacks, subject, Next, years, migrate, burning, progress, 13,\n",
            "Average loss at step 12000: 2.117605\n",
            "Average loss at step 14000: 2.060637\n",
            "Average loss at step 16000: 2.007933\n",
            "Average loss at step 18000: 1.948840\n",
            "Average loss at step 20000: 1.906113\n",
            "Nearest to and: Fontainebleau, Florian, GL, eject, Lastly, decorators, 880, Tone,\n",
            "Nearest to would: should, may, Would, could, shall, must, will, &apos;ll,\n",
            "Nearest to can: may, will, could, should, must, &apos;ll, cannot, gives,\n",
            "Nearest to your: my, our, his, the, their, its, Your, any,\n",
            "Nearest to about: Reuters, regarding, word, after, than, staged, Once, supports,\n",
            "Nearest to no: Lindh, sixty, earnestly, Gérard, a, cone, easier, klein,\n",
            "Nearest to .: ?, Clubs, MILAN, GDS, Pinnacle, deadlines, sounding, lago,\n",
            "Nearest to you: You, we, ye, I, they, riches, listings, something,\n",
            "Nearest to European: BNC, ETS, flag, Madrid, 182, Seville, Flensburg, ABAP,\n",
            "Nearest to out: probably, pipe, Burmese, sense, rewards, Results, ceiling, sin,\n",
            "Nearest to regulations: active, barbecue, visitors, countries, gems, questionable, departure, games,\n",
            "Nearest to property: final, Framework, module, doubt, week, heal, author, 13th,\n",
            "Nearest to response: lobby, Treat, Relax, lead, manufacturer, bays, weekly, cash,\n",
            "Nearest to priority: tomorrow, benefit, 179, cabin, GNOME, reputation, throw, finest,\n",
            "Nearest to gives: can, liked, Are, awaits, lets, Once, Download, Derivatives,\n",
            "Nearest to December: March, Durham, 05, July, October, Tampa, Poland, Karolina,\n",
            "Nearest to categories: Munich, stored, reminiscent, date, independence, examples, intention, presence,\n",
            "Nearest to setting: simple, responsibility, unique, seafront, firewall, offering, column, thoughtful,\n",
            "Nearest to latest: kick, current, calendar, 1799, Cluny, thoroughly, field, owner,\n",
            "Nearest to external: snacks, angel, burning, wilderness, progress, years, replaced, replace,\n",
            "Average loss at step 22000: 1.878093\n",
            "Average loss at step 24000: 1.851656\n",
            "Average loss at step 26000: 1.812632\n",
            "Average loss at step 28000: 1.780041\n",
            "Average loss at step 30000: 1.761838\n",
            "Nearest to and: Florian, Lastly, GL, Fontainebleau, eject, flea, complacency, selling,\n",
            "Nearest to would: should, Would, may, could, must, will, shall, might,\n",
            "Nearest to can: should, may, could, must, &apos;ll, will, cannot, Can,\n",
            "Nearest to your: my, Your, our, their, his, its, the, any,\n",
            "Nearest to about: Reuters, than, regarding, ply, Once, 309, inability, staged,\n",
            "Nearest to no: Lindh, cone, earnestly, civilizations, easier, any, a, Gérard,\n",
            "Nearest to .: ?, Clubs, Automatically, Pinnacle, MILAN, GDS, sounding, lago,\n",
            "Nearest to you: You, we, ye, they, I, listings, We, riches,\n",
            "Nearest to European: BNC, Madrid, nineteenth, flag, Seville, ETS, south, Hockey,\n",
            "Nearest to out: pipe, probably, sin, sense, Realizing, efficiently, rewards, up,\n",
            "Nearest to regulations: active, ordinances, visitors, barbecue, mandarins, potatoes, questionable, games,\n",
            "Nearest to property: Framework, perspective, module, final, doubt, author, heal, Vector,\n",
            "Nearest to response: Treat, manufacturer, Relax, lobby, bays, lead, recover, weekly,\n",
            "Nearest to priority: tomorrow, reputation, 179, GNOME, cabin, flair, benefit, finest,\n",
            "Nearest to gives: Are, can, lets, awaits, offers, liked, allows, Once,\n",
            "Nearest to December: March, July, April, October, 05, September, June, 59,\n",
            "Nearest to categories: reminiscent, spread, stored, date, Munich, Colorado, reducing, tools,\n",
            "Nearest to setting: firewall, unique, simple, seafront, shame, intimate, characteristic, responsibility,\n",
            "Nearest to latest: calendar, current, kick, quickest, meantime, 1970s, Cluny, announcement,\n",
            "Nearest to external: angel, snacks, ace, licence, burning, online, progress, industrial,\n",
            "Average loss at step 32000: 1.745115\n",
            "Average loss at step 34000: 1.718280\n",
            "Average loss at step 36000: 1.698330\n",
            "Average loss at step 38000: 1.677179\n",
            "Average loss at step 40000: 1.663252\n",
            "Nearest to and: Florian, GL, Lastly, Fontainebleau, flea, Comedy, complacency, reservas,\n",
            "Nearest to would: Would, should, may, could, &apos;d, might, must, will,\n",
            "Nearest to can: should, must, may, could, &apos;ll, cannot, will, Can,\n",
            "Nearest to your: Your, my, their, our, his, the, its, bacterial,\n",
            "Nearest to about: regarding, Reuters, 309, ply, than, Rodney, coke, SEK,\n",
            "Nearest to no: any, Lindh, easier, earnestly, cone, a, No, civilizations,\n",
            "Nearest to .: ?, Automatically, Clubs, Pinnacle, sounding, MILAN, Return, !,\n",
            "Nearest to you: You, we, ye, they, listings, I, We, Guests,\n",
            "Nearest to European: Soviet, Madrid, BNC, Seville, museum, trumped, south, flag,\n",
            "Nearest to out: pipe, up, Realizing, efficiently, rewards, sense, probably, frogs,\n",
            "Nearest to regulations: ordinances, visitors, barbecue, active, tourists, friends, gems, mandarins,\n",
            "Nearest to property: hotel, perspective, Framework, module, final, doubt, conclusion, alias,\n",
            "Nearest to response: Treat, lead, extend, recover, unwind, preference, winnings, swim,\n",
            "Nearest to priority: reputation, tomorrow, cabin, flair, benefit, 179, roundabout, goal,\n",
            "Nearest to gives: Are, lets, awaits, offers, can, allows, welcomes, overlooks,\n",
            "Nearest to December: March, April, July, October, September, June, 2007, November,\n",
            "Nearest to categories: date, spread, donation, Munich, reminiscent, proposals, days, patents,\n",
            "Nearest to setting: unique, firewall, simple, intimate, sight, tankers, cafe, shame,\n",
            "Nearest to latest: current, calendar, 1799, announcement, certificate, meantime, kick, quickest,\n",
            "Nearest to external: angel, industrial, cosmological, online, ace, snacks, burning, licence,\n",
            "Average loss at step 42000: 1.642807\n",
            "Average loss at step 44000: 1.625437\n",
            "Average loss at step 46000: 1.606461\n",
            "Average loss at step 48000: 1.611256\n",
            "Average loss at step 50000: 1.591373\n",
            "Nearest to and: Florian, Lastly, almonds, GL, murderous, but, or, complacency,\n",
            "Nearest to would: Would, should, may, might, &apos;d, will, could, must,\n",
            "Nearest to can: may, should, &apos;ll, must, cannot, could, will, Can,\n",
            "Nearest to your: Your, my, their, our, his, its, bacterial, GENERAL,\n",
            "Nearest to about: regarding, concerning, Reuters, 309, ply, coke, palatable, sunbathe,\n",
            "Nearest to no: No, any, Lindh, easier, a, cone, earnestly, formatting,\n",
            "Nearest to .: ?, Pinnacle, Automatically, Clubs, MILAN, niches, Return, sounding,\n",
            "Nearest to you: You, we, ye, they, I, listings, Guests, guests,\n",
            "Nearest to European: Soviet, BNC, trumped, injection, conveyance, Madrid, museum, Seville,\n",
            "Nearest to out: up, pipe, Realizing, rewards, frogs, sense, efficiently, Lastovo,\n",
            "Nearest to regulations: active, visitors, ordinances, sport, mandarins, Reservations, flowers, souls,\n",
            "Nearest to property: hotel, perspective, final, Framework, alias, Vector, expanding, alliance,\n",
            "Nearest to response: Treat, lead, extend, recover, raise, winnings, prove, preference,\n",
            "Nearest to priority: roundabout, tomorrow, baptized, cabin, goal, reputation, paddle, 179,\n",
            "Nearest to gives: Are, lets, awaits, offers, allows, brings, ensures, can,\n",
            "Nearest to December: March, April, July, October, September, June, November, May,\n",
            "Nearest to categories: independence, airports, donation, date, reminiscent, days, patents, spread,\n",
            "Nearest to setting: tankers, unique, firewall, intimate, seafront, location, shame, simple,\n",
            "Nearest to latest: current, 1799, calendar, certificate, announcement, bugs, Arcade, firmware,\n",
            "Nearest to external: industrial, angel, emergency, ace, cosmological, snacks, additional, extended,\n",
            "Average loss at step 52000: 1.578495\n",
            "Average loss at step 54000: 1.553770\n",
            "Average loss at step 56000: 1.546509\n",
            "Average loss at step 58000: 1.541772\n",
            "Average loss at step 60000: 1.527534\n",
            "Nearest to and: Florian, almonds, Lastly, or, complacency, hygienic, but, flea,\n",
            "Nearest to would: Would, should, &apos;d, may, might, could, will, Will,\n",
            "Nearest to can: should, may, cannot, &apos;ll, could, must, Can, will,\n",
            "Nearest to your: Your, my, their, our, his, her, GENERAL, its,\n",
            "Nearest to about: regarding, concerning, 309, ply, than, Rodney, palatable, Reuters,\n",
            "Nearest to no: No, any, easier, Lindh, formatting, cone, Gérard, iPhoto,\n",
            "Nearest to .: ?, Automatically, Pinnacle, sounding, !, MILAN, Clubs, niches,\n",
            "Nearest to you: You, we, they, ye, I, guests, Guests, anyone,\n",
            "Nearest to European: Soviet, trumped, glorious, BNC, southern, Madrid, segment, farms,\n",
            "Nearest to out: up, pipe, frogs, Realizing, psychology, back, rewards, hard,\n",
            "Nearest to regulations: active, visitors, sport, mandarins, brothers, flowers, ordinances, grandeur,\n",
            "Nearest to property: hotel, alliance, perspective, Vector, doubt, conclusion, alias, expanding,\n",
            "Nearest to response: Treat, prove, lead, raise, extend, belonging, preference, recover,\n",
            "Nearest to priority: roundabout, reputation, goal, cabin, tomorrow, baptized, 179, landlord,\n",
            "Nearest to gives: lets, Are, offers, awaits, brings, allows, provides, ensures,\n",
            "Nearest to December: April, March, July, October, November, September, June, May,\n",
            "Nearest to categories: days, examples, chances, characters, independence, intention, date, levels,\n",
            "Nearest to setting: characteristic, tankers, shame, seafront, firewall, intimate, unique, simple,\n",
            "Nearest to latest: current, integrated, Arcade, newest, bugs, trial, certificate, announcement,\n",
            "Nearest to external: industrial, emergency, HTML, cosmological, angel, extended, LTSP, additional,\n",
            "Average loss at step 62000: 1.520769\n",
            "Average loss at step 64000: 1.518846\n",
            "Average loss at step 66000: 1.492472\n",
            "Average loss at step 68000: 1.501296\n",
            "Average loss at step 70000: 1.485396\n",
            "Nearest to and: complacency, Florian, or, almonds, Lastly, but, eject, while,\n",
            "Nearest to would: Would, should, may, &apos;d, might, will, Will, could,\n",
            "Nearest to can: should, cannot, may, could, &apos;ll, Can, must, lets,\n",
            "Nearest to your: Your, my, their, his, our, her, its, bacterial,\n",
            "Nearest to about: regarding, concerning, 309, Rodney, ply, palatable, than, approximately,\n",
            "Nearest to no: No, any, Lindh, easier, greedy, plenty, formatting, iPhoto,\n",
            "Nearest to .: ?, Automatically, Pinnacle, !, MILAN, sounding, Return, modernise,\n",
            "Nearest to you: You, we, they, ye, guests, I, anyone, We,\n",
            "Nearest to European: Soviet, trumped, conveyance, BNC, new, British, southern, economics,\n",
            "Nearest to out: up, pipe, back, Realizing, hard, frogs, psychology, ceiling,\n",
            "Nearest to regulations: Reservations, efforts, tourists, voices, ordinances, active, qualities, souls,\n",
            "Nearest to property: hotel, perspective, alliance, conclusion, alias, author, Vector, appearance,\n",
            "Nearest to response: Treat, fill, prove, belonging, lead, preference, extend, raise,\n",
            "Nearest to priority: roundabout, goal, reputation, tomorrow, cabin, 179, landlord, perception,\n",
            "Nearest to gives: brings, awaits, offers, Are, lets, allows, keeps, enjoys,\n",
            "Nearest to December: April, March, July, October, November, September, June, May,\n",
            "Nearest to categories: levels, days, examples, sizes, chances, spread, teams, taxes,\n",
            "Nearest to setting: shame, tankers, characteristic, appointment, wake, sight, ambience, seafront,\n",
            "Nearest to latest: current, newest, integrated, Arcade, trends, firmware, locale, certificate,\n",
            "Nearest to external: emergency, industrial, HTML, extended, automation, cosmological, multimedia, functions,\n",
            "Average loss at step 72000: 1.484868\n",
            "Average loss at step 74000: 1.472304\n",
            "Average loss at step 76000: 1.462059\n",
            "Average loss at step 78000: 1.459139\n",
            "Average loss at step 80000: 1.455463\n",
            "Nearest to and: but, reservas, Lastly, or, almonds, complacency, while, eject,\n",
            "Nearest to would: Would, should, &apos;d, might, may, will, Will, could,\n",
            "Nearest to can: should, could, cannot, Can, &apos;ll, must, may, will,\n",
            "Nearest to your: Your, my, their, his, our, her, My, GENERAL,\n",
            "Nearest to about: concerning, regarding, 309, Rodney, ply, SEK, approximately, palatable,\n",
            "Nearest to no: No, any, plenty, Lindh, lots, greedy, iPhoto, scramble,\n",
            "Nearest to .: ?, Automatically, !, Pinnacle, —, MILAN, sounding, •,\n",
            "Nearest to you: You, we, they, ye, guests, I, Guests, visitors,\n",
            "Nearest to European: Soviet, British, new, Pakistani, Zeeland, farms, economics, trumped,\n",
            "Nearest to out: up, pipe, back, hard, Realizing, frogs, off, efficiently,\n",
            "Nearest to regulations: Reservations, flowers, efforts, tourists, rules, brothers, active, sport,\n",
            "Nearest to property: hotel, perspective, conclusion, author, alliance, residences, objective, appearance,\n",
            "Nearest to response: Treat, prove, belonging, fill, extend, raise, harness, detect,\n",
            "Nearest to priority: roundabout, goal, cabin, perception, tomorrow, flair, coziness, shockingly,\n",
            "Nearest to gives: offers, brings, awaits, lets, allows, Are, keeps, enjoys,\n",
            "Nearest to December: March, April, July, October, November, September, June, May,\n",
            "Nearest to categories: levels, sizes, examples, sorts, days, kinds, descriptions, phases,\n",
            "Nearest to setting: wake, shame, tankers, seafront, appointment, sight, environment, ambience,\n",
            "Nearest to latest: current, newest, Arcade, trends, 6815, integrated, correct, announcement,\n",
            "Nearest to external: emergency, industrial, HTML, automation, functions, automated, extended, multimedia,\n",
            "Average loss at step 82000: 1.445987\n",
            "Average loss at step 84000: 1.444834\n",
            "Average loss at step 86000: 1.425260\n",
            "Average loss at step 88000: 1.422455\n",
            "Average loss at step 90000: 1.419766\n",
            "Nearest to and: but, almonds, &amp;, complacency, Lastly, reservas, pastry, or,\n",
            "Nearest to would: Would, should, &apos;d, might, may, will, Will, could,\n",
            "Nearest to can: should, could, &apos;ll, cannot, Can, may, must, lets,\n",
            "Nearest to your: Your, my, their, our, his, her, My, its,\n",
            "Nearest to about: concerning, regarding, 309, palatable, ply, approximately, Rodney, SEK,\n",
            "Nearest to no: No, any, plenty, iPhoto, Lindh, lots, winters, Sparta,\n",
            "Nearest to .: ?, Automatically, !, Pinnacle, —, sounding, MILAN, •,\n",
            "Nearest to you: You, we, they, ye, I, guests, Guests, visitors,\n",
            "Nearest to European: Soviet, British, Jewish, Pakistani, new, economics, Europe, Zeeland,\n",
            "Nearest to out: up, back, pipe, efficiently, hard, Realizing, off, psychology,\n",
            "Nearest to regulations: Reservations, rules, flowers, tourists, efforts, preparing, active, schools,\n",
            "Nearest to property: hotel, conclusion, residences, author, perspective, Addinsoft, alliance, anthem,\n",
            "Nearest to response: Treat, prove, extend, fill, maximize, check, detect, test,\n",
            "Nearest to priority: roundabout, goal, cabin, tomorrow, flair, perception, curious, coziness,\n",
            "Nearest to gives: brings, lets, allows, offers, awaits, enables, keeps, Are,\n",
            "Nearest to December: March, April, July, October, November, June, September, May,\n",
            "Nearest to categories: levels, sizes, sorts, examples, descriptions, kinds, versions, days,\n",
            "Nearest to setting: wake, shame, courtyard, sight, environment, seafront, characteristic, appointment,\n",
            "Nearest to latest: current, newest, Arcade, trends, integrated, 6815, correct, actual,\n",
            "Nearest to external: emergency, HTML, industrial, functions, automation, input, LTSP, extended,\n",
            "Average loss at step 92000: 1.424198\n",
            "Average loss at step 94000: 1.411014\n",
            "Average loss at step 96000: 1.408497\n",
            "Average loss at step 98000: 1.396194\n",
            "Average loss at step 100000: 1.393619\n",
            "Nearest to and: nor, but, &amp;, complacency, or, Lastly, reservas, hygienic,\n",
            "Nearest to would: Would, should, &apos;d, might, may, will, could, Will,\n",
            "Nearest to can: should, could, may, cannot, Can, must, &apos;ll, lets,\n",
            "Nearest to your: Your, my, their, our, his, My, her, its,\n",
            "Nearest to about: regarding, concerning, 309, palatable, Rodney, ply, timetable, Reuters,\n",
            "Nearest to no: No, any, Gérard, plenty, Lindh, greedy, lots, Sparta,\n",
            "Nearest to .: ?, !, Automatically, •, —, sounding, Pinnacle, MILAN,\n",
            "Nearest to you: You, we, they, ye, guests, I, Guests, travelers,\n",
            "Nearest to European: Soviet, British, Jewish, economics, Europe, Latin, subterranean, democratic,\n",
            "Nearest to out: up, pipe, back, off, hard, Realizing, psychology, efficiently,\n",
            "Nearest to regulations: Reservations, rules, flowers, resources, actions, efforts, brothers, commandments,\n",
            "Nearest to property: hotel, perspective, residences, Addinsoft, author, conclusion, obligations, skyline,\n",
            "Nearest to response: Treat, prove, raise, order, test, kick, lead, fill,\n",
            "Nearest to priority: roundabout, goal, brief, landlord, tomorrow, reputation, flair, cabin,\n",
            "Nearest to gives: offers, brings, lets, allows, awaits, keeps, ensures, Are,\n",
            "Nearest to December: March, April, July, October, November, June, September, May,\n",
            "Nearest to categories: sizes, levels, versions, days, sorts, examples, ways, statements,\n",
            "Nearest to setting: wake, environment, ambience, sight, appointment, shame, courtyard, location,\n",
            "Nearest to latest: current, newest, Arcade, correct, integrated, trends, relevant, actual,\n",
            "Nearest to external: emergency, HTML, industrial, functions, automation, LTSP, optional, input,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2SFUW5Cgmsi",
        "colab_type": "text"
      },
      "source": [
        "### Flipping the Input Data\n",
        "Changin the order of the sentence of the target language improves the performance of NMT systems. Because when reversed, it helps the NMT system to establish a strong connection as the last word of the source language and the last word of the target language will be closest to each other. DON'T RUN THIS MULTIPLE TIMES as running two times gives original.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m44eW99grFn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "ac98c456-ec22-49b3-9ed3-51f1bb1220bb"
      },
      "source": [
        "## Reverse the Germen sentences\n",
        "# Remember reversing the source sentence gives better performance\n",
        "# DON'T RUN THIS MULTIPLE TIMES as running two times gives original\n",
        "train_inputs = np.fliplr(train_inputs)\n",
        "test_inputs = np.fliplr(test_inputs)\n",
        "\n",
        "print('Training and Test source data after flipping ')\n",
        "print('\\t',[src_reverse_dictionary[w] for w in train_inputs[0,:].tolist()])\n",
        "print('\\t',[tgt_reverse_dictionary[w] for w in test_inputs[0,:].tolist()])\n",
        "print()\n",
        "print('\\t',[src_reverse_dictionary[w] for w in train_inputs[10,:].tolist()])\n",
        "print('\\t',[tgt_reverse_dictionary[w] for w in test_inputs[10,:].tolist()])\n",
        "\n",
        "print()\n",
        "print('\\nTesting data after flipping')\n",
        "print('\\t',[src_reverse_dictionary[w] for w in test_inputs[0,:].tolist()])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training and Test source data after flipping \n",
            "\t ['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '.', 'können', 'nutzen', 'QuarkXPress', 'mit', 'zusammen', 'besten', 'am', '3', 'Suite', 'Creative', 'und', '2', 'Suite', 'Creative', 'Sie', 'wie', ',', 'Sie', 'erfahren', 'Hier', '<s>']\n",
            "\t ['tray', 'road', 'mistakes', 'of', 'expect', 'a', 'tabled', 'with', 'and', 'the', 'posts', 'useful', 'out', 'waiting', 'wounded', 'a', 'drinks', 'been', 'stand', '26th', 'and', 'senior', 'personal', ',', 'difficulties', 'qualifications', 'an', 'rather', 'road', 'rewriting', 'and', 'road', 'unsustainable', 'the', '2007', 'road', 'wounded', 'not', 'throughout', 'amendment', '<s>']\n",
            "\n",
            "\t ['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '.', ')', 'Import', '##AT##-##AT##', 'PSD', '&gt;', 'Fenster', '(', 'Import', '##AT##-##AT##', 'PSD', 'Palette', 'die', 'Sie', 'öffnen', ',', 'können', 'zu', 'nutzen', 'Dateien', '##AT##-##AT##', 'PSD', 'von', 'Funktionen', 'speziellen', 'die', 'Um', '<s>']\n",
            "\t ['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', ',', '<unk>', 'and', 'important', '<unk>', 'important', 'the', '<unk>', '<unk>', 'the', 'CC', 'revolution', 'are', 'proposals', 'of', 'to', 'not', 'With', 'Overall', 'countries', 'more', '<s>']\n",
            "\n",
            "\n",
            "Testing data after flipping\n",
            "\t ['Creative', '®', 'Adobe', 'der', 'Anwendungen', 'von', 'unabhängig', 'das', 'und', ',', 'bedienen', 'Medien', 'alle', 'inzwischen', 'QuarkXPress', 'von', 'Anwender', 'können', '\\xad', 'CSS', 'und', 'HTML', 'Dank', '.', 'zuvor', 'jemals', 'als', 'besser', '®', 'Illustrator', 'und', '®', 'Photoshop', ',', '8', '®', 'QuarkXPress', 'sich', 'verstehen', 'Heute', '<s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ejk7LpYgyba",
        "colab_type": "text"
      },
      "source": [
        "## Data Generations for MT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LorGmiVAg4uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = embedding_size\n",
        "\n",
        "class DataGeneratorMT(object):\n",
        "    \n",
        "    def __init__(self,batch_size,num_unroll,is_source, is_train):\n",
        "        # Number of data points in a batch\n",
        "        self._batch_size = batch_size\n",
        "        # Number of unrollings\n",
        "        self._num_unroll = num_unroll\n",
        "        # Cursors for each element in batch\n",
        "        self._cursor = [0 for offset in range(self._batch_size)]\n",
        "        \n",
        "        # Loading the learnt word embeddings\n",
        "        self._src_word_embeddings = np.load('de-embeddings.npy')\n",
        "        self._tgt_word_embeddings = np.load('en-embeddings.npy')\n",
        "        \n",
        "        # The sentence IDs being currently processed to create the \n",
        "        # current batch\n",
        "        self._sent_ids = None\n",
        "        \n",
        "        # We want a batch of data from source or target?\n",
        "        self._is_source = is_source\n",
        "        # Is this training or testing data?\n",
        "        self._is_train = is_train\n",
        "                \n",
        "    def next_batch(self, sent_ids):\n",
        "        \n",
        "        # Depending on wheter we want source or target data\n",
        "        # change the maximum sentence length\n",
        "        if self._is_source:\n",
        "            max_sent_length = src_max_sent_length\n",
        "        else:\n",
        "            max_sent_length = tgt_max_sent_length\n",
        "            \n",
        "        # Arrays to hold input and output data\n",
        "        # Word embeddings (current word)\n",
        "        batch_data = np.zeros((self._batch_size,input_size),dtype=np.float32)\n",
        "        # One-hot encoded label (next word)\n",
        "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
        "        \n",
        "        \n",
        "        # Populate each index of the batch\n",
        "        for b in range(self._batch_size):\n",
        "            \n",
        "            # Sentence IDs to get data from\n",
        "            sent_id = sent_ids[b]\n",
        "            \n",
        "            # If generating data with source sentences\n",
        "            # use src_word_embeddings\n",
        "            if self._is_source:\n",
        "                # Depending on whether we need training data or testind data\n",
        "                # choose the previously created training or testind data\n",
        "                if self._is_train:\n",
        "                    sent_text = train_inputs[sent_id]\n",
        "                else:\n",
        "                    sent_text = test_inputs[sent_id]\n",
        "                             \n",
        "                # Populate the batch data arrays\n",
        "                batch_data[b] = self._src_word_embeddings[sent_text[self._cursor[b]],:]\n",
        "                batch_labels[b] = np.zeros((vocabulary_size),dtype=np.float32)\n",
        "                batch_labels[b,sent_text[self._cursor[b]+1]] = 1.0\n",
        "            # If generating data with target sentences\n",
        "            # use tgt_word_embeddings\n",
        "            else:\n",
        "                # Depending on whether we need training data or testind data\n",
        "                # choose the previously created training or testind data\n",
        "                if self._is_train:\n",
        "                    sent_text = train_outputs[sent_id]\n",
        "                else:\n",
        "                    sent_text = test_outputs[sent_id]\n",
        "                \n",
        "                # We cannot avoid having two different embedding vectors for <s> token\n",
        "                # in soruce and target languages\n",
        "                # Therefore, if the symbol appears, we always take the source embedding vector\n",
        "                if sent_text[self._cursor[b]]!=tgt_dictionary['<s>']:\n",
        "                    batch_data[b] = self._tgt_word_embeddings[sent_text[self._cursor[b]],:]\n",
        "                else:\n",
        "                    batch_data[b] = self._src_word_embeddings[sent_text[self._cursor[b]],:]\n",
        "                \n",
        "                # Populate the data arrays\n",
        "                batch_labels[b] = np.zeros((vocabulary_size),dtype=np.float32)\n",
        "                batch_labels[b,sent_text[self._cursor[b]+1]] = 1.0\n",
        "            \n",
        "            # Update the cursor for each batch index\n",
        "            self._cursor[b] = (self._cursor[b]+1)%(max_sent_length-1)\n",
        "             \n",
        "        return batch_data,batch_labels\n",
        "        \n",
        "    def unroll_batches(self,sent_ids):\n",
        "        \n",
        "        # Only if new sentence IDs if provided\n",
        "        # else it will use the previously defined \n",
        "        # sent_ids continuously\n",
        "        if sent_ids is not None:\n",
        "            \n",
        "            self._sent_ids = sent_ids\n",
        "            # Unlike in the previous exercises we do not process a single sequence\n",
        "            # over many iterations of unrollings. We process either a source sentence or target sentence\n",
        "            # at a single go. So we reset the _cursor evrytime we generate a batch\n",
        "            self._cursor = [0 for _ in range(self._batch_size)]\n",
        "                \n",
        "        unroll_data,unroll_labels = [],[]\n",
        "        \n",
        "        # Unrolling data over time\n",
        "        for ui in range(self._num_unroll):\n",
        "            \n",
        "            if self._is_source:\n",
        "                data, labels = self.next_batch(self._sent_ids)\n",
        "            else:\n",
        "                data, labels = self.next_batch(self._sent_ids)\n",
        "                    \n",
        "            unroll_data.append(data)\n",
        "            unroll_labels.append(labels)\n",
        "        \n",
        "        # Return unrolled data and sentence IDs\n",
        "        return unroll_data, unroll_labels, self._sent_ids\n",
        "    \n",
        "    def reset_indices(self):\n",
        "        self._cursor = [0 for offset in range(self._batch_size)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV5g664qhBN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "d3e4c4b6-171e-41af-b2a8-52951fc44d1b"
      },
      "source": [
        "# Running a tiny set to see if the implementation correct\n",
        "dg = DataGeneratorMT(batch_size=5,num_unroll=20,is_source=True, is_train=True)\n",
        "u_data, u_labels, _ = dg.unroll_batches([0,1,2,3,4])\n",
        "\n",
        "print('Source data')\n",
        "for _, lbl in zip(u_data,u_labels):\n",
        "    # the the string words for returned word IDs and display the results\n",
        "    print([src_reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source data\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '.', '</s>', '</s>']\n",
            "['</s>', '</s>', 'bietet', '.', '</s>']\n",
            "['</s>', '</s>', 'Dateiformat', 'nutzen', '</s>']\n",
            "['</s>', '</s>', '##AT##-##AT##', 'optimal', '</s>']\n",
            "['</s>', '</s>', 'PSD', 'Bilder', '</s>']\n",
            "['</s>', '</s>', 'das', 'Ihre', '.']\n",
            "['</s>', '</s>', 'über', 'für', 'werden']\n",
            "['.', '</s>', 'Photoshop', 'es', 'ausgewählt']\n",
            "['können', '.', 'mit', 'Sie', 'Verwendungszweck']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTF3bbRhhEJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "588d12d8-02bf-4feb-def8-81f94fec66d7"
      },
      "source": [
        "# Running a tiny set to see if the implementation correct\n",
        "dg = DataGeneratorMT(batch_size=5,num_unroll=30,is_source=False, is_train=True)\n",
        "u_data, u_labels, _ = dg.unroll_batches([0,2,3,4,5])\n",
        "print('\\nTarget data batch')\n",
        "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
        "    # the the string words for returned word IDs and display the results\n",
        "    print([tgt_reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Target data batch\n",
            "['Here', 'QuarkXPress', 'In', 'For', 'If']\n",
            "[',', '8', 'this', 'example', 'you']\n",
            "['you', 'is', 'section', ',', 'use']\n",
            "['’', 'considered', 'we', 'you', 'PSD']\n",
            "['ll', 'by', '’', 'may', ',']\n",
            "['find', 'many', 'll', 'have', 'you']\n",
            "['out', 'to', 'explain', 'multiple', 'can']\n",
            "['how', 'have', 'when', 'layers', 'switch']\n",
            "['Creative', 'the', 'you', 'in', 'those']\n",
            "['Suite', 'best', 'should', 'your', 'layers']\n",
            "['users', 'integration', 'use', 'PSD', 'on']\n",
            "['can', 'with', 'the', 'with', 'or']\n",
            "['get', 'Photoshop', 'PSD', 'different', 'off']\n",
            "['the', '’', 'format', 'product', 'in']\n",
            "['best', 's', 'for', 'shots', 'QuarkXPress']\n",
            "['possible', 'PSD', 'your', ',', 'without']\n",
            "['interaction', 'file', 'images', 'which', 'having']\n",
            "['with', 'format', 'and', 'will', 'to']\n",
            "['QuarkXPress', 'of', 'how', 'vary', 'save']\n",
            "['.', 'any', 'to', 'from', 'a']\n",
            "['</s>', 'layout', 'get', 'publication', 'separate']\n",
            "['</s>', 'tool', 'the', 'to', 'TIFF']\n",
            "['</s>', 'available', 'most', 'publication', 'for']\n",
            "['</s>', 'today', 'out', '.', 'each']\n",
            "['</s>', '.', 'of', '</s>', 'publication']\n",
            "['</s>', '</s>', 'them', '</s>', '.']\n",
            "['</s>', '</s>', '.', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUTQJ3Nzhhyb",
        "colab_type": "text"
      },
      "source": [
        "# NMT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-HeW5bUj3ON",
        "colab_type": "text"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgNGglmXj--u",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dkqeCGRkBhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tgt_emb_mat = np.load('en-embeddings.npy')\n",
        "input_size = tgt_emb_mat.shape[1]\n",
        "\n",
        "num_nodes = 128\n",
        "batch_size = 10\n",
        "\n",
        "# We unroll the full length at one go\n",
        "# both source and target sentences\n",
        "enc_num_unrollings = 40\n",
        "dec_num_unrollings = 60"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABM-aoamr80",
        "colab_type": "text"
      },
      "source": [
        "### Defining Input/Output Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj_TDfxJmvWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Target embeddings are needed to compute the test outputs\n",
        "tgt_word_embeddings = tf.convert_to_tensor(tgt_emb_mat,name='tgt_embeddings')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4q-hLmBm7JM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "56144fa4-1543-4a4f-ea9f-df0c7d37811d"
      },
      "source": [
        "print('Defining encoder data placeholders')\n",
        "\n",
        "# Training Input placeholders (Encoder)\n",
        "# Encoder related input data, we directly feed in the embeddings\n",
        "enc_train_inputs = []\n",
        "\n",
        "# Defining unrolled training inputs\n",
        "for ui in range(enc_num_unrollings):\n",
        "    enc_train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,input_size],name='train_inputs_%d'%ui))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining encoder data placeholders\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2qefkPvm9t6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "c255339a-0d90-450c-9701-4079d2eadc19"
      },
      "source": [
        "print('Defining decoder data placeholders')\n",
        "# Training Input/Output/Mask data (Decoder)\n",
        "# Decoder inputs and outputs\n",
        "dec_train_inputs, dec_train_labels = [],[]\n",
        "# We use masking to mask out the any of the </s> elements\n",
        "# from the loss computation in the decoder\n",
        "dec_train_masks = []\n",
        "\n",
        "# Defining unrolled training inputs\n",
        "for ui in range(dec_num_unrollings):\n",
        "    dec_train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,input_size],name='dec_train_inputs_%d'%ui))\n",
        "    dec_train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'dec_train_labels_%d'%ui))\n",
        "    dec_train_masks.append(tf.placeholder(tf.float32, shape=[batch_size,1],name='dec_train_masks_%d'%ui))\n",
        "\n",
        "    \n",
        "enc_test_input = [tf.placeholder(tf.float32, shape=[batch_size,input_size], name='test_input_%d'%ui) for ui in range(enc_num_unrollings)]\n",
        "dec_test_input = tf.nn.embedding_lookup(tgt_word_embeddings,[tgt_dictionary['<s>']])\n",
        "print('Done')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining decoder data placeholders\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3VIbyvSm-iU",
        "colab_type": "text"
      },
      "source": [
        "### Encoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eckGWSfurvmM",
        "colab_type": "text"
      },
      "source": [
        "Got a `tf.compat.v1` has no module `contrib` error. Solution was to copy paste this file that has the `xavier_initializer()`\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/contrib/layers/python/layers/initializers.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ5g0tNNrcNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.ops import random_ops\n",
        "\n",
        "\n",
        "def xavier_initializer(uniform=True, seed=None, dtype=dtypes.float32):\n",
        "  \"\"\"Returns an initializer performing \"Xavier\" initialization for weights.\n",
        "  This function implements the weight initialization from:\n",
        "  Xavier Glorot and Yoshua Bengio (2010):\n",
        "           [Understanding the difficulty of training deep feedforward neural\n",
        "           networks. International conference on artificial intelligence and\n",
        "           statistics.](\n",
        "           http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n",
        "  This initializer is designed to keep the scale of the gradients roughly the\n",
        "  same in all layers. In uniform distribution this ends up being the range:\n",
        "  `x = sqrt(6. / (in + out)); [-x, x]` and for normal distribution a standard\n",
        "  deviation of `sqrt(2. / (in + out))` is used.\n",
        "  Args:\n",
        "    uniform: Whether to use uniform or normal distributed random initialization.\n",
        "    seed: A Python integer. Used to create random seeds. See\n",
        "          `tf.compat.v1.set_random_seed` for behavior.\n",
        "    dtype: The data type. Only floating point types are supported.\n",
        "  Returns:\n",
        "    An initializer for a weight matrix.\n",
        "  \"\"\"\n",
        "  return variance_scaling_initializer(factor=1.0, mode='FAN_AVG',\n",
        "                                      uniform=uniform, seed=seed, dtype=dtype)\n",
        "\n",
        "xavier_initializer_conv2d = xavier_initializer\n",
        "\n",
        "\n",
        "def variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False,\n",
        "                                 seed=None, dtype=dtypes.float32):\n",
        "  \"\"\"Returns an initializer that generates tensors without scaling variance.\n",
        "  When initializing a deep network, it is in principle advantageous to keep\n",
        "  the scale of the input variance constant, so it does not explode or diminish\n",
        "  by reaching the final layer. This initializer use the following formula:\n",
        "  ```python\n",
        "    if mode='FAN_IN': # Count only number of input connections.\n",
        "      n = fan_in\n",
        "    elif mode='FAN_OUT': # Count only number of output connections.\n",
        "      n = fan_out\n",
        "    elif mode='FAN_AVG': # Average number of inputs and output connections.\n",
        "      n = (fan_in + fan_out)/2.0\n",
        "      truncated_normal(shape, 0.0, stddev=sqrt(factor / n))\n",
        "  ```\n",
        "  * To get [Delving Deep into Rectifiers](\n",
        "     http://arxiv.org/pdf/1502.01852v1.pdf) (also know as the \"MSRA \n",
        "     initialization\"), use (Default):<br/>\n",
        "    `factor=2.0 mode='FAN_IN' uniform=False`\n",
        "  * To get [Convolutional Architecture for Fast Feature Embedding](\n",
        "     http://arxiv.org/abs/1408.5093), use:<br/>\n",
        "    `factor=1.0 mode='FAN_IN' uniform=True`\n",
        "  * To get [Understanding the difficulty of training deep feedforward neural\n",
        "    networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf),\n",
        "    use:<br/>\n",
        "    `factor=1.0 mode='FAN_AVG' uniform=True.`\n",
        "  * To get `xavier_initializer` use either:<br/>\n",
        "    `factor=1.0 mode='FAN_AVG' uniform=True`, or<br/>\n",
        "    `factor=1.0 mode='FAN_AVG' uniform=False`.\n",
        "  Args:\n",
        "    factor: Float.  A multiplicative factor.\n",
        "    mode: String.  'FAN_IN', 'FAN_OUT', 'FAN_AVG'.\n",
        "    uniform: Whether to use uniform or normal distributed random initialization.\n",
        "    seed: A Python integer. Used to create random seeds. See\n",
        "          `tf.compat.v1.set_random_seed` for behavior.\n",
        "    dtype: The data type. Only floating point types are supported.\n",
        "  Returns:\n",
        "    An initializer that generates tensors with unit variance.\n",
        "  Raises:\n",
        "    ValueError: if `dtype` is not a floating point type.\n",
        "    TypeError: if `mode` is not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG'].\n",
        "  \"\"\"\n",
        "  if not dtype.is_floating:\n",
        "    raise TypeError('Cannot create initializer for non-floating point type.')\n",
        "  if mode not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG']:\n",
        "    raise TypeError('Unknown mode %s [FAN_IN, FAN_OUT, FAN_AVG]', mode)\n",
        "\n",
        "  # pylint: disable=unused-argument\n",
        "  def _initializer(shape, dtype=dtype, partition_info=None):\n",
        "    \"\"\"Initializer function.\"\"\"\n",
        "    if not dtype.is_floating:\n",
        "      raise TypeError('Cannot create initializer for non-floating point type.')\n",
        "    # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\n",
        "    # This is the right thing for matrix multiply and convolutions.\n",
        "    if shape:\n",
        "      fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n",
        "      fan_out = float(shape[-1])\n",
        "    else:\n",
        "      fan_in = 1.0\n",
        "      fan_out = 1.0\n",
        "    for dim in shape[:-2]:\n",
        "      fan_in *= float(dim)\n",
        "      fan_out *= float(dim)\n",
        "    if mode == 'FAN_IN':\n",
        "      # Count only number of input connections.\n",
        "      n = fan_in\n",
        "    elif mode == 'FAN_OUT':\n",
        "      # Count only number of output connections.\n",
        "      n = fan_out\n",
        "    elif mode == 'FAN_AVG':\n",
        "      # Average number of inputs and output connections.\n",
        "      n = (fan_in + fan_out) / 2.0\n",
        "    if uniform:\n",
        "      # To get stddev = math.sqrt(factor / n) need to adjust for uniform.\n",
        "      limit = math.sqrt(3.0 * factor / n)\n",
        "      return random_ops.random_uniform(shape, -limit, limit,\n",
        "                                       dtype, seed=seed)\n",
        "    else:\n",
        "      # To get stddev = math.sqrt(factor / n) need to adjust for truncated.\n",
        "      trunc_stddev = math.sqrt(1.3 * factor / n)\n",
        "      return random_ops.truncated_normal(shape, 0.0, trunc_stddev, dtype,\n",
        "                                         seed=seed)\n",
        "  # pylint: enable=unused-argument\n",
        "\n",
        "  return _initializer"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzqbZLxTnWrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "04ce6e52-9fc4-4eb0-abb8-deeb4b84b44a"
      },
      "source": [
        "print('Defining Encoder Parameters')\n",
        "with tf.variable_scope('Encoder'):\n",
        "    \n",
        "    # Input gate (i_t) - How much memory to write to cell state\n",
        "    # We use xavier intialization as this gives better results \n",
        "    enc_ix = tf.get_variable('ix',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_im = tf.get_variable('im',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.05, 0.05),name='ib')\n",
        "    \n",
        "    # Forget gate (f_t) - How much memory to discard from cell state\n",
        "    enc_fx = tf.get_variable('fx',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_fm = tf.get_variable('fm',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.05, 0.05),name='fb')\n",
        "    \n",
        "    # Candidate value (c~_t) - Used to compute the current cell state                            \n",
        "    enc_cx = tf.get_variable('cx',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_cm = tf.get_variable('cm',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.05,0.05),name='cb') \n",
        "    \n",
        "    # Output gate - How much memory to output from the cell state\n",
        "    enc_ox = tf.get_variable('ox',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_om = tf.get_variable('om',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.05,0.05),name='ob') \n",
        "    \n",
        "    # Variables saving state across unrollings (testing).\n",
        "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_output')\n",
        "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name = 'train_cell')\n",
        "    \n",
        "    saved_test_output = tf.Variable(tf.zeros([batch_size, num_nodes]),trainable=False, name='test_output')\n",
        "    saved_test_state = tf.Variable(tf.zeros([batch_size, num_nodes]),trainable=False, name='test_cell')\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining Encoder Parameters\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}