{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT-TF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPakN88uTyq/8+wB3sCkkC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikramkrishnan9885/MyColab/blob/master/NMT_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jrg9r7QSLJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "4f3dbe92-bfa6-490c-b9cc-c8f0bb235089"
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: six, termcolor, protobuf, google-pasta, keras-preprocessing, tensorboard, wrapt, numpy, wheel, absl-py, keras-applications, astor, opt-einsum, tensorflow-estimator, grpcio, gast\n",
            "Required-by: fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "592_0sD1SXky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "32084007-d347-499e-8570-47e000eec34a"
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGYRTvZRSi-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46db3a75-9568-4a3c-9a63-62c2586a3008"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 35kB/s \n",
            "y\n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.30.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (49.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=bb25ced7a722a3c4eea0505b4c29c1c6a554852bde03ff0c9f4492ff48c8d3d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoXHD3yNPbRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "66449f3f-e97d-4521-b637-421dc2b61cf2"
      },
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import zipfile\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "#import tensorflow as tf\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import csv\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import nltk\n",
        "\n",
        "import urllib.request"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAUAycJUItC7",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions for word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DA03ggcIsKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_cursors = None\n",
        "tot_sentences = None\n",
        "src_max_sent_length, tgt_max_sent_length = 0, 0\n",
        "src_dictionary, tgt_dictionary = {}, {}\n",
        "src_reverse_dictionary, tgt_reverse_dictionary = {},{}\n",
        "train_inputs, train_outputs = None, None\n",
        "embedding_size = None # Dimension of the embedding vector.\n",
        "vocabulary_size = None\n",
        "\n",
        "def define_data_and_hyperparameters(\n",
        "        _tot_sentences, _src_max, _tgt_max, _src_dict, _tgt_dict,\n",
        "        _src_rev_dict, _tgt_rev_dict, _tr_inp, _tr_out, _emb_size, _vocab_size):\n",
        "    global tot_sentences, sentence_cursors\n",
        "    global src_max_sent_length, tgt_max_sent_length\n",
        "    global src_dictionary, tgt_dictionary\n",
        "    global src_reverse_dictionary, tgt_reverse_dictionary\n",
        "    global train_inputs, train_outputs\n",
        "    global embedding_size, vocabulary_size\n",
        "\n",
        "    embedding_size = _emb_size\n",
        "    vocabulary_size = _vocab_size\n",
        "    src_max_sent_length, tgt_max_sent_length = _src_max, _tgt_max\n",
        "\n",
        "    src_dictionary = _src_dict\n",
        "    tgt_dictionary = _tgt_dict\n",
        "\n",
        "    src_reverse_dictionary = _src_rev_dict\n",
        "    tgt_reverse_dictionary = _tgt_rev_dict\n",
        "\n",
        "    train_inputs = _tr_inp\n",
        "    train_outputs = _tr_out\n",
        "\n",
        "    tot_sentences = _tot_sentences\n",
        "    sentence_cursors = [0 for _ in range(tot_sentences)]\n",
        "\n",
        "\n",
        "def generate_batch_for_word2vec(batch_size, window_size, is_source):\n",
        "    # window_size is the amount of words we're looking at from each side of a given word\n",
        "    # creates a single batch\n",
        "    global sentence_cursors\n",
        "    global src_dictionary, tgt_dictionary\n",
        "    global train_inputs, train_outputs\n",
        "    span = 2 * window_size + 1  # [ skip_window target skip_window ]\n",
        "\n",
        "    batch = np.ndarray(shape=(batch_size, span - 1), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    # e.g if skip_window = 2 then span = 5\n",
        "    # span is the length of the whole frame we are considering for a single word (left + word + right)\n",
        "    # skip_window is the length of one side\n",
        "\n",
        "    sentence_ids_for_batch = np.random.randint(0, tot_sentences, batch_size)\n",
        "\n",
        "    for b_i in range(batch_size):\n",
        "        sent_id = sentence_ids_for_batch[b_i]\n",
        "\n",
        "        if is_source:\n",
        "            buffer = train_inputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n",
        "        else:\n",
        "            buffer = train_outputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n",
        "        assert buffer.size == span, 'Buffer length (%d), Current data index (%d), Span(%d)' % (\n",
        "        buffer.size, sentence_cursors[sent_id], span)\n",
        "        # If we only have EOS tokesn in the sampled text, we sample a new one\n",
        "        if is_source:\n",
        "            while np.all(buffer == src_dictionary['</s>']):\n",
        "                # reset the sentence_cursors for that cap_id\n",
        "                sentence_cursors[sent_id] = 0\n",
        "                # sample a new cap_id\n",
        "                sent_id = np.random.randint(0, tot_sentences)\n",
        "                buffer = train_inputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n",
        "        else:\n",
        "            while np.all(buffer == tgt_dictionary['</s>']):\n",
        "                # reset the sentence_cursors for that cap_id\n",
        "                sentence_cursors[sent_id] = 0\n",
        "                # sample a new cap_id\n",
        "                sent_id = np.random.randint(0, tot_sentences)\n",
        "                buffer = train_outputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n",
        "\n",
        "        # fill left and right sides of batch\n",
        "        batch[b_i, :window_size] = buffer[:window_size]\n",
        "        batch[b_i, window_size:] = buffer[window_size + 1:]\n",
        "\n",
        "        labels[b_i, 0] = buffer[window_size]\n",
        "\n",
        "        # increase the corresponding index\n",
        "        if is_source:\n",
        "            sentence_cursors[sent_id] = (sentence_cursors[sent_id] + 1) % (src_max_sent_length - span)\n",
        "        else:\n",
        "            sentence_cursors[sent_id] = (sentence_cursors[sent_id] + 1) % (tgt_max_sent_length - span)\n",
        "\n",
        "    assert batch.shape[0] == batch_size and batch.shape[1] == span - 1\n",
        "    return batch, labels\n",
        "\n",
        "\n",
        "def print_some_batches():\n",
        "    global sentence_cursors, tot_sentences\n",
        "    global src_reverse_dictionary\n",
        "\n",
        "    for window_size in [1, 2]:\n",
        "        sentence_cursors = [0 for _ in range(tot_sentences)]\n",
        "        batch, labels = generate_batch_for_word2vec(batch_size=8, window_size=window_size, is_source=True)\n",
        "        print('\\nwith window_size = %d:' % (window_size))\n",
        "        print('    batch:', [[src_reverse_dictionary[bii] for bii in bi] for bi in batch])\n",
        "        print('    labels:', [src_reverse_dictionary[li] for li in labels.reshape(8)])\n",
        "\n",
        "    sentence_cursors = [0 for _ in range(tot_sentences)]\n",
        "\n",
        "batch_size, window_size = None, None\n",
        "valid_size, valid_window, valid_examples = None, None, None\n",
        "num_sampled = None\n",
        "\n",
        "train_dataset, train_labels = None, None\n",
        "valid_dataset = None\n",
        "\n",
        "softmax_weights, softmax_biases = None, None\n",
        "\n",
        "loss, optimizer, similarity, normalized_embeddings = None, None, None, None\n",
        "\n",
        "def define_word2vec_tensorflow(batch_size):\n",
        "\n",
        "    global embedding_size, window_size\n",
        "    global\tvalid_size, valid_window, valid_examples\n",
        "    global num_sampled\n",
        "    global train_dataset, train_labels\n",
        "    global valid_dataset\n",
        "    global softmax_weights, softmax_biases\n",
        "    global loss, optimizer, similarity\n",
        "    global vocabulary_size, embedding_size\n",
        "    global normalized_embeddings\n",
        "\n",
        "\n",
        "    window_size = 2  # How many words to consider left and right.\n",
        "    # We pick a random validation set to sample nearest neighbors. here we limit the\n",
        "    # validation samples to the words that have a low numeric ID, which by\n",
        "    # construction are also the most frequent.\n",
        "    valid_size = 20  # Random set of words to evaluate similarity on.\n",
        "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "    # pick 16 samples from 100\n",
        "    valid_examples = np.array(np.random.randint(0, valid_window, valid_size // 2))\n",
        "    valid_examples = np.append(valid_examples, np.random.randint(1000, 1000 + valid_window, valid_size // 2))\n",
        "    num_sampled = 32  # Number of negative examples to sample.\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "\n",
        "    # Input data.\n",
        "    train_dataset = tf.compat.v1.placeholder(tf.compat.v1.int32, shape=[batch_size, 2 * window_size])\n",
        "    train_labels = tf.compat.v1.placeholder(tf.compat.v1.int32, shape=[batch_size, 1])\n",
        "    valid_dataset = tf.compat.v1.constant(valid_examples, dtype=tf.compat.v1.int32)\n",
        "\n",
        "    # Variables.\n",
        "    # embedding, vector for each word in the vocabulary\n",
        "    embeddings = tf.compat.v1.Variable(tf.compat.v1.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.compat.v1.float32))\n",
        "    softmax_weights = tf.compat.v1.Variable(tf.compat.v1.truncated_normal([vocabulary_size, embedding_size],\n",
        "                                                      stddev=1.0 / math.sqrt(embedding_size), dtype=tf.compat.v1.float32))\n",
        "    softmax_biases = tf.compat.v1.Variable(tf.compat.v1.zeros([vocabulary_size], dtype=tf.compat.v1.float32))\n",
        "\n",
        "    # Model.\n",
        "    # Look up embeddings for inputs.\n",
        "    # this might efficiently find the embeddings for given ids (traind dataset)\n",
        "    # manually doing this might not be efficient given there are 50000 entries in embeddings\n",
        "    stacked_embedings = None\n",
        "    print('Defining %d embedding lookups representing each word in the context' % (2 * window_size))\n",
        "    for i in range(2 * window_size):\n",
        "        embedding_i = tf.compat.v1.nn.embedding_lookup(embeddings, train_dataset[:, i])\n",
        "        x_size, y_size = embedding_i.get_shape().as_list()\n",
        "        if stacked_embedings is None:\n",
        "            stacked_embedings = tf.compat.v1.reshape(embedding_i, [x_size, y_size, 1])\n",
        "        else:\n",
        "            stacked_embedings = tf.compat.v1.concat(axis=2,\n",
        "                                          values=[stacked_embedings, tf.compat.v1.reshape(embedding_i, [x_size, y_size, 1])])\n",
        "\n",
        "    assert stacked_embedings.get_shape().as_list()[2] == 2 * window_size\n",
        "    print(\"Stacked embedding size: %s\" % stacked_embedings.get_shape().as_list())\n",
        "    mean_embeddings = tf.compat.v1.reduce_mean(stacked_embedings, 2, keepdims=False)\n",
        "    print(\"Reduced mean embedding size: %s\" % mean_embeddings.get_shape().as_list())\n",
        "\n",
        "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
        "    # inputs are embeddings of the train words\n",
        "    # with this loss we optimize weights, biases, embeddings\n",
        "\n",
        "    loss = tf.compat.v1.reduce_mean(\n",
        "        tf.compat.v1.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=mean_embeddings,\n",
        "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
        "\n",
        "    # Optimizer.\n",
        "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(0.001).minimize(loss)\n",
        "\n",
        "    # Compute the similarity between minibatch examples and all embeddings.\n",
        "    # We use the cosine distance:\n",
        "    norm = tf.compat.v1.sqrt(tf.compat.v1.reduce_sum(tf.compat.v1.square(embeddings), 1, keepdims=True))\n",
        "    normalized_embeddings = embeddings / norm\n",
        "    valid_embeddings = tf.compat.v1.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "    similarity = tf.compat.v1.matmul(valid_embeddings, tf.compat.v1.transpose(normalized_embeddings))\n",
        "\n",
        "\n",
        "def run_word2vec_source(batch_size):\n",
        "    global embedding_size, window_size\n",
        "    global valid_size, valid_window, valid_examples\n",
        "    global num_sampled\n",
        "    global train_dataset, train_labels\n",
        "    global valid_dataset\n",
        "    global softmax_weights, softmax_biases\n",
        "    global loss, optimizer, similarity, normalized_embeddings\n",
        "    global src_reverse_dictionary\n",
        "    global vocabulary_size, embedding_size\n",
        "\n",
        "    num_steps = 100001\n",
        "\n",
        "    config=tf.compat.v1.ConfigProto(allow_soft_placement=True) \n",
        "    config.gpu_options.allow_growth = True\t\n",
        "    \t\n",
        "    with tf.compat.v1.Session(config=config) as session:\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('Initialized')\n",
        "        average_loss = 0\n",
        "        for step in range(num_steps):\n",
        "\n",
        "            batch_data, batch_labels = generate_batch_for_word2vec(batch_size, window_size, is_source=True)\n",
        "            feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n",
        "            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "            average_loss += l\n",
        "            if (step + 1) % 2000 == 0:\n",
        "                if step > 0:\n",
        "                    average_loss = average_loss / 2000\n",
        "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "                print('Average loss at step %d: %f' % (step + 1, average_loss))\n",
        "                average_loss = 0\n",
        "            # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "            if (step + 1) % 10000 == 0:\n",
        "                sim = similarity.eval()\n",
        "                for i in range(valid_size):\n",
        "                    valid_word = src_reverse_dictionary[valid_examples[i]]\n",
        "                    top_k = 8  # number of nearest neighbors\n",
        "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "                    log = 'Nearest to %s:' % valid_word\n",
        "                    for k in range(top_k):\n",
        "                        close_word = src_reverse_dictionary[nearest[k]]\n",
        "                        log = '%s %s,' % (log, close_word)\n",
        "                    print(log)\n",
        "        cbow_final_embeddings = normalized_embeddings.eval()\n",
        "\n",
        "    np.save('de-embeddings.npy', cbow_final_embeddings)\n",
        "\n",
        "def run_word2vec_target(batch_size):\n",
        "    global embedding_size, window_size\n",
        "    global valid_size, valid_window, valid_examples\n",
        "    global num_sampled\n",
        "    global train_dataset, train_labels\n",
        "    global valid_dataset\n",
        "    global softmax_weights, softmax_biases\n",
        "    global loss, optimizer, similarity, normalized_embeddings\n",
        "    global tgt_reverse_dictionary\n",
        "    global vocabulary_size, embedding_size\n",
        "\n",
        "    num_steps = 100001\n",
        "    \n",
        "    config=tf.compat.v1.ConfigProto(allow_soft_placement=True) \n",
        "    config.gpu_options.allow_growth = True\t\n",
        "    with tf.compat.v1.Session(config=config) as session:\n",
        "        tf.compat.v1.global_variables_initializer().run()\n",
        "        print('Initialized')\n",
        "        average_loss = 0\n",
        "        for step in range(num_steps):\n",
        "\n",
        "            batch_data, batch_labels = generate_batch_for_word2vec(batch_size, window_size, is_source=False)\n",
        "            feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n",
        "            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "            average_loss += l\n",
        "            if (step + 1) % 2000 == 0:\n",
        "                if step > 0:\n",
        "                    average_loss = average_loss / 2000\n",
        "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "                print('Average loss at step %d: %f' % (step + 1, average_loss))\n",
        "                average_loss = 0\n",
        "            # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "            if (step + 1) % 10000 == 0:\n",
        "                sim = similarity.eval()\n",
        "                for i in range(valid_size):\n",
        "                    valid_word = tgt_reverse_dictionary[valid_examples[i]]\n",
        "                    top_k = 8  # number of nearest neighbors\n",
        "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "                    log = 'Nearest to %s:' % valid_word\n",
        "                    for k in range(top_k):\n",
        "                        close_word = tgt_reverse_dictionary[nearest[k]]\n",
        "                        log = '%s %s,' % (log, close_word)\n",
        "                    print(log)\n",
        "        cbow_final_embeddings = normalized_embeddings.eval()\n",
        "\n",
        "    np.save('en-embeddings.npy', cbow_final_embeddings)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHyAMyYe1nNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "3e0640d1-b45b-4b02-b1aa-5cf3c69b16fb"
      },
      "source": [
        "print('Beginning file download with urllib2...')\n",
        "\n",
        "url_train_de = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de'\n",
        "urllib.request.urlretrieve(url_train_de, 'train.de')\n",
        "\n",
        "url_train_en = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en'\n",
        "urllib.request.urlretrieve(url_train_en, 'train.en')\n",
        "\n",
        "url_vocab_de = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.de'\n",
        "urllib.request.urlretrieve(url_vocab_de, 'vocab.50K.de')\n",
        "\n",
        "url_vocab_en = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.en'\n",
        "urllib.request.urlretrieve(url_vocab_en, 'vocab.50K.en')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning file download with urllib2...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('vocab.50K.en', <http.client.HTTPMessage at 0x7f41644f9ba8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50qYf6SQB8Nq",
        "colab_type": "text"
      },
      "source": [
        "# Data prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkfgOtixCBTD",
        "colab_type": "text"
      },
      "source": [
        "## Load vocabs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TinWhTBr4IbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "32093bad-30db-48cb-8f8f-435d7cf8877c"
      },
      "source": [
        "#  SEE THE VOCAB CLASSES IN THE PYTORCH EXAMPLES\n",
        "# Building source language vocabulary\n",
        "\n",
        "# Contains word string -> ID mapping\n",
        "src_dictionary = dict()\n",
        "\n",
        "# Read the vocabulary file\n",
        "with open('vocab.50K.de', encoding='utf-8') as f:\n",
        "    # Read and store every line\n",
        "    for line in f:\n",
        "        #we are discarding last char as it is new line char\n",
        "        src_dictionary[line[:-1]] = len(src_dictionary)\n",
        "\n",
        "# Build a reverse dictionary with the mapping ID -> word string\n",
        "src_reverse_dictionary = dict(zip(src_dictionary.values(),src_dictionary.keys()))\n",
        "\n",
        "# Print some of the words in the dictionary\n",
        "print('Source')\n",
        "print('\\t',list(src_dictionary.items())[:10])\n",
        "print('\\t',list(src_reverse_dictionary.items())[:10])\n",
        "print('\\t','Vocabulary size: ', len(src_dictionary))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source\n",
            "\t [('<unk>', 0), ('<s>', 1), ('</s>', 2), (',', 3), ('.', 4), ('die', 5), ('der', 6), ('und', 7), ('in', 8), ('zu', 9)]\n",
            "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, ','), (4, '.'), (5, 'die'), (6, 'der'), (7, 'und'), (8, 'in'), (9, 'zu')]\n",
            "\t Vocabulary size:  50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VQfxpTz4ZAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "b0459659-d226-4403-fba0-540cc4a87737"
      },
      "source": [
        "# Building target language vocabulary\n",
        "\n",
        "# Contains word string -> ID mapping\n",
        "tgt_dictionary = dict()\n",
        "\n",
        "# Read the vocabulary file\n",
        "with open('vocab.50K.en', encoding='utf-8') as f:\n",
        "    # Read and store every line\n",
        "    for line in f:\n",
        "        #we are discarding last char as it is new line char\n",
        "        tgt_dictionary[line[:-1]] = len(tgt_dictionary)\n",
        "\n",
        "# Build a reverse dictionary with the mapping ID -> word string\n",
        "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(),tgt_dictionary.keys()))\n",
        "\n",
        "# Print some of the words in the dictionary\n",
        "print('Target')\n",
        "print('\\t',list(tgt_dictionary.items())[:10])\n",
        "print('\\t',list(tgt_reverse_dictionary.items())[:10])\n",
        "print('\\t','Vocabulary size: ', len(tgt_dictionary))\n",
        "\n",
        "# Each language has 50000 words\n",
        "vocabulary_size = 50000"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target\n",
            "\t [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('the', 3), (',', 4), ('.', 5), ('of', 6), ('and', 7), ('to', 8), ('in', 9)]\n",
            "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'the'), (4, ','), (5, '.'), (6, 'of'), (7, 'and'), (8, 'to'), (9, 'in')]\n",
            "\t Vocabulary size:  50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFLspdzcCPrb",
        "colab_type": "text"
      },
      "source": [
        "## Load training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNshPNK6CVRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6322c242-7775-475d-de16-256a07c74e84"
      },
      "source": [
        "# Contains the training sentences\n",
        "source_sent = [] # Input\n",
        "target_sent = [] # Output\n",
        "\n",
        "# Contains the testing sentences\n",
        "test_source_sent = [] # Input\n",
        "test_target_sent = [] # Output\n",
        "\n",
        "# We grab around 100 lines of data that are interleaved \n",
        "# in the first 50000 sentences\n",
        "test_indices = [l_i for l_i in range(50,50001,500)]\n",
        "\n",
        "# Read the source data file and read the first 250,000 lines (except first 50)\n",
        "with open('train.de', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        # discarding first 50 translations as there was some\n",
        "        # english to english mappings found in the first few lines. which are wrong\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        \n",
        "        if len(source_sent)<250000 and l_i not in test_indices:\n",
        "            source_sent.append(line)\n",
        "        elif l_i in test_indices:\n",
        "            test_source_sent.append(line)\n",
        "        \n",
        "# Read the target data file and read the first 250,000 lines (except first 50)            \n",
        "with open('train.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        # discarding first 50 translations as there was some\n",
        "        # english to english mappings found in the first few lines. which are wrong\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        \n",
        "        if len(target_sent)<250000 and l_i not in test_indices:\n",
        "            target_sent.append(line)\n",
        "        elif l_i in test_indices:\n",
        "            test_target_sent.append(line)\n",
        "        \n",
        "# Make sure we extracted same number of both extracted source and target sentences         \n",
        "assert len(source_sent)==len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
        "\n",
        "# Print some source sentences\n",
        "print('Sample translations (%d)'%len(source_sent))\n",
        "for i in range(0,250000,10000):\n",
        "    print('(',i,') DE: ', source_sent[i])\n",
        "    print('(',i,') EN: ', target_sent[i])\n",
        "\n",
        "# Print some target sentences\n",
        "print('Sample test translations (%d)'%len(test_source_sent))\n",
        "for i in range(0,100,10):\n",
        "    print('DE: ', test_source_sent[i])\n",
        "    print('EN: ', test_target_sent[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample translations (250000)\n",
            "( 0 ) DE:  Hier erfahren Sie , wie Sie Creative Suite 2 und Creative Suite 3 am besten zusammen mit QuarkXPress nutzen können .\n",
            "\n",
            "( 0 ) EN:  Here , you ’ ll find out how Creative Suite users can get the best possible interaction with QuarkXPress .\n",
            "\n",
            "( 10000 ) DE:  Für die sehr günstigen Wochen- und Monatskarten ( 1 Monat ca.\n",
            "\n",
            "( 10000 ) EN:  It is THE trendy area of Marseille .\n",
            "\n",
            "( 20000 ) DE:  Freuen Sie sich auf die romantische Atmosphäre in den Zimmern und Apartments .\n",
            "\n",
            "( 20000 ) EN:  Enjoy the romantic atmosphere of one of the guest rooms or apartments .\n",
            "\n",
            "( 30000 ) DE:  Zu zwiespältig sind Dr. Gutherzens Erfahrungen aus frühen Studententagen verlaufen , in denen er sich in die Gefielde von durch Heidegger geprägten Autor / innen begeben hat und dort ständig mit strengem Blick darauf verwiesen wurde , er habe bestimmte Theorieressourcen und Gedankengebäude einfach noch nicht gründlich genug verstanden und könne deshalb nicht begreifen , warum seine Einwände zu bestimmten Texten und Diskursen nicht stichhaltig seien .\n",
            "\n",
            "( 30000 ) EN:  This vagueness lends itself to an idealisation of violence , formulated in concepts of &quot; assault &quot; against imaginary authorities or enthusiastic notions of &quot; blissful traumatic knowledge &quot; .\n",
            "\n",
            "( 40000 ) DE:  Sie veranlassen den untergeordneten Prozess , sich während seiner gesamten Lebensdauer lediglich einmal mit dem SQL ##AT##-##AT## Server zu verbinden , anstatt bei jedem Aufruf einer Seite , die eine Verbindung benötigt .\n",
            "\n",
            "( 40000 ) EN:  They cause the child process to simply connect only once for its entire lifespan , instead of every time it processes a page that requires connecting to the SQL server .\n",
            "\n",
            "( 50000 ) DE:  Je intensiver man dabei bleibt , desto bessere Ergebnisse erzielt man .\n",
            "\n",
            "( 50000 ) EN:  The more intensively you do them , the better the results .\n",
            "\n",
            "( 60000 ) DE:  In allen Zimmern ist Digitalfernsehen und Internetzugang für sowohl Geschäftsreisende als auch Urlauber erhältlich .\n",
            "\n",
            "( 60000 ) EN:  All rooms offer digital TV and Internet access appealing to both corporate and leisure guests .\n",
            "\n",
            "( 70000 ) DE:  Bitte beachten Sie , dass Ihr Check ##AT##-##AT## in ##AT##-##AT## Code nicht mit der Buchungsnummer identisch ist .\n",
            "\n",
            "( 70000 ) EN:  Please note that the check ##AT##-##AT## in number and your reservation number are not the same .\n",
            "\n",
            "( 80000 ) DE:  Auch die Art , wie man einen eigenen Weißabgleich vornehmen kann , darf angepasst werden .\n",
            "\n",
            "( 80000 ) EN:  Another thing that should be reassessed is the way in which the user creates his own white balance .\n",
            "\n",
            "( 90000 ) DE:  Weitere Supportoptionen ( http : / / support.microsoft.com / contactus ) : Stellen Sie Ihre Fragen im Web , wenden Sie sich an Microsoft Support Services , oder teilen Sie uns Ihre Meinung mit .\n",
            "\n",
            "( 90000 ) EN:  Other Support Options ( http : / / support.microsoft.com / default.aspx ? pr = csshome ) : Use the Web to ask a question , contact Microsoft Customer Support Services , or provide feedback .\n",
            "\n",
            "( 100000 ) DE:  Ik vond het geen 4 ##STAR## waard . Het appartement oogde erg schroezelig en gedateerd , personeel sprak erg gebrekkig Engels .\n",
            "\n",
            "( 100000 ) EN:  Trousse d &apos;information manquante et devrait inclure une carte du site ainsi que des services et activités sur les lieu ou dans la commune ainsi les attraits touristiques de la région .\n",
            "\n",
            "( 110000 ) DE:  Dieses Bild spiegelt sich in Ihrem Unternehmen und Ihren Produkten wieder .\n",
            "\n",
            "( 110000 ) EN:  This image reflects on your company and products .\n",
            "\n",
            "( 120000 ) DE:  Alle Zimmer sind mit Digital ##AT##-##AT## TV und DVD und kostenlosem Breitbandanschluss sowie Direktwahltelefon ausgestattet .\n",
            "\n",
            "( 120000 ) EN:  Our rooms include a romantic four ##AT##-##AT## poster and two easy access ground floor rooms . All rooms are equipped to hotel standards with Digital TV and DVD , free broadband connections and free local and national direct dial phones .\n",
            "\n",
            "( 130000 ) DE:  Nothing if im quite honet . I wouldnt stay here again or recommend it to anyone i know .\n",
            "\n",
            "( 130000 ) EN:  the room was basic but spacoius and clean , the staff were friendly and helpful , the food was tasty , all in all , lovely place to stay !\n",
            "\n",
            "( 140000 ) DE:  Es gibt 4 verschiedene Möglichkeiten , Cannon Blast zu Ihrem Blog oder Ihrer Website hinzuzufügen .\n",
            "\n",
            "( 140000 ) EN:  There are 4 different ways of posting Cannon Blast to your blog or website .\n",
            "\n",
            "( 150000 ) DE:  Wenn die Buchung vor 14 : 00 Uhr 3 , Tage vor dem geplanten Anreisetag storniert wird , fällt keine Stornierungsgebühr an .\n",
            "\n",
            "( 150000 ) EN:  There will be no cancellation charge if a booking is cancelled before 14 : 00 3 days before your date of arrival .\n",
            "\n",
            "( 160000 ) DE:  Im geräumigen Hotelrestaurant Al Caminetto kosten Sie Gerichte aus Mailand und aus aller Welt .\n",
            "\n",
            "( 160000 ) EN:  The hotel ’ s restaurant , Al Caminetto , serves Milanese and international cuisine .\n",
            "\n",
            "( 170000 ) DE:  Während der 60 &apos; er Jahre gab es viele Regisseure die in die Wüste von Ameria zogen um , mit der ...\n",
            "\n",
            "( 170000 ) EN:  During the 1960s , numerous movie directors chose Almeria &apos;s desert ##AT##-##AT## like landscape to film some of ...\n",
            "\n",
            "( 180000 ) DE:  Tikje krappe kamer voor het aanwezige meubilair en de lift is absoluut niet meer van deze tijd : veel te klein .\n",
            "\n",
            "( 180000 ) EN:  Chambre minuscule , rien à voir avec les photos présentées , SDB &quot; vieillotte &quot; . Absence totale d &apos;insonorisation : l &apos;intimité de vos voisins de chambre en direct ....... Séjour écourté ......\n",
            "\n",
            "( 190000 ) DE:  Das Großunternehmen sieht sich einfach die Produkte des kleinen Unternehmens an und unterstellt so viele Patentverletzungen , wie es nur geht .\n",
            "\n",
            "( 190000 ) EN:  The large corporation will look at the products of the small company and bring up as many patent infringement assertions as possible .\n",
            "\n",
            "( 200000 ) DE:  Wochentags bis 22 Uhr , Samstags bis 18 Uhr geöffnet . Sehr sympathische Atmosphäre .\n",
            "\n",
            "( 200000 ) EN:  This is an interactive multimedia tour ( choice of languages ) through Weimar &apos;s history from prehistoric times to the present .\n",
            "\n",
            "( 210000 ) DE:  Wann möchten Sie im Entrecercas übernachten ?\n",
            "\n",
            "( 210000 ) EN:  When would you like to stay at the Entrecercas ?\n",
            "\n",
            "( 220000 ) DE:  In der ordentlichen Sitzung am 22. September 2008 befasste sich der Aufsichtsrat mit strategischen Themen aus den einzelnen Geschäftsbereichen wie der Positionierung des Kassamarktes im Wettbewerb mit außerbörslichen Handelsplattformen , den Innovationen im Derivatesegment und verschiedenen Aktivitäten im Nachhandelsbereich .\n",
            "\n",
            "( 220000 ) EN:  At the regular meeting on 22 September 2008 , the Supervisory Board dealt with strategic issues from the various business areas , such as the positioning of the cash market in competition with OTC trading platforms , innovation in the derivatives segment and various post ##AT##-##AT## trading activities .\n",
            "\n",
            "( 230000 ) DE:  Ich hatte keine Sekunde zum Entspannen .\n",
            "\n",
            "( 230000 ) EN:  I never had even one second to relax .\n",
            "\n",
            "( 240000 ) DE:  Das Englisch sprechende Personal steht Ihnen mit Rat und Tat zur Seite , informiert über Sehenswürdigkeiten und arrangiert Ihren Transfer .\n",
            "\n",
            "( 240000 ) EN:  The English ##AT##-##AT## speaking staff are always on hand to make your stay special .\n",
            "\n",
            "Sample test translations (100)\n",
            "DE:  Heute verstehen sich QuarkXPress ® 8 , Photoshop ® und Illustrator ® besser als jemals zuvor . Dank HTML und CSS ­ können Anwender von QuarkXPress inzwischen alle Medien bedienen , und das unabhängig von Anwendungen der Adobe ® Creative Suite ® wie Adobe Flash ® ( SWF ) und Adobe Dreamweaver ® .\n",
            "\n",
            "EN:  Today , QuarkXPress ® 8 has tighter integration with Photoshop ® and Illustrator ® than ever before , and through standards like HTML and CSS , QuarkXPress users can publish across media both independently and alongside Adobe ® Creative Suite ® applications like Adobe Flash ® ( SWF ) and Adobe Dreamweaver ® .\n",
            "\n",
            "DE:  Das Hotel Opera befindet sich in der Nähe des Royal Theatre , Kongens Nytorv , &apos; Stroget &apos; und Nyhavn .\n",
            "\n",
            "EN:  Hotel Opera is situated near The Royal Theatre , Kongens Nytorv , &quot; Strøget &quot; and fascinating Nyhavn .\n",
            "\n",
            "DE:  Es existieren Busverbindungen in nahezu jeden Ort der Provence ( eventuell mit Umsteigen in Aix ##AT##-##AT## en ##AT##-##AT## Provence ) , allerdings sollte beachtet werden , dass die letzten Busse abends ca. um 19 Uhr fahren .\n",
            "\n",
            "EN:  As always in France those highways are expensive but practical , comfortable and fast .\n",
            "\n",
            "DE:  15. einem Dritten bei dem Verstoss gegen eine dieser Regeln zu helfen .\n",
            "\n",
            "EN:  15. assist any third party in engaging in any activity prohibited by these Terms .\n",
            "\n",
            "DE:  Es war staubig , das Bad schmutzig . Sogar die Beleuchtung an der Wand im Flur ( Seitengebäude ) war richtig verstaubt .\n",
            "\n",
            "EN:  It was rather old fashioned in the decoration .\n",
            "\n",
            "DE:  Die Bewohner des Nordens sind ein buntes Völkergemisch aus den verschiedensten Bergstämmen und den Nord ##AT##-##AT## Thais oder kon mueang ; die traditionell in den fruchtbaren Tiefebenen Nordthailands siedeln . In vielerlei Hinsicht halten sich die Nord Thais für die &quot; wahren &quot; Thais , die die Thai ##AT##-##AT## Kultur noch am besten über die Zeit gerettet haben .\n",
            "\n",
            "EN:  From Pratu Chiang Mai market , songthaews also travel to Hang Dong ( 20 baht ) and San Patong , south ##AT##-##AT## west of Chiang Mai .\n",
            "\n",
            "DE:  Auch ist , so denkt Dr. Gutherz , bereits die erste Seite sehr viel versprechend , da sie eine Definition des klinischen Psychotrauma ##AT##-##AT## Begriffes enthält , der er gänzlich zustimmen kann .\n",
            "\n",
            "EN:  At the rhetorical climax of this summary , Dr Goodheart comes across some sentences expressed with great pathos .\n",
            "\n",
            "DE:  Das Cleddau Bridge Hotel ist der ideale Platz um zu entspannen oder geschäftlich zu reisen .\n",
            "\n",
            "EN:  Cleddau Bridge hotel is the ideal place for those who want a relaxing holiday or who travel for business .\n",
            "\n",
            "DE:  Bei einer digitalen Bildkette wird das Intensitätssignal für jedes Pixel ohne analoge Zwischenschritte direkt in der Detektoreinheit digitalisiert , d.h. in Zahlen umgewandelt .\n",
            "\n",
            "EN:  A digital image chain is an image chain that is equipped with a digital detector instead of an analogue one .\n",
            "\n",
            "DE:  Sehr freundliche Auszubildende an der Rezeption , die sehr bemüht noch einen Flug für mich gebucht hat .\n",
            "\n",
            "EN:  First of all I did not like the price ... the next day I went to Milano to a 4 star Hotel for 10 Euro less and super service .. I had a problem with my Internetconnection and the Hotel Maritim did not react right .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSIm4r03Dd9B",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6y2AAMADhrO",
        "colab_type": "text"
      },
      "source": [
        "### Split data into token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNKNvR5YCefJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keep track of how many unknown words were encountered\n",
        "src_unk_count, tgt_unk_count = 0, 0\n",
        "\n",
        "def split_to_tokens(sent,is_source):\n",
        "    '''\n",
        "    This function takes in a sentence (source or target)\n",
        "    and preprocess the sentency with various steps (e.g. removing punctuation)\n",
        "    '''\n",
        "    \n",
        "    global src_unk_count, tgt_unk_count\n",
        "\n",
        "    # Remove punctuation and new-line chars\n",
        "    sent = sent.replace(',',' ,')\n",
        "    sent = sent.replace('.',' .')\n",
        "    sent = sent.replace('\\n',' ') \n",
        "    \n",
        "    sent_toks = sent.split(' ')\n",
        "    for t_i, tok in enumerate(sent_toks):\n",
        "        if is_source:\n",
        "            # src_dictionary contain the word -> word ID mapping for source vocabulary\n",
        "            if tok not in src_dictionary.keys():\n",
        "                if not len(tok.strip())==0:\n",
        "                    sent_toks[t_i] = '<unk>'\n",
        "                    src_unk_count += 1\n",
        "        else:\n",
        "            # tgt_dictionary contain the word -> word ID mapping for target vocabulary\n",
        "            if tok not in tgt_dictionary.keys():\n",
        "                if not len(tok.strip())==0:\n",
        "                    sent_toks[t_i] = '<unk>'\n",
        "                    #print(tok)\n",
        "                    tgt_unk_count += 1\n",
        "    return sent_toks"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BHGU1r-DT77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "31b5502d-743a-4883-f460-87cb39b4be10"
      },
      "source": [
        "# Let us first look at some statistics of the sentences\n",
        "# Train - source data\n",
        "source_len = []\n",
        "source_mean, source_std = 0,0\n",
        "for sent in source_sent:\n",
        "    source_len.append(len(split_to_tokens(sent,True)))\n",
        "\n",
        "print('(Source) Sentence mean length: ', np.mean(source_len))\n",
        "print('(Source) Sentence stddev length: ', np.std(source_len))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Source) Sentence mean length:  26.244692\n",
            "(Source) Sentence stddev length:  13.854376414156501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOQ8nhS1DWGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "54ba21e1-198a-45f5-82d1-dcab03dca826"
      },
      "source": [
        "# Let us first look at some statistics of the sentences\n",
        "# Train - target data\n",
        "target_len = []\n",
        "for sent in target_sent:\n",
        "    target_len.append(len(split_to_tokens(sent,False)))\n",
        "\n",
        "print('(Target) Sentence mean length: ', np.mean(target_len))\n",
        "print('(Target) Sentence stddev length: ', np.std(target_len))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Target) Sentence mean length:  28.275308\n",
            "(Target) Sentence stddev length:  14.925498769057468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7pMPDRFDZkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "e916bebf-3e35-4e16-86d3-13cac89d4eb8"
      },
      "source": [
        "# Let us first look at some statistics of the sentences\n",
        "# Test - source data\n",
        "test_source_len = []\n",
        "for sent in test_source_sent:\n",
        "    test_source_len.append(len(split_to_tokens(sent, True)))\n",
        "    \n",
        "print('(Test-Source) Sentence mean length: ', np.mean(test_source_len))\n",
        "print('(Test-Source) Sentence stddev length: ', np.std(test_source_len))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Test-Source) Sentence mean length:  26.61\n",
            "(Test-Source) Sentence stddev length:  14.800604717375572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUmP8bZGDbVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "9dc91dc3-b829-49b1-94f7-740030500f4d"
      },
      "source": [
        "# Let us first look at some statistics of the sentences\n",
        "# Test - target data\n",
        "test_target_len = []\n",
        "test_tgt_mean, test_tgt_std = 0,0\n",
        "for sent in test_target_sent:\n",
        "    test_target_len.append(len(split_to_tokens(sent, False)))\n",
        "    \n",
        "print('(Test-Target) Sentence mean length: ', np.mean(test_target_len))\n",
        "print('(Test-Target) Sentence stddev length: ', np.std(test_target_len))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Test-Target) Sentence mean length:  29.08\n",
            "(Test-Target) Sentence stddev length:  16.19424589167399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqJ_iHdbD7n5",
        "colab_type": "text"
      },
      "source": [
        "### Making training and testing data fixed length\n",
        "Here we get all the source sentences and target sentences to a fixed length. This is, so that we can process the sentences as batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBSowcRGD983",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "7cb8f25e-01b4-4871-ad8f-35be1608fa07"
      },
      "source": [
        "# Processing training data\n",
        "\n",
        "src_unk_count, tgt_unk_count = 0, 0\n",
        "\n",
        "train_inputs = []\n",
        "train_outputs = []\n",
        "\n",
        "# Chosen based on previously found statistics\n",
        "src_max_sent_length = 41 \n",
        "tgt_max_sent_length = 61\n",
        "\n",
        "print('Processing Training Data ...\\n')\n",
        "for s_i, (src_sent, tgt_sent) in enumerate(zip(source_sent,target_sent)):\n",
        "    # Break source and target sentences to word lists\n",
        "    src_sent_tokens = split_to_tokens(src_sent,True)\n",
        "    tgt_sent_tokens = split_to_tokens(tgt_sent,False)\n",
        "    \n",
        "    # Append <s> token's ID to the beggining of source sentence\n",
        "    num_src_sent = [src_dictionary['<s>']]\n",
        "    # Add the rest of word IDs for words found in the source sentence \n",
        "    for tok in src_sent_tokens:\n",
        "        if tok in src_dictionary.keys():\n",
        "            num_src_sent.append(src_dictionary[tok])\n",
        "\n",
        "    # If the lenghth of the source sentence below the maximum allowed length\n",
        "    # append </s> token's ID to the end\n",
        "    if len(num_src_sent)<src_max_sent_length:\n",
        "        num_src_sent.extend([src_dictionary['</s>'] for _ in range(src_max_sent_length - len(num_src_sent))])\n",
        "\n",
        "    # If the length exceed the maximum allowed length\n",
        "    # truncate the sentence\n",
        "    elif len(num_src_sent)>src_max_sent_length:\n",
        "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
        "        \n",
        "    # Make sure the sentence is of length src_max_sent_length\n",
        "    assert len(num_src_sent)==src_max_sent_length,len(num_src_sent)\n",
        "\n",
        "    train_inputs.append(num_src_sent)\n",
        "    \n",
        "    # Create the numeric target sentence with word IDs\n",
        "    # append <s> to the beginning and append actual words later\n",
        "    num_tgt_sent = [tgt_dictionary['<s>']]\n",
        "    for tok in tgt_sent_tokens:\n",
        "        if tok in tgt_dictionary.keys():\n",
        "            num_tgt_sent.append(tgt_dictionary[tok])\n",
        "        \n",
        "    ## Modifying the outputs such that all the outputs have max_length elements\n",
        "    if len(num_tgt_sent)<tgt_max_sent_length:\n",
        "        num_tgt_sent.extend([tgt_dictionary['</s>'] for _ in range(tgt_max_sent_length - len(num_tgt_sent))])\n",
        "    elif len(num_tgt_sent)>tgt_max_sent_length:\n",
        "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
        "        \n",
        "    train_outputs.append(num_tgt_sent)\n",
        "    \n",
        "print('Unk counts Src: %d, Tgt: %d'%(src_unk_count, tgt_unk_count))\n",
        "print('Sentences ',len(train_inputs))\n",
        "\n",
        "assert len(train_inputs)  == len(source_sent),\\\n",
        "        'Size of total elements: %d, Total sentences: %d'\\\n",
        "                %(len(train_inputs),len(source_sent))\n",
        "\n",
        "# Making inputs and outputs NumPy arrays\n",
        "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
        "train_outputs = np.array(train_outputs, dtype=np.int32)\n",
        "\n",
        "# Make sure number of inputs and outputs dividable by 100\n",
        "train_inputs = train_inputs[:(train_inputs.shape[0]//100)*100,:]\n",
        "train_outputs = train_outputs[:(train_outputs.shape[0]//100)*100,:]\n",
        "print('\\t Done processing training data \\n')\n",
        "\n",
        "# Printing some data\n",
        "print('Samples from training data')\n",
        "for ti in range(10):\n",
        "    print('\\t',[src_reverse_dictionary[w]  for w in train_inputs[ti,:].tolist()])\n",
        "    print('\\t',[tgt_reverse_dictionary[w]  for w in train_outputs[ti,:].tolist()])\n",
        "print()\n",
        "print('\\tSentences ',train_inputs.shape[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Training Data ...\n",
            "\n",
            "Unk counts Src: 464223, Tgt: 214783\n",
            "Sentences  250000\n",
            "\t Done processing training data \n",
            "\n",
            "Samples from training data\n",
            "\t ['<s>', 'Hier', 'erfahren', 'Sie', ',', 'wie', 'Sie', 'Creative', 'Suite', '2', 'und', 'Creative', 'Suite', '3', 'am', 'besten', 'zusammen', 'mit', 'QuarkXPress', 'nutzen', 'können', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Here', ',', 'you', '’', 'll', 'find', 'out', 'how', 'Creative', 'Suite', 'users', 'can', 'get', 'the', 'best', 'possible', 'interaction', 'with', 'QuarkXPress', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Sie', 'werden', 'überrascht', 'sein', ',', 'wie', 'einfach', 'sich', 'mit', 'Quark', 'das', 'volle', 'Potenzial', 'Ihrer', 'Design', '##AT##-##AT##', 'Software', 'erschließen', 'lässt', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'You', '’', 'll', 'be', 'surprised', 'how', 'easy', 'Quark', 'has', 'made', 'it', 'to', 'unlock', 'the', 'full', 'potential', 'of', 'all', 'your', 'design', 'software', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Häufig', 'wird', 'die', 'Meinung', 'vertreten', ',', 'dass', 'QuarkXPress', '8', 'von', 'allen', 'heute', 'verfügbaren', 'Layout', '##AT##-##AT##', 'Programmen', 'die', 'beste', 'Integration', 'mit', 'Photoshop', 'über', 'das', 'PSD', '##AT##-##AT##', 'Dateiformat', 'bietet', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'QuarkXPress', '8', 'is', 'considered', 'by', 'many', 'to', 'have', 'the', 'best', 'integration', 'with', 'Photoshop', '’', 's', 'PSD', 'file', 'format', 'of', 'any', 'layout', 'tool', 'available', 'today', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'In', 'diesem', 'Abschnitt', 'erläutern', 'wir', ',', 'wann', 'Sie', 'für', 'Ihre', 'Bilder', 'das', 'PSD', '##AT##-##AT##', 'Format', 'verwenden', 'sollten', 'und', 'wie', 'Sie', 'es', 'für', 'Ihre', 'Bilder', 'optimal', 'nutzen', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'In', 'this', 'section', 'we', '’', 'll', 'explain', 'when', 'you', 'should', 'use', 'the', 'PSD', 'format', 'for', 'your', 'images', 'and', 'how', 'to', 'get', 'the', 'most', 'out', 'of', 'them', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Angenommen', 'Sie', 'haben', 'verschiedene', 'Ebenen', 'in', 'Ihrer', 'PSD', '##AT##-##AT##', 'Datei', 'mit', 'verschiedenen', 'Darstellungen', 'eines', 'Produkts', ',', 'die', 'je', 'nach', 'Verwendungszweck', 'ausgewählt', 'werden', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'For', 'example', ',', 'you', 'may', 'have', 'multiple', 'layers', 'in', 'your', 'PSD', 'with', 'different', 'product', 'shots', ',', 'which', 'will', 'vary', 'from', 'publication', 'to', 'publication', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Wenn', 'Sie', 'mit', 'PSD', 'arbeiten', ',', 'können', 'Sie', 'diese', 'Ebenen', 'in', 'QuarkXPress', 'ein-', 'oder', 'ausschalten', ',', 'ohne', 'für', 'jede', 'Veröffentlichung', 'eine', 'eigene', 'TIFF', '##AT##-##AT##', 'Datei', 'generieren', 'zu', 'müssen', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'If', 'you', 'use', 'PSD', ',', 'you', 'can', 'switch', 'those', 'layers', 'on', 'or', 'off', 'in', 'QuarkXPress', 'without', 'having', 'to', 'save', 'a', 'separate', 'TIFF', 'for', 'each', 'publication', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Eine', 'andere', 'mögliche', 'Frage', 'für', 'die', 'Entscheidung', 'zwischen', 'PSD', 'und', 'TIFF', 'ist', ':', '„', 'Muss', 'ich', 'für', 'dieses', 'Bild', 'eine', '<unk>', 'verwenden', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Another', 'question', 'that', 'might', 'tip', 'you', 'in', 'favor', 'of', 'PSD', 'is', ',', '&quot;', 'Do', 'I', 'need', 'to', 'use', 'a', 'spot', 'color', 'with', 'this', 'image', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '&quot;', 'In', 'den', 'meisten', '<unk>', 'sind', '<unk>', 'oft', 'problematisch', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '&quot;', 'Using', 'spot', 'colors', 'in', 'most', 'image', 'formats', 'is', 'often', 'complicated', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Da', 'QuarkXPress', 'jedoch', 'PSD', '##AT##-##AT##', 'Kanäle', 'unterstützt', ',', 'geht', 'es', 'mit', 'PSD', 'einfacher', 'und', 'flexibler', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'However', ',', 'because', 'of', 'the', 'way', 'QuarkXPress', 'supports', 'PSD', 'channels', ',', 'it', '’', 's', 'simpler', 'and', 'more', 'flexible', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Erstellen', 'Sie', 'einen', 'Rahmen', 'und', 'gehen', 'Sie', 'dann', 'auf', 'Datei', '&gt;', 'Importieren', '.', '.', '.', 'oder', 'ziehen', 'Sie', 'das', 'Bild', 'einfach', 'per', 'Drag', '&amp;', 'Drop', 'von', 'Ihrem', 'Desktop', ',', 'aus', 'dem', 'Finder', 'oder', 'einer', 'Anwendung', 'wie', 'Adobe', 'Bridge', '<unk>', '–']\n",
            "\t ['<s>', 'Bringing', 'the', 'PSD', 'files', 'into', 'QuarkXPress', 'is', 'the', 'same', 'as', 'any', 'other', 'image', '.', 'Create', 'a', 'Box', 'and', 'then', 'use', 'File', '&gt;', 'Import', '.', '.', '.', 'or', 'simply', 'drag', 'and', 'drop', 'the', 'image', 'from', 'your', 'desktop', ',', 'Finder', 'or', 'an', 'application', 'like', 'Adobe', 'Bridge', '®', 'with', 'or', 'without', 'creating', 'a', 'box', 'first', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\n",
            "\tSentences  250000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq-Pbd4GEa7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "07a7c55b-2361-445a-d2e7-f8d15ab24e5a"
      },
      "source": [
        "# Processing Test data\n",
        "\n",
        "src_unk_count, tgt_unk_count = 0, 0\n",
        "print('Processing testing data ....\\n')\n",
        "test_inputs = []\n",
        "test_outputs = []\n",
        "for s_i, (src_sent,tgt_sent) in enumerate(zip(test_source_sent,test_target_sent)):\n",
        "    src_sent_tokens = split_to_tokens(src_sent,True)\n",
        "    tgt_sent_tokens = split_to_tokens(tgt_sent,False)\n",
        "    \n",
        "    num_src_sent = [src_dictionary['<s>']]\n",
        "    for tok in src_sent_tokens:\n",
        "        if tok in src_dictionary.keys():\n",
        "            num_src_sent.append(src_dictionary[tok])\n",
        "    \n",
        "    num_tgt_sent = [src_dictionary['<s>']]\n",
        "    for tok in tgt_sent_tokens:\n",
        "        if tok in tgt_dictionary.keys():\n",
        "            num_tgt_sent.append(tgt_dictionary[tok])\n",
        "        \n",
        "    # Append </s> if the length is not src_max_sent_length\n",
        "    if len(num_src_sent)<src_max_sent_length:\n",
        "        num_src_sent.extend([src_dictionary['</s>'] for _ in range(src_max_sent_length - len(num_src_sent))])\n",
        "    # Truncate the sentence if length is over src_max_sent_length\n",
        "    elif len(num_src_sent)>src_max_sent_length:\n",
        "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
        "        \n",
        "    assert len(num_src_sent)==src_max_sent_length, len(num_src_sent)\n",
        "\n",
        "    test_inputs.append(num_src_sent)\n",
        "    \n",
        "    # Append </s> is length is not tgt_max_sent_length\n",
        "    if len(num_tgt_sent)<tgt_max_sent_length:\n",
        "        num_tgt_sent.extend([tgt_dictionary['</s>'] for _ in range(tgt_max_sent_length - len(num_tgt_sent))])\n",
        "    # Truncate the sentence if length over tgt_max_sent_length\n",
        "    elif len(num_tgt_sent)>tgt_max_sent_length:\n",
        "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
        "        \n",
        "    assert len(num_tgt_sent)==tgt_max_sent_length, len(num_tgt_sent)\n",
        "\n",
        "    test_outputs.append(num_tgt_sent)\n",
        "\n",
        "# Printing some data\n",
        "print('Unk counts Tgt: %d, Tgt: %d'%(src_unk_count, tgt_unk_count))    \n",
        "print('Done processing testing data ....\\n')\n",
        "test_inputs = np.array(test_inputs,dtype=np.int32)\n",
        "test_outputs = np.array(test_outputs,dtype=np.int32)\n",
        "print('Samples from training data')\n",
        "for ti in range(10):\n",
        "    print('\\t',[src_reverse_dictionary[w]  for w in test_inputs[ti,:].tolist()])\n",
        "    print('\\t',[tgt_reverse_dictionary[w]  for w in test_outputs[ti,:].tolist()])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing testing data ....\n",
            "\n",
            "Unk counts Tgt: 212, Tgt: 107\n",
            "Done processing testing data ....\n",
            "\n",
            "Samples from training data\n",
            "\t ['<s>', 'Heute', 'verstehen', 'sich', 'QuarkXPress', '®', '8', ',', 'Photoshop', '®', 'und', 'Illustrator', '®', 'besser', 'als', 'jemals', 'zuvor', '.', 'Dank', 'HTML', 'und', 'CSS', '\\xad', 'können', 'Anwender', 'von', 'QuarkXPress', 'inzwischen', 'alle', 'Medien', 'bedienen', ',', 'und', 'das', 'unabhängig', 'von', 'Anwendungen', 'der', 'Adobe', '®', 'Creative']\n",
            "\t ['<s>', 'Today', ',', 'QuarkXPress', '®', '8', 'has', 'tighter', 'integration', 'with', 'Photoshop', '®', 'and', 'Illustrator', '®', 'than', 'ever', 'before', ',', 'and', 'through', 'standards', 'like', 'HTML', 'and', 'CSS', ',', 'QuarkXPress', 'users', 'can', 'publish', 'across', 'media', 'both', 'independently', 'and', 'alongside', 'Adobe', '®', 'Creative', 'Suite', '®', 'applications', 'like', 'Adobe', 'Flash', '®', '(', 'SWF', ')', 'and', 'Adobe', 'Dreamweaver', '®', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Je', 'mehr', 'Zeit', 'wir', 'mit', 'Gilad', 'und', 'dem', 'Rest', 'des', 'Teams', 'in', 'Israel', 'verbracht', 'haben', '(', 'um', 'nicht', 'den', 'lauten', 'Hahn', 'zu', 'erwähnen', 'der', '<unk>', 'bei', 'denen', 'über', 'den', 'Campus', '<unk>', ')', 'desto', 'überzeugter', 'waren', 'wir', '–', 'zusammen', 'können', 'wir']\n",
            "\t ['<s>', 'The', 'more', 'time', 'we', 'spent', 'with', 'Gilad', 'as', 'well', 'as', 'the', 'rest', 'of', 'the', 'team', 'in', 'Israel', '(', 'not', 'to', 'mention', 'the', 'very', 'loud', '<unk>', 'that', 'runs', 'around', 'in', 'their', 'campus', ')', ',', 'the', 'more', 'convinced', 'we', 'all', 'became', '-', 'we', '’', 'll', 'be', 'better', 'off', 'together', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '34', 'Diese', 'a', 'Worte', 'sind', 'wahr', 'und', 'treu', ';', 'darum', '<unk>', 'sie', 'nicht', ',', 'und', 'b', 'nehmt', 'auch', 'nichts', 'davon', 'weg', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '34', 'These', 'sayings', 'are', 'a', 'true', 'and', 'faithful', ';', 'wherefore', ',', 'transgress', 'them', 'not', ',', 'neither', 'b', 'take', 'therefrom', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '&#124;', 'Ferienwohnungen', '1', 'Zi', '&#124;', 'Ferienhäuser', '&#124;', 'Landhäuser', '&#124;', 'Autovermietung', '&#124;', 'Last', 'Minute', 'Angebote', '!', '!', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', '&#124;', '1', 'Bedroom', 'Apts', '&#124;', 'Holiday', 'houses', '&#124;', 'Rural', 'Homes', '&#124;', 'Car', 'Rental', '&#124;', 'Last', 'Minute', 'Offers', '!', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Der', '<unk>', 'Teil', 'der', 'Insel', 'besteht', 'aus', 'Granit', 'und', '<unk>', ',', 'von', 'Ton', 'überlagert', ',', 'und', 'bildet', 'eine', 'ca', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'A', 'battle', 'between', 'Denmark', 'and', 'Sweden', 'in', '<unk>', 'led', 'to', 'Swedish', 'control', 'of', 'the', 'island', ',', 'but', 'it', 'was', 'brief', '-', 'they', 'left', 'again', 'the', 'same', 'year', '.', 'In', 'the', '<unk>', 'of', '<unk>', '<unk>', ',', '<unk>', ',', '<unk>', 'and', '<unk>', 'were', 'given', 'to', 'Sweden', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Mag', 'sein', ',', 'dass', 'du', 'deine', 'ersten', '<unk>', 'in', 'einem', '<unk>', ',', '<unk>', 'Kahn', '<unk>', '-', 'aber', 'mit', 'der', 'Zeit', 'wirst', 'du', 'dich', 'zum', '<unk>', '<unk>', 'oder', 'edlen', 'Katamaran', '<unk>', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'You', 'may', 'be', 'starting', 'in', 'a', '<unk>', 'old', 'tub', 'of', 'a', 'boat', ',', 'but', 'in', 'no', 'time', 'at', 'all', 'you', '&apos;ll', 'be', 'able', 'to', 'buy', 'a', 'fancy', '<unk>', ',', 'or', 'a', 'classy', 'catamaran', '.', 'Turn', 'your', 'newfound', 'fame', 'into', 'money', ',', 'and', 'spend', 'it', 'to', 'buy', 'lavish', 'new', 'homes', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'In', 'der', '<unk>', 'im', 'Internet', 'müßte', 'die', 'Zufahrt', 'beschrieben', 'werden', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'There', 'are', 'no', 'adverse', 'comments', 'about', 'this', 'hotel', 'at', 'all', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Ideale', 'Lage', 'für', 'Exkursionen', 'in', 'die', 'Stadt', 'und', 'Nähe', 'zur', 'Promenade', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'There', 'was', 'plenty', 'of', 'space', 'in', 'the', 'room', 'and', 'a', 'nice', 'garden', 'to', 'sit', 'and', 'have', 'a', 'drink', 'and', 'smoke', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Das', 'Hotel', '<unk>', 'verfügt', 'über', 'eine', 'ideale', ',', 'ruhige', 'Lage', 'in', 'einem', 'geschäftigen', 'Viertel', 'mit', 'guter', 'Verkehrsanbindung', '.', 'Der', 'Bahnhof', 'und', 'eine', 'U', '##AT##-##AT##', 'Bahnstation', 'liegen', 'in', 'der', 'Nähe', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Hotel', '<unk>', 'welcomes', 'you', 'to', 'a', 'busy', 'yet', 'quiet', 'area', 'of', 'Milan', ',', 'within', 'walking', 'distance', 'of', 'excellent', 'transport', 'links', ',', 'including', 'the', 'central', 'railway', 'station', 'and', 'the', 'Repubblica', 'metro', 'station', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Zum', 'klimatisierten', 'Hotel', 'gehören', 'auch', 'ein', '<unk>', 'und', 'eine', 'traumhafte', 'Sonnenterrasse', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\t ['<s>', 'Apart', 'from', 'this', ',', 'the', 'guests', 'can', 'enjoy', 'the', 'facility', 'of', 'an', 'independent', 'air', '##AT##-##AT##', 'conditioning', 'system', ',', 'a', 'jacuzzi', 'and', 'a', 'beautiful', 'sun', 'terrace', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ypb9z2TEfe5",
        "colab_type": "text"
      },
      "source": [
        "### Learning word embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU1qRIJaHzYC",
        "colab_type": "text"
      },
      "source": [
        "#### Run algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STcf6PTbF0b2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "574480a2-3c4a-409e-9f88-3f9e887fb214"
      },
      "source": [
        "# Total number of sentences\n",
        "tot_sentences = train_inputs.shape[0]\n",
        "print('Total number of training sentences: ',tot_sentences)\n",
        "\n",
        "# we keep a cursor for each sentence in the training set\n",
        "sentence_cursors = [0 for _ in range(tot_sentences)] \n",
        "\n",
        "batch_size = 64\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "\n",
        "# Defining various things needed by the python script\n",
        "define_data_and_hyperparameters(\n",
        "    tot_sentences, \n",
        "    src_max_sent_length, \n",
        "    tgt_max_sent_length, \n",
        "    src_dictionary, \n",
        "    tgt_dictionary,\n",
        "    src_reverse_dictionary, \n",
        "    tgt_reverse_dictionary, \n",
        "    train_inputs, \n",
        "    train_outputs, \n",
        "    embedding_size,\n",
        "    vocabulary_size\n",
        ")\n",
        "\n",
        "# Print some batches to make sure the data generator is correct\n",
        "print_some_batches()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of training sentences:  250000\n",
            "\n",
            "with window_size = 1:\n",
            "    batch: [['<s>', 'Ende'], ['<s>', ':'], ['<s>', 'Sie'], ['<s>', 'AB'], ['<s>', 'gibt'], ['<s>', 'hat'], ['<s>', 'nimmt'], ['<s>', 'möchten']]\n",
            "    labels: ['Seit', 'F', 'Denken', 'Der', 'Es', 'Jaca', 'Man', 'Wann']\n",
            "\n",
            "with window_size = 2:\n",
            "    batch: [['<s>', 'Betten', '13', '€'], ['<s>', '##STAR##', 'Programm', 'teilt'], ['<s>', 'Wir', 'diese', '<unk>'], ['<s>', 'Der', 'zwischen', '<unk>'], ['<s>', 'Schließlich', 'die', 'Zivilgesellschaft'], ['<s>', '<unk>', ',', '<unk>'], ['<s>', 'Das', 'besitzt', 'großzügig'], ['<s>', 'Unseren', 'steht', 'auch']]\n",
            "    labels: ['ab', 'Das', 'haben', 'Wanderweg', 'muss', 'Garten', 'Appartement', 'Gäste']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwfe-6RLKx3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "e48b5ae3-9d96-4b89-9b0d-0390c5fa3912"
      },
      "source": [
        "# Define TensorFlow ops for learning word embeddings\n",
        "define_word2vec_tensorflow(batch_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining 4 embedding lookups representing each word in the context\n",
            "Stacked embedding size: [64, 128, 4]\n",
            "Reduced mean embedding size: [64, 128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU1-ie2VL7DG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f2d500d-432b-46e6-cc4e-2fea47446508"
      },
      "source": [
        "# Run embedding learning for source language\n",
        "# Stores the de-embeddings-tmp.npy into the disk\n",
        "run_word2vec_source(batch_size)\n",
        "# Run embedding learning for target language\n",
        "# Stores the en-embeddings-tmp.npy to the disk\n",
        "run_word2vec_target(batch_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Average loss at step 2000: 3.532927\n",
            "Average loss at step 4000: 2.701178\n",
            "Average loss at step 6000: 2.484155\n",
            "Average loss at step 8000: 2.377980\n",
            "Average loss at step 10000: 2.294842\n",
            "Nearest to und: unterhält, Schenken, bisher, besinnen, Güterverkehrs, Autofokus, Tennis, Ruhige,\n",
            "Nearest to bei: Sinn, Laptop, hielten, Klimagipfel, Setzen, siegen, Yeshé, Laufe,\n",
            "Nearest to ich: Ich, Uniform, bildete, dies, beispielsweise, mysqld, down, kürzere,\n",
            "Nearest to -: Seltenheit, desk, 1790, Gesetzesvorlage, Verträge, förderlich, letztendlichen, Swisscom,\n",
            "Nearest to wenn: Wahrlich, sage, Siehe, dein, D3, weiter, daß, siehe,\n",
            "Nearest to Union: Altstadt, much, Fläche, quite, Benalmadena, Zusammenstellung, hundertprozentig, Frankfurt,\n",
            "Nearest to die: diese, formiert, Diese, Dichter, BIS, ihre, Ihre, Ihren,\n",
            "Nearest to Die: Unsere, die, Eine, der, Ferien, vegetarischen, Seine, jahre,\n",
            "Nearest to :: &#93;, página, &gt;, Kay, sicherheitsrelevanten, vous, gestiegenen, IUU,\n",
            "Nearest to alle: Mitglied, nur, Lagos, Männer, Canaria, Stärken, Dubliner, ca,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Fehlverhalten, Sicherheitsmaßnahmen, schwerwiegenden, Minderheitenschutz, Nam, abzulenken,\n",
            "Nearest to beste: Entdecke, Modem, Strecke, Somit, verließ, Stränden, kollektiven, Bewertung,\n",
            "Nearest to anderem: sitzen, mehrere, menos, eigenem, Mai, Unterrichts, häufig, Kinder,\n",
            "Nearest to Aufenthalt: Doppelbett, Fahrzeug, besuchte, Siehe, Saxophon, adresse, Genießen, kostenlosen,\n",
            "Nearest to zahlreichen: Website, Tatsache, Ankunft, Probieren, brutal, Stornierungen, optimale, entlang,\n",
            "Nearest to erfüllen: Auskünfte, Schwellenländer, unverbindliche, entlang, Fairness, Einwilligung, begleiteten, Tagungsräume,\n",
            "Nearest to Werte: geräumige, LCD, richtiges, Spanien, Verwendung, Umzüge, Ansonsten, nächste,\n",
            "Nearest to 2002: normalen, Ecofin, Bonn, tropische, 94, mühsam, dezentralisierte, Züge,\n",
            "Nearest to getan: Tips, Ihrem, lokale, Löcher, Pension, zivilen, walk, Sellaronda,\n",
            "Nearest to Natürlich: Ebenso, hier, hinaus, Wollen, Außerdem, gemütlicher, wäre, Entspannen,\n",
            "Average loss at step 12000: 2.249412\n",
            "Average loss at step 14000: 2.214257\n",
            "Average loss at step 16000: 2.170386\n",
            "Average loss at step 18000: 2.140362\n",
            "Average loss at step 20000: 2.094432\n",
            "Nearest to und: besinnen, Güterverkehrs, Schenken, versinkt, oder, bisher, Wänden, Schlampen,\n",
            "Nearest to bei: neben, Sinn, nach, hielten, Knesset, feine, anzugeben, auf,\n",
            "Nearest to ich: Ich, er, sie, sagen, kürzere, dies, bildete, mysqld,\n",
            "Nearest to -: 1790, PRI, Seltenheit, desk, Verträge, Swisscom, voor, bij,\n",
            "Nearest to wenn: Wahrlich, daß, weil, sage, Siehe, ob, weiß, werdet,\n",
            "Nearest to Union: vorsätzlich, much, Fläche, Altstadt, Gefangenschaft, quite, Zusammenstellung, Metall,\n",
            "Nearest to die: diese, ihre, Ihre, Die, Dichter, keine, eine, unsere,\n",
            "Nearest to Die: Unsere, Eine, die, Seine, eine, Frauentag, Diese, Alle,\n",
            "Nearest to :: &#93;, Kay, &gt;, nachhaltigere, sicherheitsrelevanten, gestiegenen, acht, Hinweis,\n",
            "Nearest to alle: diese, Spätere, Lagos, Stärken, Dubliner, Wölfe, Evolution, Unsere,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Fehlverhalten, Sicherheitsmaßnahmen, schwerwiegenden, Minderheitenschutz, benannt, abzulenken,\n",
            "Nearest to beste: Entdecke, Chance, Modem, bevorzugte, Figur, Zahlen, relativ, Slogan,\n",
            "Nearest to anderem: sitzen, menos, Konferenzen, Gerätes, Unterrichts, mehrere, Kinder, Versandkosten,\n",
            "Nearest to Aufenthalt: Urlaub, Doppelbett, kostenlosen, Gegner, Fahrzeug, Ceuta, Beginnen, heart,\n",
            "Nearest to zahlreichen: Verbesserung, Lied, brutal, Website, Tippen, Stornierungen, Klagen, Buchstaben,\n",
            "Nearest to erfüllen: Auskünfte, Schwellenländer, begleiteten, Einwilligung, Fairness, unterstützen, verhält, könne,\n",
            "Nearest to Werte: nächste, Schieberegler, richtiges, Position, Verwendung, Klicke, geräumige, LCD,\n",
            "Nearest to 2002: 2004, tropische, normalen, Ecofin, Bonn, 94, Pflicht, Stühle,\n",
            "Nearest to getan: Tips, Löcher, Antiquitäten, gepflegt, töten, lokale, Ergänzungen, wünsche,\n",
            "Nearest to Natürlich: Ebenso, Außerdem, Wollen, hier, genau, Dabei, Somit, hinaus,\n",
            "Average loss at step 22000: 2.058442\n",
            "Average loss at step 24000: 2.020735\n",
            "Average loss at step 26000: 1.985546\n",
            "Average loss at step 28000: 1.965755\n",
            "Average loss at step 30000: 1.922434\n",
            "Nearest to und: besinnen, oder, Güterverkehrs, Schenken, Schlampen, versinkt, Zink, bisher,\n",
            "Nearest to bei: neben, Knesset, wegen, hielten, anzugeben, nach, derselbe, Hierin,\n",
            "Nearest to ich: Ich, er, sie, sagen, dies, jemand, Bundesregierung, mysqld,\n",
            "Nearest to -: 1790, Portland, Hörner, erarbeiten, virtuelles, Gelb, Jungen, absolutely,\n",
            "Nearest to wenn: weil, Wahrlich, falls, daß, werdet, da, ob, sieht,\n",
            "Nearest to Union: vorsätzlich, Altstadt, Gefangenschaft, Rand, Zusammenstellung, much, strukturieren, Frankfurt,\n",
            "Nearest to die: diese, ihre, Die, Ihre, keine, Dichter, unsere, einige,\n",
            "Nearest to Die: Eine, Unsere, Diese, die, Seine, Gute, Frauentag, Alle,\n",
            "Nearest to :: &#93;, Kay, nachhaltigere, mittelfristig, Ladegeräte, acht, sicherheitsrelevanten, Hinweis,\n",
            "Nearest to alle: diese, Spätere, Männer, Zustellbetten, Aussichtsplattform, jeweils, Statistiken, weitere,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Sicherheitsmaßnahmen, Fehlverhalten, schwerwiegenden, benannt, Minderheitenschutz, Vollmitgliedschaft,\n",
            "Nearest to beste: Entdecke, bevorzugte, Slogan, perfekte, exzellent, verboten, Spezialität, Modem,\n",
            "Nearest to anderem: Umständen, sitzen, vielseitige, mehrere, Unterrichts, Getränken, Gerätes, Leitung,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Fahrzeug, Ceuta, Doppelbett, Überblick, Visualisierung, Gegner,\n",
            "Nearest to zahlreichen: Klagen, umliegenden, modernem, ihrer, Bürgerkrieg, Anwendern, Verbesserung, betreuten,\n",
            "Nearest to erfüllen: unterstützen, Schwellenländer, begleiteten, Auskünfte, Einwilligung, Fairness, könne, Claude,\n",
            "Nearest to Werte: nächste, Schieberegler, Sonderpreise, Englischen, Umzüge, richtiges, Lastschrift, Reisezeit,\n",
            "Nearest to 2002: 2004, Mai, 2008, 2005, 1986, 1968, Ecofin, August,\n",
            "Nearest to getan: Löcher, wünsche, töten, Tips, Hallenbad, lokale, Antiquitäten, duldet,\n",
            "Nearest to Natürlich: Ebenso, genau, Wollen, hier, hierzu, Außerdem, Weiterhin, Gelegenheit,\n",
            "Average loss at step 32000: 1.892842\n",
            "Average loss at step 34000: 1.871343\n",
            "Average loss at step 36000: 1.831999\n",
            "Average loss at step 38000: 1.800740\n",
            "Average loss at step 40000: 1.773401\n",
            "Nearest to und: besinnen, oder, Güterverkehrs, Zink, Schlampen, versinkt, zurücklassen, Baldo,\n",
            "Nearest to bei: wegen, neben, Knesset, anzugeben, folgen, hielten, Belarussen, mit,\n",
            "Nearest to ich: Ich, er, sie, sagen, jemand, Bundesregierung, mysqld, verehrter,\n",
            "Nearest to -: Portland, 1790, erarbeiten, Gelb, Hörner, absolutely, Startseite, virtuelles,\n",
            "Nearest to wenn: falls, weil, Wahrlich, sieht, da, werdet, daß, braucht,\n",
            "Nearest to Union: vorsätzlich, Gefangenschaft, Rand, much, Altstadt, Zusammenstellung, nahe, strukturieren,\n",
            "Nearest to die: diese, ihre, Die, keine, seine, Ihre, einige, unsere,\n",
            "Nearest to Die: Unsere, Eine, Diese, die, Seine, Frauentag, Alle, Zur,\n",
            "Nearest to :: &#93;, Kay, mittelfristig, nachhaltigere, ad, acht, Ladegeräte, Ferry,\n",
            "Nearest to alle: sämtliche, jeweils, Welche, aller, diese, Jerusalems, Alle, Bleche,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Sicherheitsmaßnahmen, Fehlverhalten, schwerwiegenden, Minderheitenschutz, benannt, Hillary,\n",
            "Nearest to beste: perfekte, bevorzugte, ideale, exzellent, Entdecke, Spezialität, Slogan, Writer,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, vielseitige, sitzen, Unterrichts, http, Panoramablick, Schwierigkeitsgrade,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Ceuta, Visualisierung, Rahmen, Fahrzeug, Überblick, Doppelbett,\n",
            "Nearest to zahlreichen: modernem, Klagen, umliegenden, belassen, öffentlichen, Anwendern, Vermietung, ihrer,\n",
            "Nearest to erfüllen: spielen, unterstützen, könne, begleiteten, angibt, Auskünfte, halten, Schwellenländer,\n",
            "Nearest to Werte: nächste, Schieberegler, Lastschrift, Reisezeit, Plattformen, Sonderpreise, Umzüge, Röhre,\n",
            "Nearest to 2002: 2004, 2005, 2008, 2003, Mai, Oktober, 2006, April,\n",
            "Nearest to getan: Löcher, wünsche, töten, tun, Hallenbad, Tips, Geld, Antiquitäten,\n",
            "Nearest to Natürlich: Ebenso, genau, Wollen, Jetzt, hier, sogar, Somit, Gelegenheit,\n",
            "Average loss at step 42000: 1.746793\n",
            "Average loss at step 44000: 1.729458\n",
            "Average loss at step 46000: 1.694936\n",
            "Average loss at step 48000: 1.681684\n",
            "Average loss at step 50000: 1.665288\n",
            "Nearest to und: besinnen, oder, Zink, Güterverkehrs, versinkt, Schlampen, Baldo, dachten,\n",
            "Nearest to bei: wegen, Bei, hinter, hielten, Knesset, folgen, Hierin, neben,\n",
            "Nearest to ich: Ich, er, sie, jemand, sagen, mysqld, Bundesregierung, verehrter,\n",
            "Nearest to -: Startseite, Hörner, erarbeiten, Portland, Gelb, vielleicht, virtuelles, 1790,\n",
            "Nearest to wenn: falls, weil, Wahrlich, sieht, braucht, sobald, ob, dann,\n",
            "Nearest to Union: vorsätzlich, Rand, much, strukturieren, Altstadt, Charles, Gefangenschaft, Zusammenstellung,\n",
            "Nearest to die: diese, ihre, Die, keine, deren, unsere, seine, einige,\n",
            "Nearest to Die: Unsere, Eine, Diese, Seine, die, Zur, Frauentag, Alle,\n",
            "Nearest to :: Kay, &#93;, ad, mittelfristig, nachhaltigere, Alt, acht, Ladegeräte,\n",
            "Nearest to alle: sämtliche, Welche, aller, jeweils, Italienischen, allen, Jerusalems, Alle,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Sicherheitsmaßnahmen, Fehlverhalten, schwerwiegenden, benannt, Minderheitenschutz, landet,\n",
            "Nearest to beste: perfekte, ideale, bevorzugte, Entdecke, exzellent, Writer, Spezialität, sonniger,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Unterrichts, vielseitige, Konfigurationsdateien, Jahren, Schwierigkeitsgrade, Panoramablick,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Ceuta, Wünschen, Rahmen, Visualisierung, Fitnessraum, Alp,\n",
            "Nearest to zahlreichen: vielen, modernem, belassen, umliegenden, Klagen, verschiedenen, Vermietung, tropischen,\n",
            "Nearest to erfüllen: spielen, beenden, unterstützen, begleiteten, könne, angibt, halten, dürfen,\n",
            "Nearest to Werte: nächste, Lastschrift, Plattformen, Sonderpreise, Regionen, Reisezeit, Schieberegler, Überweisung,\n",
            "Nearest to 2002: 2004, 2005, 2003, 2006, Oktober, Mai, 2007, 2008,\n",
            "Nearest to getan: tun, wünsche, Löcher, töten, Geld, darum, mitteilen, erstaunlich,\n",
            "Nearest to Natürlich: Ebenso, Außerdem, Was, Weiterhin, hierzu, Leider, Dabei, Somit,\n",
            "Average loss at step 52000: 1.640508\n",
            "Average loss at step 54000: 1.621003\n",
            "Average loss at step 56000: 1.613053\n",
            "Average loss at step 58000: 1.581456\n",
            "Average loss at step 60000: 1.580258\n",
            "Nearest to und: besinnen, oder, Baldo, Zink, Shakespeares, Quotenregelung, versinkt, Schlampen,\n",
            "Nearest to bei: Bei, wegen, anzugeben, Hierin, Knesset, hinter, hielten, folgen,\n",
            "Nearest to ich: Ich, er, jemand, sagen, sie, Bundesregierung, mysqld, man,\n",
            "Nearest to -: Startseite, erarbeiten, vielleicht, Portland, substantielle, 1790, virtuelles, Mitleidenschaft,\n",
            "Nearest to wenn: falls, weil, Wahrlich, ob, indem, sobald, braucht, sieht,\n",
            "Nearest to Union: vorsätzlich, Kommission, Zusammenstellung, Rand, Skipiste, Altstadt, Agenda, Gefangenschaft,\n",
            "Nearest to die: diese, ihre, bestimmte, Die, keine, deren, seine, unsere,\n",
            "Nearest to Die: Unsere, Eine, Seine, Diese, die, Zur, Jede, Frauentag,\n",
            "Nearest to :: Kay, nachhaltigere, Alt, &#93;, ad, mittelfristig, kaufe, Ladegeräte,\n",
            "Nearest to alle: sämtliche, Welche, allen, jeweils, Alle, aller, nur, Italienischen,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Sicherheitsmaßnahmen, Fehlverhalten, schwerwiegenden, benannt, Imam, Minderheitenschutz,\n",
            "Nearest to beste: perfekte, ideale, Writer, Entdecke, exzellent, bevorzugte, richtige, größte,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Unterrichts, Kind, vielseitige, Panoramablick, Konfigurationsdateien, Jahren,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Wünschen, angenehmen, Rahmen, Ceuta, Gegner, Aufschwung,\n",
            "Nearest to zahlreichen: vielen, umliegenden, verschiedenen, Klagen, modernem, belassen, Bars, Theatern,\n",
            "Nearest to erfüllen: beenden, unterstützen, spielen, angibt, Rechenschaft, begleiteten, halten, dürfen,\n",
            "Nearest to Werte: Lastschrift, Regionen, Reisezeit, Plattformen, Knowledge, nächste, Wirksamkeit, Verwendung,\n",
            "Nearest to 2002: 2004, 2005, Mai, 2003, 2006, November, August, 1968,\n",
            "Nearest to getan: tun, wünsche, Löcher, töten, Geld, erstaunlich, darum, denn,\n",
            "Nearest to Natürlich: Ebenso, Außerdem, Dabei, Weiterhin, Leider, Was, hierzu, Heute,\n",
            "Average loss at step 62000: 1.565125\n",
            "Average loss at step 64000: 1.552753\n",
            "Average loss at step 66000: 1.530380\n",
            "Average loss at step 68000: 1.519418\n",
            "Average loss at step 70000: 1.503824\n",
            "Nearest to und: besinnen, oder, Zink, Baldo, versinkt, Schlampen, Shakespeares, Quotenregelung,\n",
            "Nearest to bei: Bei, hinter, Knesset, hielten, folgen, anzugeben, Hierin, wegen,\n",
            "Nearest to ich: Ich, er, jemand, sie, sagen, man, Netzbetreiber, vorkommen,\n",
            "Nearest to -: Startseite, vielleicht, substantielle, erarbeiten, absolutely, –, Mitleidenschaft, Hörner,\n",
            "Nearest to wenn: falls, weil, dann, sobald, ob, Wahrlich, sodass, bevor,\n",
            "Nearest to Union: vorsätzlich, Kommission, Skipiste, Charles, Altstadt, Type, Pension, nächst,\n",
            "Nearest to die: ihre, diese, bestimmte, Die, keine, deren, seine, unsere,\n",
            "Nearest to Die: Unsere, Eine, Seine, Diese, die, Zur, Jede, Frauentag,\n",
            "Nearest to :: Kay, nachhaltigere, —, Alt, Ladegeräte, », kaufe, ad,\n",
            "Nearest to alle: sämtliche, allen, Welche, Alle, Jerusalems, aller, jeweils, nur,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, Fehlverhalten, Sicherheitsmaßnahmen, schwerwiegenden, benannt, Minderheitenschutz, Imam,\n",
            "Nearest to beste: perfekte, ideale, richtige, bevorzugte, Entdecke, Writer, exzellent, automatische,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Berücksichtigung, Kind, Leitung, vielseitige, Konfigurationsdateien, Unterrichts,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Wünschen, angenehmen, Ausflug, Weg, Aufschwung, Traum,\n",
            "Nearest to zahlreichen: vielen, verschiedenen, umliegenden, Klagen, Bars, edlen, diversen, Theatern,\n",
            "Nearest to erfüllen: unterstützen, beenden, spielen, schützen, behalten, übernehmen, Rechenschaft, angibt,\n",
            "Nearest to Werte: Lastschrift, Regionen, Knowledge, Plattformen, Verwendung, Reisezeit, Versorgungsunternehmen, Rechnung,\n",
            "Nearest to 2002: 2004, 2005, 2003, 2006, Mai, November, August, 2007,\n",
            "Nearest to getan: tun, wünsche, erstaunlich, töten, darum, Geld, mitteilen, Straßenlärm,\n",
            "Nearest to Natürlich: Ebenso, Außerdem, Weiterhin, Dabei, Leider, Somit, Was, Allerdings,\n",
            "Average loss at step 72000: 1.499181\n",
            "Average loss at step 74000: 1.474416\n",
            "Average loss at step 76000: 1.480925\n",
            "Average loss at step 78000: 1.464711\n",
            "Average loss at step 80000: 1.454757\n",
            "Nearest to und: besinnen, oder, Baldo, Zink, Shakespeares, Quotenregelung, versinkt, funktionsfähigen,\n",
            "Nearest to bei: Bei, hinter, anzugeben, Hierin, wegen, Referenzwert, Knesset, trotz,\n",
            "Nearest to ich: Ich, er, jemand, man, sie, Netzbetreiber, vorkommen, verweisen,\n",
            "Nearest to -: Startseite, erarbeiten, substantielle, Mitleidenschaft, vielleicht, 143, Hörner, wohl,\n",
            "Nearest to wenn: falls, weil, sobald, sodass, ob, bevor, indem, Wahrlich,\n",
            "Nearest to Union: Kommission, Agenda, Altstadt, Koordination, Type, Skipiste, strukturieren, vorsätzlich,\n",
            "Nearest to die: diese, ihre, Die, bestimmte, deren, seine, jede, keine,\n",
            "Nearest to Die: Unsere, Seine, Eine, Diese, die, Jede, Welche, Zur,\n",
            "Nearest to :: Kay, —, nachhaltigere, &#93;, », ., Alt, ##STAR##,\n",
            "Nearest to alle: sämtliche, allen, Alle, Welche, Jerusalems, aller, jeweils, nur,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, schwerwiegenden, Fehlverhalten, Sicherheitsmaßnahmen, benannt, Minderheitenschutz, Imam,\n",
            "Nearest to beste: perfekte, ideale, richtige, bevorzugte, führende, exzellent, automatische, optimale,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Kind, http, Leitung, vielseitige, Leistungsspektrum, ihnen,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Wünschen, angenehmen, Sommerurlaub, Traum, Aufschwung, Fitnessraum,\n",
            "Nearest to zahlreichen: vielen, umliegenden, verschiedenen, Klagen, kleineren, Theatern, belassen, edlen,\n",
            "Nearest to erfüllen: beenden, schützen, unterstützen, spielen, behalten, Rechenschaft, halten, erklären,\n",
            "Nearest to Werte: Plattformen, Knowledge, Lastschrift, Verwendung, Rechnung, Reisezeit, Pakete, Eintauchen,\n",
            "Nearest to 2002: 2004, 2005, 2003, 2006, Mai, November, August, 1986,\n",
            "Nearest to getan: tun, wünsche, erstaunlich, darum, geheizt, gesagt, töten, Geld,\n",
            "Nearest to Natürlich: Ebenso, Weiterhin, Außerdem, Dabei, Leider, Was, Deshalb, Allerdings,\n",
            "Average loss at step 82000: 1.460487\n",
            "Average loss at step 84000: 1.432748\n",
            "Average loss at step 86000: 1.428075\n",
            "Average loss at step 88000: 1.430211\n",
            "Average loss at step 90000: 1.416314\n",
            "Nearest to und: besinnen, Baldo, oder, Zink, Shakespeares, funktionsfähigen, Schlampen, Quotenregelung,\n",
            "Nearest to bei: Bei, anzugeben, wegen, hinter, Hierin, Referenzwert, trotz, elementarer,\n",
            "Nearest to ich: Ich, er, jemand, man, sie, vorkommen, verweisen, Netzbetreiber,\n",
            "Nearest to -: Startseite, erarbeiten, –, Hörner, substantielle, wohl, Mitleidenschaft, funktionierte,\n",
            "Nearest to wenn: falls, weil, sobald, indem, bevor, wobei, sodass, Wahrlich,\n",
            "Nearest to Union: Kommission, Type, vorsätzlich, Agenda, strukturieren, Pension, nächst, Koordination,\n",
            "Nearest to die: diese, ihre, bestimmte, Die, seine, deren, unsere, dieselbe,\n",
            "Nearest to Die: Unsere, Seine, Diese, Eine, die, Jede, Welche, Meine,\n",
            "Nearest to :: &#93;, Kay, —, nachhaltigere, », ., ##STAR##, sozialdemokratische,\n",
            "Nearest to alle: sämtliche, allen, Alle, Welche, jeweils, nur, Jerusalems, aller,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, schwerwiegenden, Fehlverhalten, benannt, Sicherheitsmaßnahmen, Minderheitenschutz, MEZ,\n",
            "Nearest to beste: perfekte, ideale, richtige, optimale, bevorzugte, führende, exzellent, automatische,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Leistungsspektrum, Leitung, http, vielseitige, Kind, Konfigurationsdateien,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Wünschen, angenehmen, Sommerurlaub, Aufschwung, Ausflug, Fitnessraum,\n",
            "Nearest to zahlreichen: vielen, umliegenden, verschiedenen, Klagen, Theatern, archäologischen, kleineren, edlen,\n",
            "Nearest to erfüllen: beenden, schützen, unterstützen, behalten, spielen, erklären, informieren, beobachten,\n",
            "Nearest to Werte: Regionen, Plattformen, Knowledge, Reisezeit, Pakete, Rechnung, Lastschrift, Devisenmarkt,\n",
            "Nearest to 2002: 2004, 2005, 2006, 2003, Mai, August, November, 1986,\n",
            "Nearest to getan: tun, wünsche, gesagt, Geld, darum, erstaunlich, alles, töten,\n",
            "Nearest to Natürlich: Ebenso, Weiterhin, Außerdem, Leider, Was, Allerdings, Dabei, Somit,\n",
            "Average loss at step 92000: 1.410688\n",
            "Average loss at step 94000: 1.409906\n",
            "Average loss at step 96000: 1.400469\n",
            "Average loss at step 98000: 1.394817\n",
            "Average loss at step 100000: 1.386351\n",
            "Nearest to und: besinnen, Baldo, oder, Zink, Shakespeares, funktionsfähigen, sowie, Quotenregelung,\n",
            "Nearest to bei: Bei, anzugeben, hinter, Hierin, wegen, Referenzwert, trotz, hielten,\n",
            "Nearest to ich: Ich, er, jemand, man, vorkommen, sie, Netzbetreiber, sagen,\n",
            "Nearest to -: Startseite, substantielle, wohl, –, erarbeiten, Aufsichtsratsmitglieder, Mitleidenschaft, Hörner,\n",
            "Nearest to wenn: falls, weil, sobald, solange, sodass, obwohl, indem, wohin,\n",
            "Nearest to Union: Kommission, Type, vorsätzlich, Agenda, strukturieren, Pension, Zentralbank, nächst,\n",
            "Nearest to die: diese, ihre, bestimmte, Die, deren, unsere, seine, dieselbe,\n",
            "Nearest to Die: Seine, Unsere, Diese, Eine, die, Jede, Keine, Welche,\n",
            "Nearest to :: &#93;, —, Kay, », nachhaltigere, ##STAR##, sozialdemokratische, .,\n",
            "Nearest to alle: sämtliche, allen, Alle, jeweils, Welche, nur, einzelne, Metaphern,\n",
            "Nearest to Berichterstatterin: Warenverkehr, halfen, schwerwiegenden, Fehlverhalten, benannt, Sicherheitsmaßnahmen, MEZ, Minderheitenschutz,\n",
            "Nearest to beste: perfekte, ideale, richtige, optimale, führende, größte, bevorzugte, exzellent,\n",
            "Nearest to anderem: Umständen, Denkmalschutz, Leitung, Leistungsspektrum, Berücksichtigung, vielseitige, ihnen, Kind,\n",
            "Nearest to Aufenthalt: Urlaub, Besuch, Sommerurlaub, Aufschwung, angenehmen, Wünschen, Drink, Fitnessraum,\n",
            "Nearest to zahlreichen: vielen, umliegenden, Klagen, verschiedenen, edlen, Theatern, archäologischen, italienischen,\n",
            "Nearest to erfüllen: beenden, schützen, verarbeiten, unterstützen, behalten, reduzieren, erklären, spielen,\n",
            "Nearest to Werte: Regionen, Plattformen, Länder, Pakete, Knowledge, Rechnung, Wirksamkeit, Nutzung,\n",
            "Nearest to 2002: 2004, 2006, 2005, 2003, 2001, Mai, November, 1986,\n",
            "Nearest to getan: tun, wünsche, darum, gesagt, verkündet, Geld, töten, Glück,\n",
            "Nearest to Natürlich: Weiterhin, Ebenso, Außerdem, Allerdings, Deshalb, Dabei, Leider, Heute,\n",
            "Initialized\n",
            "Average loss at step 2000: 4.275495\n",
            "Average loss at step 4000: 2.818988\n",
            "Average loss at step 6000: 2.494669\n",
            "Average loss at step 8000: 2.316638\n",
            "Average loss at step 10000: 2.209912\n",
            "Nearest to and: GL, places, machining, eject, venue, pillows, Florian, Fontainebleau,\n",
            "Nearest to would: should, may, paid, will, signatories, shall, could, requirements,\n",
            "Nearest to can: will, may, must, could, should, cannot, gives, to,\n",
            "Nearest to your: the, our, its, 1979, my, their, any, this,\n",
            "Nearest to about: word, Kabila, after, Are, Canaries, Reuters, ply, again,\n",
            "Nearest to no: great, 2009, Lindh, een, sixty, dash, only, cargo,\n",
            "Nearest to .: ?, Clubs, MILAN, lago, Pinnacle, !, deadlines, sounding,\n",
            "Nearest to you: You, we, I, riches, something, ye, advanced, please,\n",
            "Nearest to European: Seville, BNC, book, trees, Madrid, flag, b, museum,\n",
            "Nearest to out: probably, tradition, ceiling, total, taken, sense, Burmese, protection,\n",
            "Nearest to regulations: active, name, barbecue, stored, departure, countries, Deposit, Kingdom,\n",
            "Nearest to property: week, final, appropriate, doubt, path, review, highest, charge,\n",
            "Nearest to response: stations, lobby, look, maximum, U, Group, High, sensitive,\n",
            "Nearest to priority: carry, tomorrow, line, mixer, reputation, 179, cabin, shockingly,\n",
            "Nearest to gives: can, liked, entrada, simply, manually, Derivatives, boldness, calmed,\n",
            "Nearest to December: cozy, 17.00, Berlin, Karolina, Durham, Yes, Chairman, Likewise,\n",
            "Nearest to categories: Munich, Islands, stored, 600, US, Assad, date, Vulcan,\n",
            "Nearest to setting: simple, unique, responsibility, n, art, fruits, example, winter,\n",
            "Nearest to latest: current, kick, visit, 1799, cathedral, thoroughly, calendar, Cluny,\n",
            "Nearest to external: snacks, subject, Next, years, migrate, burning, progress, 13,\n",
            "Average loss at step 12000: 2.117605\n",
            "Average loss at step 14000: 2.060637\n",
            "Average loss at step 16000: 2.007933\n",
            "Average loss at step 18000: 1.948840\n",
            "Average loss at step 20000: 1.906113\n",
            "Nearest to and: Fontainebleau, Florian, GL, eject, Lastly, decorators, 880, Tone,\n",
            "Nearest to would: should, may, Would, could, shall, must, will, &apos;ll,\n",
            "Nearest to can: may, will, could, should, must, &apos;ll, cannot, gives,\n",
            "Nearest to your: my, our, his, the, their, its, Your, any,\n",
            "Nearest to about: Reuters, regarding, word, after, than, staged, Once, supports,\n",
            "Nearest to no: Lindh, sixty, earnestly, Gérard, a, cone, easier, klein,\n",
            "Nearest to .: ?, Clubs, MILAN, GDS, Pinnacle, deadlines, sounding, lago,\n",
            "Nearest to you: You, we, ye, I, they, riches, listings, something,\n",
            "Nearest to European: BNC, ETS, flag, Madrid, 182, Seville, Flensburg, ABAP,\n",
            "Nearest to out: probably, pipe, Burmese, sense, rewards, Results, ceiling, sin,\n",
            "Nearest to regulations: active, barbecue, visitors, countries, gems, questionable, departure, games,\n",
            "Nearest to property: final, Framework, module, doubt, week, heal, author, 13th,\n",
            "Nearest to response: lobby, Treat, Relax, lead, manufacturer, bays, weekly, cash,\n",
            "Nearest to priority: tomorrow, benefit, 179, cabin, GNOME, reputation, throw, finest,\n",
            "Nearest to gives: can, liked, Are, awaits, lets, Once, Download, Derivatives,\n",
            "Nearest to December: March, Durham, 05, July, October, Tampa, Poland, Karolina,\n",
            "Nearest to categories: Munich, stored, reminiscent, date, independence, examples, intention, presence,\n",
            "Nearest to setting: simple, responsibility, unique, seafront, firewall, offering, column, thoughtful,\n",
            "Nearest to latest: kick, current, calendar, 1799, Cluny, thoroughly, field, owner,\n",
            "Nearest to external: snacks, angel, burning, wilderness, progress, years, replaced, replace,\n",
            "Average loss at step 22000: 1.878093\n",
            "Average loss at step 24000: 1.851656\n",
            "Average loss at step 26000: 1.812632\n",
            "Average loss at step 28000: 1.780041\n",
            "Average loss at step 30000: 1.761838\n",
            "Nearest to and: Florian, Lastly, GL, Fontainebleau, eject, flea, complacency, selling,\n",
            "Nearest to would: should, Would, may, could, must, will, shall, might,\n",
            "Nearest to can: should, may, could, must, &apos;ll, will, cannot, Can,\n",
            "Nearest to your: my, Your, our, their, his, its, the, any,\n",
            "Nearest to about: Reuters, than, regarding, ply, Once, 309, inability, staged,\n",
            "Nearest to no: Lindh, cone, earnestly, civilizations, easier, any, a, Gérard,\n",
            "Nearest to .: ?, Clubs, Automatically, Pinnacle, MILAN, GDS, sounding, lago,\n",
            "Nearest to you: You, we, ye, they, I, listings, We, riches,\n",
            "Nearest to European: BNC, Madrid, nineteenth, flag, Seville, ETS, south, Hockey,\n",
            "Nearest to out: pipe, probably, sin, sense, Realizing, efficiently, rewards, up,\n",
            "Nearest to regulations: active, ordinances, visitors, barbecue, mandarins, potatoes, questionable, games,\n",
            "Nearest to property: Framework, perspective, module, final, doubt, author, heal, Vector,\n",
            "Nearest to response: Treat, manufacturer, Relax, lobby, bays, lead, recover, weekly,\n",
            "Nearest to priority: tomorrow, reputation, 179, GNOME, cabin, flair, benefit, finest,\n",
            "Nearest to gives: Are, can, lets, awaits, offers, liked, allows, Once,\n",
            "Nearest to December: March, July, April, October, 05, September, June, 59,\n",
            "Nearest to categories: reminiscent, spread, stored, date, Munich, Colorado, reducing, tools,\n",
            "Nearest to setting: firewall, unique, simple, seafront, shame, intimate, characteristic, responsibility,\n",
            "Nearest to latest: calendar, current, kick, quickest, meantime, 1970s, Cluny, announcement,\n",
            "Nearest to external: angel, snacks, ace, licence, burning, online, progress, industrial,\n",
            "Average loss at step 32000: 1.745115\n",
            "Average loss at step 34000: 1.718280\n",
            "Average loss at step 36000: 1.698330\n",
            "Average loss at step 38000: 1.677179\n",
            "Average loss at step 40000: 1.663252\n",
            "Nearest to and: Florian, GL, Lastly, Fontainebleau, flea, Comedy, complacency, reservas,\n",
            "Nearest to would: Would, should, may, could, &apos;d, might, must, will,\n",
            "Nearest to can: should, must, may, could, &apos;ll, cannot, will, Can,\n",
            "Nearest to your: Your, my, their, our, his, the, its, bacterial,\n",
            "Nearest to about: regarding, Reuters, 309, ply, than, Rodney, coke, SEK,\n",
            "Nearest to no: any, Lindh, easier, earnestly, cone, a, No, civilizations,\n",
            "Nearest to .: ?, Automatically, Clubs, Pinnacle, sounding, MILAN, Return, !,\n",
            "Nearest to you: You, we, ye, they, listings, I, We, Guests,\n",
            "Nearest to European: Soviet, Madrid, BNC, Seville, museum, trumped, south, flag,\n",
            "Nearest to out: pipe, up, Realizing, efficiently, rewards, sense, probably, frogs,\n",
            "Nearest to regulations: ordinances, visitors, barbecue, active, tourists, friends, gems, mandarins,\n",
            "Nearest to property: hotel, perspective, Framework, module, final, doubt, conclusion, alias,\n",
            "Nearest to response: Treat, lead, extend, recover, unwind, preference, winnings, swim,\n",
            "Nearest to priority: reputation, tomorrow, cabin, flair, benefit, 179, roundabout, goal,\n",
            "Nearest to gives: Are, lets, awaits, offers, can, allows, welcomes, overlooks,\n",
            "Nearest to December: March, April, July, October, September, June, 2007, November,\n",
            "Nearest to categories: date, spread, donation, Munich, reminiscent, proposals, days, patents,\n",
            "Nearest to setting: unique, firewall, simple, intimate, sight, tankers, cafe, shame,\n",
            "Nearest to latest: current, calendar, 1799, announcement, certificate, meantime, kick, quickest,\n",
            "Nearest to external: angel, industrial, cosmological, online, ace, snacks, burning, licence,\n",
            "Average loss at step 42000: 1.642807\n",
            "Average loss at step 44000: 1.625437\n",
            "Average loss at step 46000: 1.606461\n",
            "Average loss at step 48000: 1.611256\n",
            "Average loss at step 50000: 1.591373\n",
            "Nearest to and: Florian, Lastly, almonds, GL, murderous, but, or, complacency,\n",
            "Nearest to would: Would, should, may, might, &apos;d, will, could, must,\n",
            "Nearest to can: may, should, &apos;ll, must, cannot, could, will, Can,\n",
            "Nearest to your: Your, my, their, our, his, its, bacterial, GENERAL,\n",
            "Nearest to about: regarding, concerning, Reuters, 309, ply, coke, palatable, sunbathe,\n",
            "Nearest to no: No, any, Lindh, easier, a, cone, earnestly, formatting,\n",
            "Nearest to .: ?, Pinnacle, Automatically, Clubs, MILAN, niches, Return, sounding,\n",
            "Nearest to you: You, we, ye, they, I, listings, Guests, guests,\n",
            "Nearest to European: Soviet, BNC, trumped, injection, conveyance, Madrid, museum, Seville,\n",
            "Nearest to out: up, pipe, Realizing, rewards, frogs, sense, efficiently, Lastovo,\n",
            "Nearest to regulations: active, visitors, ordinances, sport, mandarins, Reservations, flowers, souls,\n",
            "Nearest to property: hotel, perspective, final, Framework, alias, Vector, expanding, alliance,\n",
            "Nearest to response: Treat, lead, extend, recover, raise, winnings, prove, preference,\n",
            "Nearest to priority: roundabout, tomorrow, baptized, cabin, goal, reputation, paddle, 179,\n",
            "Nearest to gives: Are, lets, awaits, offers, allows, brings, ensures, can,\n",
            "Nearest to December: March, April, July, October, September, June, November, May,\n",
            "Nearest to categories: independence, airports, donation, date, reminiscent, days, patents, spread,\n",
            "Nearest to setting: tankers, unique, firewall, intimate, seafront, location, shame, simple,\n",
            "Nearest to latest: current, 1799, calendar, certificate, announcement, bugs, Arcade, firmware,\n",
            "Nearest to external: industrial, angel, emergency, ace, cosmological, snacks, additional, extended,\n",
            "Average loss at step 52000: 1.578495\n",
            "Average loss at step 54000: 1.553770\n",
            "Average loss at step 56000: 1.546509\n",
            "Average loss at step 58000: 1.541772\n",
            "Average loss at step 60000: 1.527534\n",
            "Nearest to and: Florian, almonds, Lastly, or, complacency, hygienic, but, flea,\n",
            "Nearest to would: Would, should, &apos;d, may, might, could, will, Will,\n",
            "Nearest to can: should, may, cannot, &apos;ll, could, must, Can, will,\n",
            "Nearest to your: Your, my, their, our, his, her, GENERAL, its,\n",
            "Nearest to about: regarding, concerning, 309, ply, than, Rodney, palatable, Reuters,\n",
            "Nearest to no: No, any, easier, Lindh, formatting, cone, Gérard, iPhoto,\n",
            "Nearest to .: ?, Automatically, Pinnacle, sounding, !, MILAN, Clubs, niches,\n",
            "Nearest to you: You, we, they, ye, I, guests, Guests, anyone,\n",
            "Nearest to European: Soviet, trumped, glorious, BNC, southern, Madrid, segment, farms,\n",
            "Nearest to out: up, pipe, frogs, Realizing, psychology, back, rewards, hard,\n",
            "Nearest to regulations: active, visitors, sport, mandarins, brothers, flowers, ordinances, grandeur,\n",
            "Nearest to property: hotel, alliance, perspective, Vector, doubt, conclusion, alias, expanding,\n",
            "Nearest to response: Treat, prove, lead, raise, extend, belonging, preference, recover,\n",
            "Nearest to priority: roundabout, reputation, goal, cabin, tomorrow, baptized, 179, landlord,\n",
            "Nearest to gives: lets, Are, offers, awaits, brings, allows, provides, ensures,\n",
            "Nearest to December: April, March, July, October, November, September, June, May,\n",
            "Nearest to categories: days, examples, chances, characters, independence, intention, date, levels,\n",
            "Nearest to setting: characteristic, tankers, shame, seafront, firewall, intimate, unique, simple,\n",
            "Nearest to latest: current, integrated, Arcade, newest, bugs, trial, certificate, announcement,\n",
            "Nearest to external: industrial, emergency, HTML, cosmological, angel, extended, LTSP, additional,\n",
            "Average loss at step 62000: 1.520769\n",
            "Average loss at step 64000: 1.518846\n",
            "Average loss at step 66000: 1.492472\n",
            "Average loss at step 68000: 1.501296\n",
            "Average loss at step 70000: 1.485396\n",
            "Nearest to and: complacency, Florian, or, almonds, Lastly, but, eject, while,\n",
            "Nearest to would: Would, should, may, &apos;d, might, will, Will, could,\n",
            "Nearest to can: should, cannot, may, could, &apos;ll, Can, must, lets,\n",
            "Nearest to your: Your, my, their, his, our, her, its, bacterial,\n",
            "Nearest to about: regarding, concerning, 309, Rodney, ply, palatable, than, approximately,\n",
            "Nearest to no: No, any, Lindh, easier, greedy, plenty, formatting, iPhoto,\n",
            "Nearest to .: ?, Automatically, Pinnacle, !, MILAN, sounding, Return, modernise,\n",
            "Nearest to you: You, we, they, ye, guests, I, anyone, We,\n",
            "Nearest to European: Soviet, trumped, conveyance, BNC, new, British, southern, economics,\n",
            "Nearest to out: up, pipe, back, Realizing, hard, frogs, psychology, ceiling,\n",
            "Nearest to regulations: Reservations, efforts, tourists, voices, ordinances, active, qualities, souls,\n",
            "Nearest to property: hotel, perspective, alliance, conclusion, alias, author, Vector, appearance,\n",
            "Nearest to response: Treat, fill, prove, belonging, lead, preference, extend, raise,\n",
            "Nearest to priority: roundabout, goal, reputation, tomorrow, cabin, 179, landlord, perception,\n",
            "Nearest to gives: brings, awaits, offers, Are, lets, allows, keeps, enjoys,\n",
            "Nearest to December: April, March, July, October, November, September, June, May,\n",
            "Nearest to categories: levels, days, examples, sizes, chances, spread, teams, taxes,\n",
            "Nearest to setting: shame, tankers, characteristic, appointment, wake, sight, ambience, seafront,\n",
            "Nearest to latest: current, newest, integrated, Arcade, trends, firmware, locale, certificate,\n",
            "Nearest to external: emergency, industrial, HTML, extended, automation, cosmological, multimedia, functions,\n",
            "Average loss at step 72000: 1.484868\n",
            "Average loss at step 74000: 1.472304\n",
            "Average loss at step 76000: 1.462059\n",
            "Average loss at step 78000: 1.459139\n",
            "Average loss at step 80000: 1.455463\n",
            "Nearest to and: but, reservas, Lastly, or, almonds, complacency, while, eject,\n",
            "Nearest to would: Would, should, &apos;d, might, may, will, Will, could,\n",
            "Nearest to can: should, could, cannot, Can, &apos;ll, must, may, will,\n",
            "Nearest to your: Your, my, their, his, our, her, My, GENERAL,\n",
            "Nearest to about: concerning, regarding, 309, Rodney, ply, SEK, approximately, palatable,\n",
            "Nearest to no: No, any, plenty, Lindh, lots, greedy, iPhoto, scramble,\n",
            "Nearest to .: ?, Automatically, !, Pinnacle, —, MILAN, sounding, •,\n",
            "Nearest to you: You, we, they, ye, guests, I, Guests, visitors,\n",
            "Nearest to European: Soviet, British, new, Pakistani, Zeeland, farms, economics, trumped,\n",
            "Nearest to out: up, pipe, back, hard, Realizing, frogs, off, efficiently,\n",
            "Nearest to regulations: Reservations, flowers, efforts, tourists, rules, brothers, active, sport,\n",
            "Nearest to property: hotel, perspective, conclusion, author, alliance, residences, objective, appearance,\n",
            "Nearest to response: Treat, prove, belonging, fill, extend, raise, harness, detect,\n",
            "Nearest to priority: roundabout, goal, cabin, perception, tomorrow, flair, coziness, shockingly,\n",
            "Nearest to gives: offers, brings, awaits, lets, allows, Are, keeps, enjoys,\n",
            "Nearest to December: March, April, July, October, November, September, June, May,\n",
            "Nearest to categories: levels, sizes, examples, sorts, days, kinds, descriptions, phases,\n",
            "Nearest to setting: wake, shame, tankers, seafront, appointment, sight, environment, ambience,\n",
            "Nearest to latest: current, newest, Arcade, trends, 6815, integrated, correct, announcement,\n",
            "Nearest to external: emergency, industrial, HTML, automation, functions, automated, extended, multimedia,\n",
            "Average loss at step 82000: 1.445987\n",
            "Average loss at step 84000: 1.444834\n",
            "Average loss at step 86000: 1.425260\n",
            "Average loss at step 88000: 1.422455\n",
            "Average loss at step 90000: 1.419766\n",
            "Nearest to and: but, almonds, &amp;, complacency, Lastly, reservas, pastry, or,\n",
            "Nearest to would: Would, should, &apos;d, might, may, will, Will, could,\n",
            "Nearest to can: should, could, &apos;ll, cannot, Can, may, must, lets,\n",
            "Nearest to your: Your, my, their, our, his, her, My, its,\n",
            "Nearest to about: concerning, regarding, 309, palatable, ply, approximately, Rodney, SEK,\n",
            "Nearest to no: No, any, plenty, iPhoto, Lindh, lots, winters, Sparta,\n",
            "Nearest to .: ?, Automatically, !, Pinnacle, —, sounding, MILAN, •,\n",
            "Nearest to you: You, we, they, ye, I, guests, Guests, visitors,\n",
            "Nearest to European: Soviet, British, Jewish, Pakistani, new, economics, Europe, Zeeland,\n",
            "Nearest to out: up, back, pipe, efficiently, hard, Realizing, off, psychology,\n",
            "Nearest to regulations: Reservations, rules, flowers, tourists, efforts, preparing, active, schools,\n",
            "Nearest to property: hotel, conclusion, residences, author, perspective, Addinsoft, alliance, anthem,\n",
            "Nearest to response: Treat, prove, extend, fill, maximize, check, detect, test,\n",
            "Nearest to priority: roundabout, goal, cabin, tomorrow, flair, perception, curious, coziness,\n",
            "Nearest to gives: brings, lets, allows, offers, awaits, enables, keeps, Are,\n",
            "Nearest to December: March, April, July, October, November, June, September, May,\n",
            "Nearest to categories: levels, sizes, sorts, examples, descriptions, kinds, versions, days,\n",
            "Nearest to setting: wake, shame, courtyard, sight, environment, seafront, characteristic, appointment,\n",
            "Nearest to latest: current, newest, Arcade, trends, integrated, 6815, correct, actual,\n",
            "Nearest to external: emergency, HTML, industrial, functions, automation, input, LTSP, extended,\n",
            "Average loss at step 92000: 1.424198\n",
            "Average loss at step 94000: 1.411014\n",
            "Average loss at step 96000: 1.408497\n",
            "Average loss at step 98000: 1.396194\n",
            "Average loss at step 100000: 1.393619\n",
            "Nearest to and: nor, but, &amp;, complacency, or, Lastly, reservas, hygienic,\n",
            "Nearest to would: Would, should, &apos;d, might, may, will, could, Will,\n",
            "Nearest to can: should, could, may, cannot, Can, must, &apos;ll, lets,\n",
            "Nearest to your: Your, my, their, our, his, My, her, its,\n",
            "Nearest to about: regarding, concerning, 309, palatable, Rodney, ply, timetable, Reuters,\n",
            "Nearest to no: No, any, Gérard, plenty, Lindh, greedy, lots, Sparta,\n",
            "Nearest to .: ?, !, Automatically, •, —, sounding, Pinnacle, MILAN,\n",
            "Nearest to you: You, we, they, ye, guests, I, Guests, travelers,\n",
            "Nearest to European: Soviet, British, Jewish, economics, Europe, Latin, subterranean, democratic,\n",
            "Nearest to out: up, pipe, back, off, hard, Realizing, psychology, efficiently,\n",
            "Nearest to regulations: Reservations, rules, flowers, resources, actions, efforts, brothers, commandments,\n",
            "Nearest to property: hotel, perspective, residences, Addinsoft, author, conclusion, obligations, skyline,\n",
            "Nearest to response: Treat, prove, raise, order, test, kick, lead, fill,\n",
            "Nearest to priority: roundabout, goal, brief, landlord, tomorrow, reputation, flair, cabin,\n",
            "Nearest to gives: offers, brings, lets, allows, awaits, keeps, ensures, Are,\n",
            "Nearest to December: March, April, July, October, November, June, September, May,\n",
            "Nearest to categories: sizes, levels, versions, days, sorts, examples, ways, statements,\n",
            "Nearest to setting: wake, environment, ambience, sight, appointment, shame, courtyard, location,\n",
            "Nearest to latest: current, newest, Arcade, correct, integrated, trends, relevant, actual,\n",
            "Nearest to external: emergency, HTML, industrial, functions, automation, LTSP, optional, input,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2SFUW5Cgmsi",
        "colab_type": "text"
      },
      "source": [
        "### Flipping the Input Data\n",
        "Changin the order of the sentence of the target language improves the performance of NMT systems. Because when reversed, it helps the NMT system to establish a strong connection as the last word of the source language and the last word of the target language will be closest to each other. DON'T RUN THIS MULTIPLE TIMES as running two times gives original.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m44eW99grFn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "ac98c456-ec22-49b3-9ed3-51f1bb1220bb"
      },
      "source": [
        "## Reverse the Germen sentences\n",
        "# Remember reversing the source sentence gives better performance\n",
        "# DON'T RUN THIS MULTIPLE TIMES as running two times gives original\n",
        "train_inputs = np.fliplr(train_inputs)\n",
        "test_inputs = np.fliplr(test_inputs)\n",
        "\n",
        "print('Training and Test source data after flipping ')\n",
        "print('\\t',[src_reverse_dictionary[w] for w in train_inputs[0,:].tolist()])\n",
        "print('\\t',[tgt_reverse_dictionary[w] for w in test_inputs[0,:].tolist()])\n",
        "print()\n",
        "print('\\t',[src_reverse_dictionary[w] for w in train_inputs[10,:].tolist()])\n",
        "print('\\t',[tgt_reverse_dictionary[w] for w in test_inputs[10,:].tolist()])\n",
        "\n",
        "print()\n",
        "print('\\nTesting data after flipping')\n",
        "print('\\t',[src_reverse_dictionary[w] for w in test_inputs[0,:].tolist()])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training and Test source data after flipping \n",
            "\t ['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '.', 'können', 'nutzen', 'QuarkXPress', 'mit', 'zusammen', 'besten', 'am', '3', 'Suite', 'Creative', 'und', '2', 'Suite', 'Creative', 'Sie', 'wie', ',', 'Sie', 'erfahren', 'Hier', '<s>']\n",
            "\t ['tray', 'road', 'mistakes', 'of', 'expect', 'a', 'tabled', 'with', 'and', 'the', 'posts', 'useful', 'out', 'waiting', 'wounded', 'a', 'drinks', 'been', 'stand', '26th', 'and', 'senior', 'personal', ',', 'difficulties', 'qualifications', 'an', 'rather', 'road', 'rewriting', 'and', 'road', 'unsustainable', 'the', '2007', 'road', 'wounded', 'not', 'throughout', 'amendment', '<s>']\n",
            "\n",
            "\t ['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '.', ')', 'Import', '##AT##-##AT##', 'PSD', '&gt;', 'Fenster', '(', 'Import', '##AT##-##AT##', 'PSD', 'Palette', 'die', 'Sie', 'öffnen', ',', 'können', 'zu', 'nutzen', 'Dateien', '##AT##-##AT##', 'PSD', 'von', 'Funktionen', 'speziellen', 'die', 'Um', '<s>']\n",
            "\t ['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', ',', '<unk>', 'and', 'important', '<unk>', 'important', 'the', '<unk>', '<unk>', 'the', 'CC', 'revolution', 'are', 'proposals', 'of', 'to', 'not', 'With', 'Overall', 'countries', 'more', '<s>']\n",
            "\n",
            "\n",
            "Testing data after flipping\n",
            "\t ['Creative', '®', 'Adobe', 'der', 'Anwendungen', 'von', 'unabhängig', 'das', 'und', ',', 'bedienen', 'Medien', 'alle', 'inzwischen', 'QuarkXPress', 'von', 'Anwender', 'können', '\\xad', 'CSS', 'und', 'HTML', 'Dank', '.', 'zuvor', 'jemals', 'als', 'besser', '®', 'Illustrator', 'und', '®', 'Photoshop', ',', '8', '®', 'QuarkXPress', 'sich', 'verstehen', 'Heute', '<s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ejk7LpYgyba",
        "colab_type": "text"
      },
      "source": [
        "## Data Generations for MT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LorGmiVAg4uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = embedding_size\n",
        "\n",
        "class DataGeneratorMT(object):\n",
        "    \n",
        "    def __init__(self,batch_size,num_unroll,is_source, is_train):\n",
        "        # Number of data points in a batch\n",
        "        self._batch_size = batch_size\n",
        "        # Number of unrollings\n",
        "        self._num_unroll = num_unroll\n",
        "        # Cursors for each element in batch\n",
        "        self._cursor = [0 for offset in range(self._batch_size)]\n",
        "        \n",
        "        # Loading the learnt word embeddings\n",
        "        self._src_word_embeddings = np.load('de-embeddings.npy')\n",
        "        self._tgt_word_embeddings = np.load('en-embeddings.npy')\n",
        "        \n",
        "        # The sentence IDs being currently processed to create the \n",
        "        # current batch\n",
        "        self._sent_ids = None\n",
        "        \n",
        "        # We want a batch of data from source or target?\n",
        "        self._is_source = is_source\n",
        "        # Is this training or testing data?\n",
        "        self._is_train = is_train\n",
        "                \n",
        "    def next_batch(self, sent_ids):\n",
        "        \n",
        "        # Depending on wheter we want source or target data\n",
        "        # change the maximum sentence length\n",
        "        if self._is_source:\n",
        "            max_sent_length = src_max_sent_length\n",
        "        else:\n",
        "            max_sent_length = tgt_max_sent_length\n",
        "            \n",
        "        # Arrays to hold input and output data\n",
        "        # Word embeddings (current word)\n",
        "        batch_data = np.zeros((self._batch_size,input_size),dtype=np.float32)\n",
        "        # One-hot encoded label (next word)\n",
        "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
        "        \n",
        "        \n",
        "        # Populate each index of the batch\n",
        "        for b in range(self._batch_size):\n",
        "            \n",
        "            # Sentence IDs to get data from\n",
        "            sent_id = sent_ids[b]\n",
        "            \n",
        "            # If generating data with source sentences\n",
        "            # use src_word_embeddings\n",
        "            if self._is_source:\n",
        "                # Depending on whether we need training data or testind data\n",
        "                # choose the previously created training or testind data\n",
        "                if self._is_train:\n",
        "                    sent_text = train_inputs[sent_id]\n",
        "                else:\n",
        "                    sent_text = test_inputs[sent_id]\n",
        "                             \n",
        "                # Populate the batch data arrays\n",
        "                batch_data[b] = self._src_word_embeddings[sent_text[self._cursor[b]],:]\n",
        "                batch_labels[b] = np.zeros((vocabulary_size),dtype=np.float32)\n",
        "                batch_labels[b,sent_text[self._cursor[b]+1]] = 1.0\n",
        "            # If generating data with target sentences\n",
        "            # use tgt_word_embeddings\n",
        "            else:\n",
        "                # Depending on whether we need training data or testind data\n",
        "                # choose the previously created training or testind data\n",
        "                if self._is_train:\n",
        "                    sent_text = train_outputs[sent_id]\n",
        "                else:\n",
        "                    sent_text = test_outputs[sent_id]\n",
        "                \n",
        "                # We cannot avoid having two different embedding vectors for <s> token\n",
        "                # in soruce and target languages\n",
        "                # Therefore, if the symbol appears, we always take the source embedding vector\n",
        "                if sent_text[self._cursor[b]]!=tgt_dictionary['<s>']:\n",
        "                    batch_data[b] = self._tgt_word_embeddings[sent_text[self._cursor[b]],:]\n",
        "                else:\n",
        "                    batch_data[b] = self._src_word_embeddings[sent_text[self._cursor[b]],:]\n",
        "                \n",
        "                # Populate the data arrays\n",
        "                batch_labels[b] = np.zeros((vocabulary_size),dtype=np.float32)\n",
        "                batch_labels[b,sent_text[self._cursor[b]+1]] = 1.0\n",
        "            \n",
        "            # Update the cursor for each batch index\n",
        "            self._cursor[b] = (self._cursor[b]+1)%(max_sent_length-1)\n",
        "             \n",
        "        return batch_data,batch_labels\n",
        "        \n",
        "    def unroll_batches(self,sent_ids):\n",
        "        \n",
        "        # Only if new sentence IDs if provided\n",
        "        # else it will use the previously defined \n",
        "        # sent_ids continuously\n",
        "        if sent_ids is not None:\n",
        "            \n",
        "            self._sent_ids = sent_ids\n",
        "            # Unlike in the previous exercises we do not process a single sequence\n",
        "            # over many iterations of unrollings. We process either a source sentence or target sentence\n",
        "            # at a single go. So we reset the _cursor evrytime we generate a batch\n",
        "            self._cursor = [0 for _ in range(self._batch_size)]\n",
        "                \n",
        "        unroll_data,unroll_labels = [],[]\n",
        "        \n",
        "        # Unrolling data over time\n",
        "        for ui in range(self._num_unroll):\n",
        "            \n",
        "            if self._is_source:\n",
        "                data, labels = self.next_batch(self._sent_ids)\n",
        "            else:\n",
        "                data, labels = self.next_batch(self._sent_ids)\n",
        "                    \n",
        "            unroll_data.append(data)\n",
        "            unroll_labels.append(labels)\n",
        "        \n",
        "        # Return unrolled data and sentence IDs\n",
        "        return unroll_data, unroll_labels, self._sent_ids\n",
        "    \n",
        "    def reset_indices(self):\n",
        "        self._cursor = [0 for offset in range(self._batch_size)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV5g664qhBN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "d3e4c4b6-171e-41af-b2a8-52951fc44d1b"
      },
      "source": [
        "# Running a tiny set to see if the implementation correct\n",
        "dg = DataGeneratorMT(batch_size=5,num_unroll=20,is_source=True, is_train=True)\n",
        "u_data, u_labels, _ = dg.unroll_batches([0,1,2,3,4])\n",
        "\n",
        "print('Source data')\n",
        "for _, lbl in zip(u_data,u_labels):\n",
        "    # the the string words for returned word IDs and display the results\n",
        "    print([src_reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source data\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '.', '</s>', '</s>']\n",
            "['</s>', '</s>', 'bietet', '.', '</s>']\n",
            "['</s>', '</s>', 'Dateiformat', 'nutzen', '</s>']\n",
            "['</s>', '</s>', '##AT##-##AT##', 'optimal', '</s>']\n",
            "['</s>', '</s>', 'PSD', 'Bilder', '</s>']\n",
            "['</s>', '</s>', 'das', 'Ihre', '.']\n",
            "['</s>', '</s>', 'über', 'für', 'werden']\n",
            "['.', '</s>', 'Photoshop', 'es', 'ausgewählt']\n",
            "['können', '.', 'mit', 'Sie', 'Verwendungszweck']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTF3bbRhhEJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "588d12d8-02bf-4feb-def8-81f94fec66d7"
      },
      "source": [
        "# Running a tiny set to see if the implementation correct\n",
        "dg = DataGeneratorMT(batch_size=5,num_unroll=30,is_source=False, is_train=True)\n",
        "u_data, u_labels, _ = dg.unroll_batches([0,2,3,4,5])\n",
        "print('\\nTarget data batch')\n",
        "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
        "    # the the string words for returned word IDs and display the results\n",
        "    print([tgt_reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Target data batch\n",
            "['Here', 'QuarkXPress', 'In', 'For', 'If']\n",
            "[',', '8', 'this', 'example', 'you']\n",
            "['you', 'is', 'section', ',', 'use']\n",
            "['’', 'considered', 'we', 'you', 'PSD']\n",
            "['ll', 'by', '’', 'may', ',']\n",
            "['find', 'many', 'll', 'have', 'you']\n",
            "['out', 'to', 'explain', 'multiple', 'can']\n",
            "['how', 'have', 'when', 'layers', 'switch']\n",
            "['Creative', 'the', 'you', 'in', 'those']\n",
            "['Suite', 'best', 'should', 'your', 'layers']\n",
            "['users', 'integration', 'use', 'PSD', 'on']\n",
            "['can', 'with', 'the', 'with', 'or']\n",
            "['get', 'Photoshop', 'PSD', 'different', 'off']\n",
            "['the', '’', 'format', 'product', 'in']\n",
            "['best', 's', 'for', 'shots', 'QuarkXPress']\n",
            "['possible', 'PSD', 'your', ',', 'without']\n",
            "['interaction', 'file', 'images', 'which', 'having']\n",
            "['with', 'format', 'and', 'will', 'to']\n",
            "['QuarkXPress', 'of', 'how', 'vary', 'save']\n",
            "['.', 'any', 'to', 'from', 'a']\n",
            "['</s>', 'layout', 'get', 'publication', 'separate']\n",
            "['</s>', 'tool', 'the', 'to', 'TIFF']\n",
            "['</s>', 'available', 'most', 'publication', 'for']\n",
            "['</s>', 'today', 'out', '.', 'each']\n",
            "['</s>', '.', 'of', '</s>', 'publication']\n",
            "['</s>', '</s>', 'them', '</s>', '.']\n",
            "['</s>', '</s>', '.', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['</s>', '</s>', '</s>', '</s>', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUTQJ3Nzhhyb",
        "colab_type": "text"
      },
      "source": [
        "# NMT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-HeW5bUj3ON",
        "colab_type": "text"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgNGglmXj--u",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dkqeCGRkBhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tgt_emb_mat = np.load('en-embeddings.npy')\n",
        "input_size = tgt_emb_mat.shape[1]\n",
        "\n",
        "num_nodes = 128\n",
        "batch_size = 10\n",
        "\n",
        "# We unroll the full length at one go\n",
        "# both source and target sentences\n",
        "enc_num_unrollings = 40\n",
        "dec_num_unrollings = 60"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABM-aoamr80",
        "colab_type": "text"
      },
      "source": [
        "### Defining Input/Output Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj_TDfxJmvWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Target embeddings are needed to compute the test outputs\n",
        "tgt_word_embeddings = tf.convert_to_tensor(tgt_emb_mat,name='tgt_embeddings')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4q-hLmBm7JM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "56144fa4-1543-4a4f-ea9f-df0c7d37811d"
      },
      "source": [
        "print('Defining encoder data placeholders')\n",
        "\n",
        "# Training Input placeholders (Encoder)\n",
        "# Encoder related input data, we directly feed in the embeddings\n",
        "enc_train_inputs = []\n",
        "\n",
        "# Defining unrolled training inputs\n",
        "for ui in range(enc_num_unrollings):\n",
        "    enc_train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,input_size],name='train_inputs_%d'%ui))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining encoder data placeholders\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2qefkPvm9t6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "c255339a-0d90-450c-9701-4079d2eadc19"
      },
      "source": [
        "print('Defining decoder data placeholders')\n",
        "# Training Input/Output/Mask data (Decoder)\n",
        "# Decoder inputs and outputs\n",
        "dec_train_inputs, dec_train_labels = [],[]\n",
        "# We use masking to mask out the any of the </s> elements\n",
        "# from the loss computation in the decoder\n",
        "dec_train_masks = []\n",
        "\n",
        "# Defining unrolled training inputs\n",
        "for ui in range(dec_num_unrollings):\n",
        "    dec_train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,input_size],name='dec_train_inputs_%d'%ui))\n",
        "    dec_train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'dec_train_labels_%d'%ui))\n",
        "    dec_train_masks.append(tf.placeholder(tf.float32, shape=[batch_size,1],name='dec_train_masks_%d'%ui))\n",
        "\n",
        "    \n",
        "enc_test_input = [tf.placeholder(tf.float32, shape=[batch_size,input_size], name='test_input_%d'%ui) for ui in range(enc_num_unrollings)]\n",
        "dec_test_input = tf.nn.embedding_lookup(tgt_word_embeddings,[tgt_dictionary['<s>']])\n",
        "print('Done')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining decoder data placeholders\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3VIbyvSm-iU",
        "colab_type": "text"
      },
      "source": [
        "### Encoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eckGWSfurvmM",
        "colab_type": "text"
      },
      "source": [
        "Got a `tf.compat.v1` has no module `contrib` error. Solution was to copy paste this file that has the `xavier_initializer()`\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/contrib/layers/python/layers/initializers.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ5g0tNNrcNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.ops import random_ops\n",
        "\n",
        "\n",
        "def xavier_initializer(uniform=True, seed=None, dtype=dtypes.float32):\n",
        "  \"\"\"Returns an initializer performing \"Xavier\" initialization for weights.\n",
        "  This function implements the weight initialization from:\n",
        "  Xavier Glorot and Yoshua Bengio (2010):\n",
        "           [Understanding the difficulty of training deep feedforward neural\n",
        "           networks. International conference on artificial intelligence and\n",
        "           statistics.](\n",
        "           http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n",
        "  This initializer is designed to keep the scale of the gradients roughly the\n",
        "  same in all layers. In uniform distribution this ends up being the range:\n",
        "  `x = sqrt(6. / (in + out)); [-x, x]` and for normal distribution a standard\n",
        "  deviation of `sqrt(2. / (in + out))` is used.\n",
        "  Args:\n",
        "    uniform: Whether to use uniform or normal distributed random initialization.\n",
        "    seed: A Python integer. Used to create random seeds. See\n",
        "          `tf.compat.v1.set_random_seed` for behavior.\n",
        "    dtype: The data type. Only floating point types are supported.\n",
        "  Returns:\n",
        "    An initializer for a weight matrix.\n",
        "  \"\"\"\n",
        "  return variance_scaling_initializer(factor=1.0, mode='FAN_AVG',\n",
        "                                      uniform=uniform, seed=seed, dtype=dtype)\n",
        "\n",
        "xavier_initializer_conv2d = xavier_initializer\n",
        "\n",
        "\n",
        "def variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False,\n",
        "                                 seed=None, dtype=dtypes.float32):\n",
        "  \"\"\"Returns an initializer that generates tensors without scaling variance.\n",
        "  When initializing a deep network, it is in principle advantageous to keep\n",
        "  the scale of the input variance constant, so it does not explode or diminish\n",
        "  by reaching the final layer. This initializer use the following formula:\n",
        "  ```python\n",
        "    if mode='FAN_IN': # Count only number of input connections.\n",
        "      n = fan_in\n",
        "    elif mode='FAN_OUT': # Count only number of output connections.\n",
        "      n = fan_out\n",
        "    elif mode='FAN_AVG': # Average number of inputs and output connections.\n",
        "      n = (fan_in + fan_out)/2.0\n",
        "      truncated_normal(shape, 0.0, stddev=sqrt(factor / n))\n",
        "  ```\n",
        "  * To get [Delving Deep into Rectifiers](\n",
        "     http://arxiv.org/pdf/1502.01852v1.pdf) (also know as the \"MSRA \n",
        "     initialization\"), use (Default):<br/>\n",
        "    `factor=2.0 mode='FAN_IN' uniform=False`\n",
        "  * To get [Convolutional Architecture for Fast Feature Embedding](\n",
        "     http://arxiv.org/abs/1408.5093), use:<br/>\n",
        "    `factor=1.0 mode='FAN_IN' uniform=True`\n",
        "  * To get [Understanding the difficulty of training deep feedforward neural\n",
        "    networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf),\n",
        "    use:<br/>\n",
        "    `factor=1.0 mode='FAN_AVG' uniform=True.`\n",
        "  * To get `xavier_initializer` use either:<br/>\n",
        "    `factor=1.0 mode='FAN_AVG' uniform=True`, or<br/>\n",
        "    `factor=1.0 mode='FAN_AVG' uniform=False`.\n",
        "  Args:\n",
        "    factor: Float.  A multiplicative factor.\n",
        "    mode: String.  'FAN_IN', 'FAN_OUT', 'FAN_AVG'.\n",
        "    uniform: Whether to use uniform or normal distributed random initialization.\n",
        "    seed: A Python integer. Used to create random seeds. See\n",
        "          `tf.compat.v1.set_random_seed` for behavior.\n",
        "    dtype: The data type. Only floating point types are supported.\n",
        "  Returns:\n",
        "    An initializer that generates tensors with unit variance.\n",
        "  Raises:\n",
        "    ValueError: if `dtype` is not a floating point type.\n",
        "    TypeError: if `mode` is not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG'].\n",
        "  \"\"\"\n",
        "  if not dtype.is_floating:\n",
        "    raise TypeError('Cannot create initializer for non-floating point type.')\n",
        "  if mode not in ['FAN_IN', 'FAN_OUT', 'FAN_AVG']:\n",
        "    raise TypeError('Unknown mode %s [FAN_IN, FAN_OUT, FAN_AVG]', mode)\n",
        "\n",
        "  # pylint: disable=unused-argument\n",
        "  def _initializer(shape, dtype=dtype, partition_info=None):\n",
        "    \"\"\"Initializer function.\"\"\"\n",
        "    if not dtype.is_floating:\n",
        "      raise TypeError('Cannot create initializer for non-floating point type.')\n",
        "    # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\n",
        "    # This is the right thing for matrix multiply and convolutions.\n",
        "    if shape:\n",
        "      fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n",
        "      fan_out = float(shape[-1])\n",
        "    else:\n",
        "      fan_in = 1.0\n",
        "      fan_out = 1.0\n",
        "    for dim in shape[:-2]:\n",
        "      fan_in *= float(dim)\n",
        "      fan_out *= float(dim)\n",
        "    if mode == 'FAN_IN':\n",
        "      # Count only number of input connections.\n",
        "      n = fan_in\n",
        "    elif mode == 'FAN_OUT':\n",
        "      # Count only number of output connections.\n",
        "      n = fan_out\n",
        "    elif mode == 'FAN_AVG':\n",
        "      # Average number of inputs and output connections.\n",
        "      n = (fan_in + fan_out) / 2.0\n",
        "    if uniform:\n",
        "      # To get stddev = math.sqrt(factor / n) need to adjust for uniform.\n",
        "      limit = math.sqrt(3.0 * factor / n)\n",
        "      return random_ops.random_uniform(shape, -limit, limit,\n",
        "                                       dtype, seed=seed)\n",
        "    else:\n",
        "      # To get stddev = math.sqrt(factor / n) need to adjust for truncated.\n",
        "      trunc_stddev = math.sqrt(1.3 * factor / n)\n",
        "      return random_ops.truncated_normal(shape, 0.0, trunc_stddev, dtype,\n",
        "                                         seed=seed)\n",
        "  # pylint: enable=unused-argument\n",
        "\n",
        "  return _initializer"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzqbZLxTnWrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "04ce6e52-9fc4-4eb0-abb8-deeb4b84b44a"
      },
      "source": [
        "print('Defining Encoder Parameters')\n",
        "with tf.variable_scope('Encoder'):\n",
        "    \n",
        "    # Input gate (i_t) - How much memory to write to cell state\n",
        "    # We use xavier intialization as this gives better results \n",
        "    enc_ix = tf.get_variable('ix',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_im = tf.get_variable('im',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.05, 0.05),name='ib')\n",
        "    \n",
        "    # Forget gate (f_t) - How much memory to discard from cell state\n",
        "    enc_fx = tf.get_variable('fx',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_fm = tf.get_variable('fm',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.05, 0.05),name='fb')\n",
        "    \n",
        "    # Candidate value (c~_t) - Used to compute the current cell state                            \n",
        "    enc_cx = tf.get_variable('cx',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_cm = tf.get_variable('cm',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.05,0.05),name='cb') \n",
        "    \n",
        "    # Output gate - How much memory to output from the cell state\n",
        "    enc_ox = tf.get_variable('ox',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_om = tf.get_variable('om',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    enc_ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.05,0.05),name='ob') \n",
        "    \n",
        "    # Variables saving state across unrollings (testing).\n",
        "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_output')\n",
        "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name = 'train_cell')\n",
        "    \n",
        "    saved_test_output = tf.Variable(tf.zeros([batch_size, num_nodes]),trainable=False, name='test_output')\n",
        "    saved_test_state = tf.Variable(tf.zeros([batch_size, num_nodes]),trainable=False, name='test_cell')\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining Encoder Parameters\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1_CSWRusGDD",
        "colab_type": "text"
      },
      "source": [
        "### Decoder: LSTM + Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ZeHqausIwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "cd9c5865-b5ca-4189-b784-bb13af988fbe"
      },
      "source": [
        "print('Defining Decoder Parameters')\n",
        "with tf.variable_scope('Decoder'):\n",
        "    \n",
        "    # Input gate (i_t) - How much memory to write to cell state\n",
        "    # We use xavier intialization as this gives better results\n",
        "    dec_ix = tf.get_variable('ix',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    dec_im = tf.get_variable('im',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    dec_ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.05, 0.05),name='ib')    \n",
        "    \n",
        "    # Forget gate (f_t) - How much memory to discard from cell state\n",
        "    dec_fx = tf.get_variable('fx',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    dec_fm = tf.get_variable('fm',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    dec_fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.05, 0.05),name='fb')    \n",
        "    \n",
        "    # Candidate value (c~_t) - Used to compute the current cell state                             \n",
        "    dec_cx = tf.get_variable('cx',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    dec_cm = tf.get_variable('cm',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    dec_cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.05,0.05),name='cb')     \n",
        "    \n",
        "    # Output gate - How much memory to output from the cell state\n",
        "    dec_ox = tf.get_variable('ox',shape=[input_size, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    dec_om = tf.get_variable('om',shape=[num_nodes, num_nodes],\n",
        "                             initializer = xavier_initializer())\n",
        "    dec_ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.05,0.05),name='ob') \n",
        "    \n",
        "    # Softmax Classifier weights and biases.\n",
        "    # If we are using sampled softmax loss, the weights dims shouldbe [50000, 64]\n",
        "    # If not, then [64, 50000]\n",
        "    w = tf.get_variable('softmax_weights',shape=[num_nodes, vocabulary_size], \n",
        "                        initializer = xavier_initializer())\n",
        "    b = tf.Variable(tf.random_uniform([vocabulary_size],-0.05,-0.05),name='softmax_bias')\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining Decoder Parameters\n",
            "\tDone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op0LsyoDsi2y",
        "colab_type": "text"
      },
      "source": [
        "### Defining LSTM Computations\n",
        "Here we first define two function enc_lstm_cell and dec_lstm_cell which define the LSTM cell computations. \n",
        "\n",
        "Next we define the computations to compute the final state variables of the encoder, feeding that into the decoder as the intial state and finally computing the LSTM output, logit values and the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jgu-5A9spur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definition of the cell computation (Encoder)\n",
        "def enc_lstm_cell(i, o, state):\n",
        "    \"\"\"Create a LSTM cell\"\"\"\n",
        "    input_gate = tf.sigmoid(tf.matmul(i, enc_ix) + tf.matmul(o, enc_im) + enc_ib)\n",
        "    forget_gate = tf.sigmoid(tf.matmul(i, enc_fx) + tf.matmul(o, enc_fm) + enc_fb)\n",
        "    update = tf.matmul(i, enc_cx) + tf.matmul(o, enc_cm) + enc_cb\n",
        "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
        "    output_gate = tf.sigmoid(tf.matmul(i, enc_ox) + tf.matmul(o, enc_om) + enc_ob)\n",
        "    return output_gate * tf.tanh(state), state"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z-2rkmwsuxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definition of the cell computation (Decoder)\n",
        "def dec_lstm_cell(i, o, state):\n",
        "    \"\"\"Create a LSTM cell\"\"\"\n",
        "    input_gate = tf.sigmoid(tf.matmul(i, dec_ix) + tf.matmul(o, dec_im) + dec_ib)\n",
        "    forget_gate = tf.sigmoid(tf.matmul(i, dec_fx) + tf.matmul(o, dec_fm) + dec_fb)\n",
        "    update = tf.matmul(i, dec_cx) + tf.matmul(o, dec_cm) + dec_cb\n",
        "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
        "    output_gate = tf.sigmoid(tf.matmul(i, dec_ox) + tf.matmul(o, dec_om) + dec_ob)\n",
        "    return output_gate * tf.tanh(state), state"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eK_twvhsvs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training inference logic\n",
        "# Holds the outputs of the all unrolled LSTM steps\n",
        "outputs = list()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7LSSTo3syYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the output and state variables\n",
        "# with zeros and iteratively update these two \n",
        "# variables with the LSTM's output and state\n",
        "output = saved_output\n",
        "state = saved_state"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc5lrcD8s02l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "56c146e5-ba54-4094-c490-5d4617bf09de"
      },
      "source": [
        "print('Calculating Encoder Output')\n",
        "# Update the output and state of the encoder iteratively\n",
        "for i in enc_train_inputs:\n",
        "    output, state = enc_lstm_cell(i, output,state)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Encoder Output\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_HefSv3s4de",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "64288282-e9b7-4b5f-be2f-b1e3631089a5"
      },
      "source": [
        "print('Calculating Decoder Output')\n",
        "# With the computations of the enc_lstm_cell done,\n",
        "# calculate the output and state of the decoder\n",
        "with tf.control_dependencies([saved_output.assign(output),\n",
        "                             saved_state.assign(state)]):\n",
        "    # Calculate the decoder state and output iteratively\n",
        "    for i in dec_train_inputs:\n",
        "        output, state = dec_lstm_cell(i, output, state)\n",
        "        outputs.append(output)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Decoder Output\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a-z4QUes907",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the logits of the decoder for all unrolled steps\n",
        "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGIlpLiltERN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions from the decoder\n",
        "train_prediction = tf.nn.softmax(logits)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ScIGqi-tG-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing related inference logic\n",
        "# We calculate the test predictions for the maximum \n",
        "# num_unrollings allowed for target language\n",
        "test_output  = saved_test_output\n",
        "test_state = saved_test_state"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpR5eKyTtKxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = []"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWr_v_gPtNbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the encoder output iteratively\n",
        "for i in enc_test_input:\n",
        "    test_output, test_state = enc_lstm_cell(i, test_output,test_state)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ch_CNRXtQPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the decoder output iteratively\n",
        "# where the input is whatever the previously output word's embedding\n",
        "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
        "                                 saved_test_state.assign(test_state)]):\n",
        "    for i in range(dec_num_unrollings):\n",
        "\n",
        "        test_output, test_state = dec_lstm_cell(dec_test_input, test_output, test_state)\n",
        "\n",
        "        test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))\n",
        "        \n",
        "        dec_test_input = tf.nn.embedding_lookup(tgt_word_embeddings,tf.argmax(test_prediction,axis=1))\n",
        "        test_predictions.append(tf.argmax(test_prediction,axis=1))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flt_C5cxtT2W",
        "colab_type": "text"
      },
      "source": [
        "### Calculating the Loss\n",
        "Here we calculate the loss. Loss is calculated by summing all the losses obtained across the time axis and averagin over the batch axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDm6SArWtc6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "960f4ac2-1b67-4056-ec8e-762709d81542"
      },
      "source": [
        "print('Calculating Loss')\n",
        "\n",
        "# No need for a tf.contro_dependencies(...) clause here\n",
        "# Because we restart the state anyway after each sentence batch\n",
        "loss_batch = tf.concat(\n",
        "    axis=0,\n",
        "    values=dec_train_masks\n",
        "    ) * tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=logits, \n",
        "        labels=tf.concat(axis=0, values=dec_train_labels)\n",
        "    )\n",
        "loss = tf.reduce_mean(loss_batch)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Loss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWuS3gldtsYD",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer\n",
        "We define the model optimization specific operations. We use two optimizers here; Adam and SGD. I observed that using Adam only cause the model to exhibit some undesired behaviors in the long run. Therefore we use Adam to get a good initial estimate for the SGD and use SGD from that point onwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VqSIVAat0mG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "a44dc3b4-85d4-4c58-d25e-ce82acd05aaf"
      },
      "source": [
        "print('Defining Optimizers')\n",
        "# These are used to decay learning rate over time\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "inc_gstep = tf.assign(global_step,global_step + 1)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining Optimizers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFMM7lXbuG5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use two optimizers, when the optimizer changes\n",
        "# we reset the global step\n",
        "reset_gstep = tf.assign(global_step,0)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC0Fw3SKuJVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate decaying learning rate\n",
        "learning_rate = tf.maximum(\n",
        "    tf.train.exponential_decay(\n",
        "        0.005, global_step, decay_steps=1, decay_rate=0.95, staircase=True\n",
        "    ), 0.00001)\n",
        "\n",
        "sgd_learning_rate = tf.maximum(\n",
        "    tf.train.exponential_decay(\n",
        "        0.005, global_step, decay_steps=1, decay_rate=0.95, staircase=True\n",
        "    ), 0.00001)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7l3E4oYuLOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use two optimizers: Adam and naive SGD\n",
        "# using Adam in the long run produced undesirable results \n",
        "# (e.g.) sudden fluctuations in BLEU\n",
        "# Therefore we use Adam to get a good starting point for optimizing\n",
        "# and then switch to SGD from that point onwards\n",
        "with tf.variable_scope('Adam'):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "with tf.variable_scope('SGD'):\n",
        "    sgd_optimizer = tf.train.GradientDescentOptimizer(sgd_learning_rate)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC1aY-kZuMzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculates gradients with clipping for Adam\n",
        "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
        "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "optimize = optimizer.apply_gradients(zip(gradients, v))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VohFGpk_uOb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculates gradients with clipping for SGD\n",
        "sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))\n",
        "sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 5.0)\n",
        "sgd_optimize = optimizer.apply_gradients(zip(sgd_gradients, v))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-4GLwaKuQkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure gradients exist flowing from decoder to encoder\n",
        "for (g_i,v_i) in zip(gradients,v):\n",
        "    assert g_i is not None, 'Gradient none for %s'%(v_i.name)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKF7YFvQvgGp",
        "colab_type": "text"
      },
      "source": [
        "### Resetting Train and Test States"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmiRDPP5vwTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset training state\n",
        "reset_train_state = tf.group(\n",
        "    tf.assign(saved_output, tf.zeros([batch_size, num_nodes])),\n",
        "    tf.assign(saved_state, tf.zeros([batch_size, num_nodes]))\n",
        ")\n",
        "\n",
        "reset_test_state = tf.group(\n",
        "    saved_test_output.assign(tf.zeros([batch_size, num_nodes])),\n",
        "    saved_test_state.assign(tf.zeros([batch_size, num_nodes]))\n",
        ")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZnwnHhxgAZ",
        "colab_type": "text"
      },
      "source": [
        "### Functions for Evaulating and Printing Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgUIco3_xm5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_and_save_train_predictions(du_labels, tr_pred, rand_idx, train_prediction_text_fname):\n",
        "    '''\n",
        "    Use this to print some predicted training samples and save it to file\n",
        "    du_labels: Decoder's unrolled labels (this is a list of dec_num_unrollings \n",
        "    where each item is [batch_size, vocabulary_size])\n",
        "    tr_pred: This is an array [dec_num_unrollings*batch_size, vocabulary_size] array\n",
        "    rand_idx: Some random index we use to pick a data point to print\n",
        "    train_prediction_text_fname: The file we save the prediction results into\n",
        "    '''\n",
        "\n",
        "    # This print_str will be written to the text file as well as printed here\n",
        "    print_str = 'Actual: ' \n",
        "    \n",
        "    # We can get each label corresponding to some sentence by traversing the\n",
        "    # concatenated labels array ([dec_num_unrollings*batch_size, vocabulary_size])\n",
        "    # with a batch_size stride\n",
        "    for w in np.argmax(np.concatenate(du_labels,axis=0)[rand_idx::batch_size],axis=1).tolist():\n",
        "        # Update the print_str\n",
        "        print_str += tgt_reverse_dictionary[w] + ' '\n",
        "        # When we encounter the end of sentence </s> we stop printing\n",
        "        if tgt_reverse_dictionary[w] == '</s>':\n",
        "            break\n",
        "    print(print_str)\n",
        "    \n",
        "    # Write to file\n",
        "    with open(os.path.join(log_dir, train_prediction_text_fname),'a',encoding='utf-8') as fa:                \n",
        "        fa.write(print_str+'\\n')  \n",
        "\n",
        "    # Now print the predicted data by following the same procedure as above\n",
        "    print()\n",
        "    print_str = 'Predicted: '\n",
        "    for w in np.argmax(tr_pred[rand_idx::batch_size],axis=1).tolist():\n",
        "        print_str += tgt_reverse_dictionary[w] + ' '\n",
        "        # When we encounter the end of sentence </s> we stop printing\n",
        "        if tgt_reverse_dictionary[w] == '</s>':\n",
        "            break\n",
        "    print(print_str)\n",
        "    with open(os.path.join(log_dir, train_prediction_text_fname),'a',encoding='utf-8') as fa:                \n",
        "        fa.write(print_str+'\\n')    \n",
        "    \n",
        "    \n",
        "def print_and_save_test_predictions(test_du_labels, test_pred_unrolled, batch_id, test_rand_idx, test_prediction_text_fname):\n",
        "    '''\n",
        "    Use this to print some predicted training samples and save it to file\n",
        "    test_du_labels: Decoder's unrolled labels (this is a list of dec_num_unrollings \n",
        "    where each item is [batch_size, vocabulary_size])\n",
        "    test_pred_unrolled: This is an array [dec_num_unrollings*batch_size, vocabulary_size] array\n",
        "    batch_id: We need this to retrieve the actual sentence for the predicted \n",
        "    test_rand_idx: Some random index we use to pick a data point to print\n",
        "    test_prediction_text_fname: The file we save the prediction results into\n",
        "    '''\n",
        "    \n",
        "    # Print the actual sentence\n",
        "    print('DE: ',test_source_sent[(batch_id*batch_size)+test_rand_idx])\n",
        "    # print_str is the string we display as results and write to a file\n",
        "    print_str = '\\t EN (TRUE):' + test_target_sent[(batch_id*batch_size)+test_rand_idx]\n",
        "    print(print_str + '\\n')\n",
        "\n",
        "    # Printing predictions\n",
        "    print_str = '\\t EN (Predicted): ' \n",
        "    \n",
        "    for test_pred in test_pred_unrolled:                            \n",
        "        print_str += tgt_reverse_dictionary[test_pred[test_rand_idx]] + ' '\n",
        "        if tgt_reverse_dictionary[test_pred[test_rand_idx]] == '</s>':\n",
        "            break\n",
        "    print(print_str + '\\n')\n",
        "\n",
        "    # Write the results to text file\n",
        "    with open(os.path.join(log_dir, test_prediction_text_fname),'a',encoding='utf-8') as fa:                                \n",
        "        fa.write(print_str+'\\n') \n",
        "        \n",
        "def create_bleu_ref_candidate_lists(all_preds, all_labels):\n",
        "    '''\n",
        "    Creates two lists (candidate list and reference list) for calcluating BLEU\n",
        "    all_preds: All the predictions\n",
        "    all_labels: Correspondign all the actual labels\n",
        "    Returns\n",
        "    cand_list: List (sentences) of lists (words in a sentence)\n",
        "    ref_list: List (sentences) of lists (words in a sentence)\n",
        "    '''\n",
        "    bleu_labels, bleu_preds = [],[]\n",
        "    \n",
        "    # calculate bleu score:        \n",
        "    # We iterate batch_size times as i=0,1,2,...,batch_size while grabbing \n",
        "    # i, i+batch_size, i+2*batch_size, i+3*batch_size elements from all_labels and all_preds\n",
        "    # This because the labels/predicitons belonging to same sentence are interleaved by batch_size \n",
        "    # due to the way concatenate labels and predictions\n",
        "    # Taking elements interleaved by batch_size gives the sequence of words belonging to the same sentence\n",
        "    ref_list, cand_list = [],[]\n",
        "    for b_i in range(batch_size):\n",
        "        tmp_lbl = all_labels[b_i::batch_size]            \n",
        "        tmp_lbl = tmp_lbl[np.where(tmp_lbl != tgt_dictionary['</s>'])]            \n",
        "        ref_str = ' '.join([tgt_reverse_dictionary[lbl] for lbl in tmp_lbl])\n",
        "        ref_list.append([ref_str])\n",
        "\n",
        "        tmp_pred = all_preds[b_i::batch_size]\n",
        "        tmp_pred = tmp_pred[np.where(tmp_pred != tgt_dictionary['</s>'])]\n",
        "        cand_str = ' '.join([tgt_reverse_dictionary[pre] for pre in tmp_pred])\n",
        "        cand_list.append(cand_str)\n",
        "\n",
        "    return cand_list, ref_list"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M-A-aANyIge",
        "colab_type": "text"
      },
      "source": [
        "### Defining a Single Step of Training\n",
        "We now define a function to train the NMT model for a single step. It takes in encoder inputs, decoder inputs and decoder labels and train the NMT for a single step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCyI-hnMyNc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_single_step(eu_data, du_data, du_labels):\n",
        "    '''\n",
        "    Define a single training step\n",
        "    eu_data: Unrolled encoder inputs (word embeddings)\n",
        "    du_data: Unrolled decoder inputs (word embeddings)\n",
        "    du_labels: Unrolled decoder outputs (one hot encoded words)\n",
        "    '''\n",
        "    # Fill the feed dict (Encoder)\n",
        "    feed_dict = {}\n",
        "    for ui,dat in enumerate(eu_data):            \n",
        "        feed_dict[enc_train_inputs[ui]] = dat    \n",
        "    \n",
        "    \n",
        "    # Fill the feed dict (Decoder) \n",
        "    for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
        "        feed_dict[dec_train_inputs[ui]] = dat\n",
        "        feed_dict[dec_train_labels[ui]] = lbl\n",
        "        # The mask masks the </s> items from being part of the loss\n",
        "        d_msk = (np.logical_not(np.argmax(lbl,axis=1)==tgt_dictionary['</s>'])).astype(np.int32).reshape(-1,1)\n",
        "        feed_dict[dec_train_masks[ui]] = d_msk\n",
        "    \n",
        "    # ======================= OPTIMIZATION ==========================\n",
        "    # Using Adam in long term gives very weird behaviors in loss\n",
        "    # so after 20000 iterations we change the optimizer to SGD\n",
        "    if (step+1)<20000:\n",
        "        _,l,tr_pred = sess.run([optimize,loss,train_prediction], feed_dict=feed_dict)\n",
        "    else:\n",
        "        _,l,tr_pred = sess.run([sgd_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
        "        \n",
        "    return l, tr_pred"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPtmdr4dyP78",
        "colab_type": "text"
      },
      "source": [
        "### Defining Data Generators and Other Related Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YunsUWK3yV5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is where all the results will be logged into\n",
        "log_dir = 'logs'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.mkdir(log_dir)\n",
        "\n",
        "# Filenames of the logs\n",
        "train_prediction_text_fname = 'train_predictions.txt'\n",
        "test_prediction_text_fname = 'test_predictions.txt'\n",
        "\n",
        "# Some configuration for the TensorFlow session\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement=True\n",
        "sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "# Initialize variables\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "# Load the word embeddings\n",
        "src_word_embeddings = np.load('de-embeddings.npy')\n",
        "tgt_word_embeddings = np.load('en-embeddings.npy')\n",
        "\n",
        "# Defining data generators\n",
        "def define_data_generators(batch_size, enc_num_unrollings, dec_num_unrollings):\n",
        "    \n",
        "    # Training data generators (Encoder and Decoder)\n",
        "    enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=enc_num_unrollings,is_source=True, is_train=True)\n",
        "    dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=dec_num_unrollings,is_source=False, is_train=True)\n",
        "\n",
        "    # Testing data generators (Encoder and Decoder)\n",
        "    test_enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=enc_num_unrollings,is_source=True, is_train=False)\n",
        "    test_dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=dec_num_unrollings,is_source=False, is_train=False)\n",
        "    \n",
        "    return enc_data_generator,dec_data_generator,test_enc_data_generator,test_dec_data_generator"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8tbenyVybT5",
        "colab_type": "text"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCj2lZMgyiCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a558019c-8daa-4eae-eb17-27b54b1cf98e"
      },
      "source": [
        "# Training and test BLEU scores\n",
        "train_bleu_scores_over_time,test_bleu_scores_over_time = [],[]\n",
        "# Loss over time\n",
        "loss_over_time = []\n",
        "\n",
        "# Labels and predictions required to calculate the BLEU scores\n",
        "# For both train and test data\n",
        "train_bleu_refs, train_bleu_cands = [],[]\n",
        "test_bleu_refs, test_bleu_cands = [],[]\n",
        "\n",
        "# Number of steps the training goes for\n",
        "num_steps = 10001\n",
        "avg_loss = 0\n",
        "\n",
        "# Defining data generators for encoder/decoder and training/testing\n",
        "enc_data_generator, dec_data_generator, \\\n",
        "test_enc_data_generator, test_dec_data_generator = \\\n",
        "define_data_generators(batch_size, enc_num_unrollings, dec_num_unrollings)\n",
        "\n",
        "print('Started Training')\n",
        "\n",
        "for step in range(num_steps):\n",
        "\n",
        "    # input (encoder) unrolling length: 40\n",
        "    # output (decoder) unrolling length: 60\n",
        "    if (step+1)%100==0:\n",
        "        print('.',end='')\n",
        "\n",
        "    # Sample a random batch of IDs from training data\n",
        "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
        "    \n",
        "    # Getting an unrolled set of data batches for the encoder\n",
        "    eu_data, _, _ = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
        "    \n",
        "    # Getting an unrolled set of data batches for the decoder\n",
        "    du_data, du_labels, _ = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
        "    \n",
        "    # Train for single step\n",
        "    l, tr_pred = train_single_step(eu_data, du_data, du_labels)\n",
        "    \n",
        "    # We don't calculate BLEU scores all the time as this is expensive, \n",
        "    # it slows down the code\n",
        "    if np.random.random()<0.1:\n",
        "        \n",
        "        # all_labels are labels obtained by concatinating all the labels in batches\n",
        "        all_labels = np.argmax(np.concatenate(du_labels,axis=0),axis=1)\n",
        "        # all_preds are predictions for all unrolled steps\n",
        "        all_preds = np.argmax(tr_pred,axis=1)\n",
        "        \n",
        "        # Get training BLEU candidates and references\n",
        "        batch_cands, batch_refs = create_bleu_ref_candidate_lists(all_preds, all_labels)\n",
        "        \n",
        "        # Accumulate training candidates/references for calculating\n",
        "        # BLEU later\n",
        "        train_bleu_refs.extend(batch_refs)\n",
        "        train_bleu_cands.extend(batch_cands)\n",
        "\n",
        "    if (step+1)%500==0:  \n",
        "        \n",
        "        # Writing actual and predicte data to train_prediction.txt file for some random sentence\n",
        "        print('Step ',step+1)\n",
        "        with open(os.path.join(log_dir, train_prediction_text_fname),'a') as fa:                                \n",
        "            fa.write('============= Step ' +  str(step+1) + ' =============\\n') \n",
        "        \n",
        "        rand_idx = np.random.randint(low=1,high=batch_size)\n",
        "        print_and_save_train_predictions(du_labels, tr_pred, rand_idx, train_prediction_text_fname)        \n",
        "        \n",
        "        # Calculating the BLEU score for the accumulated candidates/references\n",
        "        bscore = 0.0\n",
        "        bscore = corpus_bleu(train_bleu_refs,train_bleu_cands,smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
        "        train_bleu_scores_over_time.append(bscore)\n",
        "        print('(Train) BLEU (%d elements): '%(len(train_bleu_refs)),bscore)\n",
        "        \n",
        "        # Reset the candidate/reference accumulators\n",
        "        train_bleu_refs, train_bleu_cands = [],[]\n",
        "        \n",
        "        # Write BLEU score to file\n",
        "        with open(log_dir + os.sep +'blue_scores.txt','a') as fa_bleu:\n",
        "            fa_bleu.write(str(step+1) +','+str(bscore)+'\\n')\n",
        "        \n",
        "        with open(os.path.join(log_dir, train_prediction_text_fname),'a') as fa:                \n",
        "            fa.write('(Train) BLEU: %.5f\\n'%bscore)        \n",
        "        \n",
        "    avg_loss += l  # Update average loss\n",
        "    \n",
        "    sess.run(reset_train_state) # Resetting hidden state for each batch\n",
        "    \n",
        "    # ============================= TEST PHASE ==================================\n",
        "    if (step+1)%1000==0:\n",
        "        # Calculate average loss\n",
        "        print('============= Step ', str(step+1), ' =============')\n",
        "        print('\\t Loss: ',avg_loss/1000.0)\n",
        "        loss_over_time.append(avg_loss/1000.0)\n",
        "        \n",
        "        # Write losses to file\n",
        "        with open(log_dir + os.sep + 'losses.txt','a') as fa_loss:\n",
        "            fa_loss.write(str(step+1) +','+str(avg_loss/1000.0)+'\\n')\n",
        "        \n",
        "        with open(os.path.join(log_dir, train_prediction_text_fname),'a') as fa:                                \n",
        "            fa.write('============= Step ' +  str(step+1) + ' =============\\n') \n",
        "            fa.write('\\t Loss: %.5f\\n'%(avg_loss/1000.0))\n",
        "            \n",
        "        avg_loss = 0.0\n",
        "        \n",
        "        # Increase gstep to decay learning rate\n",
        "        sess.run(inc_gstep)\n",
        "        \n",
        "        # reset global step when we change the optimizer\n",
        "        if (step+1)==20000: \n",
        "            sess.run(reset_gstep)\n",
        "        \n",
        "        print('=====================================================')\n",
        "        print('(Test) Translating test sentences ...')        \n",
        "\n",
        "        print('Processing test data ... ')\n",
        "        \n",
        "        # ===================================================================================\n",
        "        # Predictions for Test data\n",
        "        for in_i in range(test_inputs.shape[0]//batch_size):\n",
        "            \n",
        "            # Generate encoder / decoder data for testing data\n",
        "            test_eu_data, test_eu_labels, _ = test_enc_data_generator.unroll_batches(sent_ids=np.arange(in_i*batch_size,(in_i+1)*batch_size))\n",
        "            test_du_data, test_du_labels, _ = test_dec_data_generator.unroll_batches(sent_ids=np.arange(in_i*batch_size,(in_i+1)*batch_size))\n",
        "            \n",
        "            # fill the feed dict\n",
        "            feed_dict = {}\n",
        "            for ui,(dat,lbl) in enumerate(zip(test_eu_data,test_eu_labels)):            \n",
        "                feed_dict[enc_test_input[ui]] = dat             \n",
        "\n",
        "            # Get predictions out with decoder          \n",
        "            # run prediction calculation this returns a list of prediction dec_num_unrollings long\n",
        "            test_pred_unrolled = sess.run(test_predictions, feed_dict=feed_dict)\n",
        "            \n",
        "            # We print a randomly selected sample from each batch\n",
        "            test_rand_idx = np.random.randint(0,batch_size) # used for printing test output\n",
        "            \n",
        "            print_and_save_test_predictions(test_du_labels, test_pred_unrolled, in_i, test_rand_idx, test_prediction_text_fname)\n",
        "            \n",
        "            # Things required to calculate test BLEU score\n",
        "            all_labels = np.argmax(np.concatenate(test_du_labels,axis=0),axis=1)\n",
        "            all_preds = np.concatenate(test_pred_unrolled, axis=0)\n",
        "            batch_cands, batch_refs = create_bleu_ref_candidate_lists(all_preds, all_labels)\n",
        "            test_bleu_refs.extend(batch_refs)\n",
        "            test_bleu_cands.extend(batch_cands)\n",
        "            \n",
        "            # Reset the test state\n",
        "            sess.run(reset_test_state)\n",
        "        \n",
        "        # Calculate test BLEU score\n",
        "        test_bleu_score = 0.0\n",
        "        test_bleu_score = corpus_bleu(test_bleu_refs,test_bleu_cands,\n",
        "                                      smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
        "        test_bleu_scores_over_time.append(test_bleu_score)\n",
        "        print('(Test) BLEU (%d elements): '%(len(test_bleu_refs)),test_bleu_score)\n",
        "        \n",
        "        test_bleu_refs, test_bleu_cands = [],[]        \n",
        "        print('=====================================================')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started Training\n",
            ".....Step  500\n",
            "Actual: Joining France &apos;s &quot; Big Three &quot; in the NBA are four more young talents : <unk> <unk> of the L <unk> . Lakers ; <unk> <unk> of the Denver <unk> ; and Johan <unk> and <unk> <unk> of the Seattle <unk> . </s> \n",
            "\n",
            "Predicted: The <unk> , <unk> <unk> , <unk> , the <unk> , the , , , , the , , the <unk> , , </s> \n",
            "(Train) BLEU (510 elements):  0.06707996439724499\n",
            ".....Step  1000\n",
            "Actual: The guest reviews are submitted by our customers after their stay at The Oriental Siam Resort . </s> \n",
            "\n",
            "Predicted: When guest reviews are submitted by our customers after their stay at <unk> <unk> Hotel . . </s> \n",
            "(Train) BLEU (490 elements):  0.17890726789378786\n",
            "============= Step  1000  =============\n",
            "\t Loss:  1.211373913615942\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Der nordwestliche Teil der Insel besteht aus Granit und Gneis , von Ton überlagert , und bildet eine ca.\n",
            "\n",
            "\t EN (TRUE):A battle between Denmark and Sweden in 1645 led to Swedish control of the island , but it was brief - they left again the same year . In the Roskildepeace of 1658 Bornholm , Skaane , Halland and Blekinge were given to Sweden .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> is a <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Tarbet Gast ist Haus im ersten Nationalpark von Schottland aufgestellt und hat eine gehobene Position hoch über dem Dorf von Tarbet und genießt spektakuläre südliche Blicke Bucht Lomond hinunter und nach der westlichen Seite von Ben Lomond .\n",
            "\n",
            "\t EN (TRUE):Tarbet Guest House is situated In Scotland ’ s first National Park and has an elevated position high above the village of Tarbet and enjoys spectacular southerly views down Loch Lomond and towards the western side of Ben Lomond .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Private Parkplätze stehen für EUR 3,50 pro Tag in der Nähe zur Verfügung .\n",
            "\n",
            "\t EN (TRUE):Private parking is possible at a location nearby and costs EUR 3.50 per day .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> is a <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Im Allgemeinen basieren sie auf Datenbanken , Templates und Skripts .\n",
            "\n",
            "\t EN (TRUE):In general they are based on databases , template and scripts .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The guest reviews are submitted by our customers after their stay at the <unk> <unk> . </s> \n",
            "\n",
            "DE:  Genießen Sie das ganze Jahr über die Sonne und erfrischen Sie sich im Außenpool , während Ihre Kinder sicher in ihren eigenen Schwimmbecken spielen .\n",
            "\n",
            "\t EN (TRUE):Soak up the year ##AT##-##AT## round sunshine as you enjoy a dip in one of the outdoor swimming pools , as children play safely in their own pools .\n",
            "\n",
            "\n",
            "\t EN (Predicted): Choose the hotel is a few to the hotel of the <unk> <unk> . </s> \n",
            "\n",
            "DE:  Karibische Küche gut . Kaum bekannt - bis jetzt !\n",
            "\n",
            "\t EN (TRUE):Can get quite dodgy at night .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Wünschen Sie Unterstützung bei der der Zentrensuche ?\n",
            "\n",
            "\t EN (TRUE):Would you like being assisted in searching a specialised centre ?\n",
            "\n",
            "\n",
            "\t EN (Predicted): Choose the hotel is a few to stay at the <unk> <unk> . </s> \n",
            "\n",
            "DE:  Tux Racer wird Ihnen helfen , die Zeit totzuschlagen und sie können OpenOffice zum Arbeiten verwenden .\n",
            "\n",
            "\t EN (TRUE):Tux Racer will help you pass the time while you wait , and you can use OpenOffice for work .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Sie können hier auch Kanufahren , Windsurfen und Tauchen ...\n",
            "\n",
            "\t EN (TRUE):Here , you can also practice aquatic sports such as yachting , windsurfing and canoeing ... you will find all kinds of water channels , from wild brooks to serene lakes .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The hotel is a few to stay at the <unk> <unk> . </s> \n",
            "\n",
            "DE:  in dieser Option ermöglicht , Dateien relativ zum aktuellen Verzeichnis einzubinden .\n",
            "\n",
            "\t EN (TRUE):in the include path allows for relative includes as it means the current directory . However , it is more efficient to explicitly use include &apos; . / file &apos; than having PHP always check the current directory for every include .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> is a <unk> <unk> . </s> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.08612985307189644\n",
            "=====================================================\n",
            ".....Step  1500\n",
            "Actual: The LateRooms rates for Hart &apos;s in Nottingham are the total price of the room and not the &apos; per person &apos; rate . </s> \n",
            "\n",
            "Predicted: The guest rates for the Hotel Hotel the Hotel the total of of the room . costs the &apos; per person . rate . </s> \n",
            "(Train) BLEU (610 elements):  0.2242501446420087\n",
            ".....Step  2000\n",
            "Actual: from € 103 from € 100 from <unk> . </s> \n",
            "\n",
            "Predicted: Private the : <unk> the <unk> <unk> the . </s> \n",
            "(Train) BLEU (490 elements):  0.2447520384386851\n",
            "============= Step  2000  =============\n",
            "\t Loss:  1.0192920450866223\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Der nordwestliche Teil der Insel besteht aus Granit und Gneis , von Ton überlagert , und bildet eine ca.\n",
            "\n",
            "\t EN (TRUE):A battle between Denmark and Sweden in 1645 led to Swedish control of the island , but it was brief - they left again the same year . In the Roskildepeace of 1658 Bornholm , Skaane , Halland and Blekinge were given to Sweden .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> <unk> is a small and <unk> , the <unk> <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Google nimmt niemals Geld für die Einbeziehung oder das Ranking von Websites und die Schaltung in den indexbasierten Suchergebnissen ist kostenlos .\n",
            "\n",
            "\t EN (TRUE):Google never accepts money to include or rank sites in our search results , and it costs nothing to appear in our organic search results .\n",
            "\n",
            "\n",
            "\t EN (Predicted): If you can be used to be used to be used to be used to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to \n",
            "\n",
            "DE:  Leicht und ergonomisch gebaut , mit einer Hand zu bedienen , stellen diese Messgeräte eine wirtschaftliche Lösung dar , wenn bei Verdacht auf Wanddickenverlust schnell geprüft werden soll .\n",
            "\n",
            "\t EN (TRUE):Lightweight and ergonomically designed for easy one ##AT##-##AT## hand operation , these gages provide cost ##AT##-##AT## effective measurement solutions in many applications that require quick inspection of materials suspected of metal wall thinning .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Eine Woche später wird Dianne Feinstein , Vorstandsvorsitzende der Inspektoren , als Nachfolgerin Moscones ernannt . Sie ist die erste Bürgermeisterin der Stadt .\n",
            "\n",
            "\t EN (TRUE):Returning by the parallel Stockton or Powell will give you a better feeling of the day to day life of the residents , and are both good for those looking for imported commodities such as tea or herbs .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> \n",
            "\n",
            "DE:  Die Prüfgeräte von Olympus erweitern den Bereich des menschlichen Auges bei der industriellen Sichtprüfung . Mit unseren Industrieendoskopen werden verdeckte Bereiche mit beschränktem Zugang sichtbar gemacht , wie z.\n",
            "\n",
            "\t EN (TRUE):Olympus test equipment expands the range of the human eye in industrial visual inspection .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Booking.com : Best Western Hotell SöderH , Söderhamn , Schweden - 29 Gästebewertungen .\n",
            "\n",
            "\t EN (TRUE):Booking.com : Best Western Hotell SöderH , Söderhamn , Sweden - 29 Guest reviews .\n",
            "\n",
            "\n",
            "\t EN (Predicted): Booking .com .com : Hotel Hotel Hotel - <unk> , <unk> . </s> \n",
            "\n",
            "DE:  Auch ist , so denkt Dr. Gutherz , bereits die erste Seite sehr viel versprechend , da sie eine Definition des klinischen Psychotrauma ##AT##-##AT## Begriffes enthält , der er gänzlich zustimmen kann .\n",
            "\n",
            "\t EN (TRUE):At the rhetorical climax of this summary , Dr Goodheart comes across some sentences expressed with great pathos .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> <unk> is a small and <unk> , the <unk> <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Alle älteren Kinder oder Erwachsene zahlen EUR 32,00 pro Übernachtung und Person für Zustellbetten .\n",
            "\n",
            "\t EN (TRUE):All older children or adults are charged EUR 32.00 per night and person for extra beds .\n",
            "\n",
            "\n",
            "\t EN (Predicted): All children are charged by charged are charged EUR per night and person . </s> \n",
            "\n",
            "DE:  Zimmerbeschreibung : Our Castle Deluxe Rooms are traditionally themed with rich luxurious fabrics and furnishings , many with excellent views over the Castle grounds .\n",
            "\n",
            "\t EN (TRUE):Room Notes : Our Castle Deluxe Rooms are traditionally themed with rich luxurious fabrics and furnishings , many with excellent views over the Castle grounds .\n",
            "\n",
            "\n",
            "\t EN (Predicted): Description Description : Hotel Hotel Hotel Hotel in the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> . </s> \n",
            "\n",
            "DE:  William Gross beschreibt die Bedeutung des Manuskripts für die Sammlung der Familie Gross .\n",
            "\n",
            "\t EN (TRUE):William Gross describes the significance of the manuscript to the Gross Family Collection .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.11469513464172182\n",
            "=====================================================\n",
            ".....Step  2500\n",
            "Actual: We offer special “ Woman style ” rooms including a special toiletries kit with everything a woman might need during her stay . </s> \n",
            "\n",
            "Predicted: This have a for for and , , , a single range , , a , wide . be to the stay . </s> \n",
            "(Train) BLEU (510 elements):  0.26995642177447443\n",
            ".....Step  3000\n",
            "Actual: Popular queries ( 1 ##AT##-##AT## 8 ) : convert pdf to doc &#124; <unk> to mp3 &#124; add &#124; convert jpeg to mpeg &#124; mp3 to track converter &#124; voice changer male to female &#124; password recovery &#124; video converter avi <unk> &#124; More queries . . . </s> \n",
            "\n",
            "Predicted: The , , <unk> <unk> <unk> ) , <unk> to , the <unk> <unk> , the <unk> <unk> <unk> <unk> to to the <unk> <unk> <unk> <unk> the . <unk> , to to the <unk> <unk> , . <unk> , . . . <unk> . . </s> \n",
            "(Train) BLEU (510 elements):  0.2766734706899857\n",
            "============= Step  3000  =============\n",
            "\t Loss:  0.9359848894476891\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Ideale Lage für Exkursionen in die Stadt und Nähe zur Promenade .\n",
            "\n",
            "\t EN (TRUE):There was plenty of space in the room and a nice garden to sit and have a drink and smoke .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Google nimmt niemals Geld für die Einbeziehung oder das Ranking von Websites und die Schaltung in den indexbasierten Suchergebnissen ist kostenlos .\n",
            "\n",
            "\t EN (TRUE):Google never accepts money to include or rank sites in our search results , and it costs nothing to appear in our organic search results .\n",
            "\n",
            "\n",
            "\t EN (Predicted): If you can use the <unk> , you can use the <unk> , and the <unk> . </s> \n",
            "\n",
            "DE:  Es existieren Busverbindungen in nahezu jeden Ort der Provence ( eventuell mit Umsteigen in Aix ##AT##-##AT## en ##AT##-##AT## Provence ) , allerdings sollte beachtet werden , dass die letzten Busse abends ca. um 19 Uhr fahren .\n",
            "\n",
            "\t EN (TRUE):As always in France those highways are expensive but practical , comfortable and fast .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> is a <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Wie hilfreich finden Sie die Demo ##AT##-##AT## CD ?\n",
            "\n",
            "\t EN (TRUE):How helpful do you find the demo CD ##AT##-##AT## ROM ?\n",
            "\n",
            "\n",
            "\t EN (Predicted): Can you can use the <unk> . </s> \n",
            "\n",
            "DE:  Genießen Sie das ganze Jahr über die Sonne und erfrischen Sie sich im Außenpool , während Ihre Kinder sicher in ihren eigenen Schwimmbecken spielen .\n",
            "\n",
            "\t EN (TRUE):Soak up the year ##AT##-##AT## round sunshine as you enjoy a dip in one of the outdoor swimming pools , as children play safely in their own pools .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The hotel is a great location for the hotel , the hotel is located in the heart of the city centre . </s> \n",
            "\n",
            "DE:  Das Haus liegt in der CCZ ##AT##-##AT## Umweltzone und bietet eine sehr gute Anbindung an das Bus- und U ##AT##-##AT## Bahnnetz .\n",
            "\n",
            "\t EN (TRUE):Set inside the central London congestion ##AT##-##AT## charging zone , this modern hotel has superb transport links , with access to the Tube and the bus network practically on the doorstep .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The hotel is located in the heart of the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Hotelparkplätze sind gegen eine kleine Gebühr vorhanden .\n",
            "\n",
            "\t EN (TRUE):Car Parking is available at the hotel at a small charge . ( check in advance as spaces are limited and certain conditions apply ) .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> is a bit of the hotel . </s> \n",
            "\n",
            "DE:  Kosten Sie mediterrane Gerichte im preisgekrönten Restaurant Molyvos .\n",
            "\n",
            "\t EN (TRUE):Enjoy award winning Mediterranean cuisine at Molyvos .\n",
            "\n",
            "\n",
            "\t EN (Predicted): Start your stay at the <unk> <unk> . </s> \n",
            "\n",
            "DE:  Sie können hier auch Kanufahren , Windsurfen und Tauchen ...\n",
            "\n",
            "\t EN (TRUE):Here , you can also practice aquatic sports such as yachting , windsurfing and canoeing ... you will find all kinds of water channels , from wild brooks to serene lakes .\n",
            "\n",
            "\n",
            "\t EN (Predicted): You can find the hotel in the heart of the city . </s> \n",
            "\n",
            "DE:  Die Musikant entspricht dem Folkrock . Dennoch finden sich in den Liedern viele musikalische Elemente aus klassisch- folklorischer Liedern aus Anatolien bis zum Mittelmeer , aus Latein- amerikanischen Märschen bis hin zu klassischen Rockklängen .\n",
            "\n",
            "\t EN (TRUE):Yorum continues to sing hopeful songs in the name of all the world &apos;s revolutionary music .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The <unk> <unk> is a <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.11837799608244191\n",
            "=====================================================\n",
            ".....Step  3500\n",
            "Actual: ) Finally , we send our matches to our <unk> helper . </s> \n",
            "\n",
            "Predicted: <unk> is , the have the information that be website . . </s> \n",
            "(Train) BLEU (570 elements):  0.28805373871379264\n",
            ".....Step  4000\n",
            "Actual: So far 15 <unk> plant species , over 1000 bird species and 200 species of mammals have been identified in Manu . </s> \n",
            "\n",
            "Predicted: <unk> , from years , , , the the years , of the years of the . been a by the . </s> \n",
            "(Train) BLEU (550 elements):  0.2988325853821862\n",
            "============= Step  4000  =============\n",
            "\t Loss:  0.8884063443243504\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Heute verstehen sich QuarkXPress ® 8 , Photoshop ® und Illustrator ® besser als jemals zuvor . Dank HTML und CSS ­ können Anwender von QuarkXPress inzwischen alle Medien bedienen , und das unabhängig von Anwendungen der Adobe ® Creative Suite ® wie Adobe Flash ® ( SWF ) und Adobe Dreamweaver ® .\n",
            "\n",
            "\t EN (TRUE):Today , QuarkXPress ® 8 has tighter integration with Photoshop ® and Illustrator ® than ever before , and through standards like HTML and CSS , QuarkXPress users can publish across media both independently and alongside Adobe ® Creative Suite ® applications like Adobe Flash ® ( SWF ) and Adobe Dreamweaver ® .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> is a new <unk> , and the <unk> <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , and <unk> , <unk> , \n",
            "\n",
            "DE:  In dem traditionellen Fischerdorf werden ausländische Gäste sehr herzlich empfangen . Seien Sie allerdings gewarnt vor dem Zustand der Strasse und fahren Sie vorsichtig !\n",
            "\n",
            "\t EN (TRUE):It &apos;s a small traditional fishing village , with friendly locals , always very welcoming to British tourists .\n",
            "\n",
            "\n",
            "\t EN (Predicted): In the summer , the hotel is very good and the hotel is very good . </s> \n",
            "\n",
            "DE:  Niedrigere Preise durch mehr Wettbewerb . Die Kosten für Kapital können durch Währungsstabilität , niedrigere Zinssätze und eine bessere Organisation der Kapitalmärkte gesenkt werden .\n",
            "\n",
            "\t EN (TRUE):In a knowledge ##AT##-##AT## based society the opportunity of education is the key to progress and equality and sustainability .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> is a lot of the <unk> , and the <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Ferienwohnungen erste Strandlinie . Dachwohnung in Conil de la Frontera , Cadiz .\n",
            "\n",
            "\t EN (TRUE):Located at the foot of the beach , this Conil beach apartment rentals , Spain is perfect for your summer vacation in Conil de la Frontera .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Das „ Ladino di Fassa “ ist jedoch mehr als ein Dialekt – es ist eine richtige Sprache .\n",
            "\n",
            "\t EN (TRUE):This is Ladin from Fassa which is more than a dialect : it is a language in its own right .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Karibische Küche gut . Kaum bekannt - bis jetzt !\n",
            "\n",
            "\t EN (TRUE):Can get quite dodgy at night .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Hotelparkplätze sind gegen eine kleine Gebühr vorhanden .\n",
            "\n",
            "\t EN (TRUE):Car Parking is available at the hotel at a small charge . ( check in advance as spaces are limited and certain conditions apply ) .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a lot of the hotel . </s> \n",
            "\n",
            "DE:  Tux Racer wird Ihnen helfen , die Zeit totzuschlagen und sie können OpenOffice zum Arbeiten verwenden .\n",
            "\n",
            "\t EN (TRUE):Tux Racer will help you pass the time while you wait , and you can use OpenOffice for work .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> is a <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Obwohl das Nazi ##AT##-##AT## Regime die Buddhistische Gemeinde in Berlin , die seit 1936 aktiv gewesen war , schloss und kurzzeitig deren Begründer Martin Steinke 1941 inhaftierte , verfolgte es die Buddhisten nicht generell .\n",
            "\n",
            "\t EN (TRUE):Although the Nazi regime closed the Buddhistische Gemeinde ( Buddhist Society ) in Berlin , which had been active from 1936 , and briefly arrested its founder Martin Steinke in 1941 , they generally did not persecute Buddhists .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> is a <unk> <unk> , and the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  Ziel von 50 ##AT##-##AT## Hand Video Poker ist ähnlich zu dem von Video Poker : eine Pokerhand mit fünf Karten , die mindestens die niedrigste Kombination von dem Spieltisch , an dem Sie sich gerade befinden , zu erlangen .\n",
            "\n",
            "\t EN (TRUE):The object of 50 ##AT##-##AT## Hand Video Poker is similar to Video Poker , to obtain a five ##AT##-##AT## card poker hand that contains at least the lowest combination on the pay table for the version you are playing .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The hotel is located in the heart of the city , the hotel is located in the heart of the city , the hotel is located in the heart of the city . </s> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.12732217952865613\n",
            "=====================================================\n",
            ".....Step  4500\n",
            "Actual: This cosy and charming place is situated next to <unk> beach in the centre of Torremolinos . </s> \n",
            "\n",
            "Predicted: The is hotel comfortable hotel in situated in to the , , the heart of the . </s> \n",
            "(Train) BLEU (430 elements):  0.2897695781066161\n",
            ".....Step  5000\n",
            "Actual: Situated in the heart of Athens within the famous <unk> Square , this renovated hotel features a warm atmosphere , comfortable accommodation and a wonderful outlook over the city . </s> \n",
            "\n",
            "Predicted: The in the heart of the , walking city sights district , the hotel hotel offers a modern and , a and in a comfortable location in the city centre </s> \n",
            "(Train) BLEU (460 elements):  0.29219756355680887\n",
            "============= Step  5000  =============\n",
            "\t Loss:  0.9035324329435825\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Das Hotel Sempione verfügt über eine ideale , ruhige Lage in einem geschäftigen Viertel mit guter Verkehrsanbindung . Der Bahnhof und eine U ##AT##-##AT## Bahnstation liegen in der Nähe .\n",
            "\n",
            "\t EN (TRUE):Hotel Sempione welcomes you to a busy yet quiet area of Milan , within walking distance of excellent transport links , including the central railway station and the Repubblica metro station .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Man schließt die Gitarre über Mikrofon oder Pickup an die PC Soundkarte an und schon kann es losgehen .\n",
            "\n",
            "\t EN (TRUE):You connect the guitar via microphone or pickup with the PC soundcard and you are ready !\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Private Parkplätze stehen für EUR 3,50 pro Tag in der Nähe zur Verfügung .\n",
            "\n",
            "\t EN (TRUE):Private parking is possible at a location nearby and costs EUR 3.50 per day .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  15. einem Dritten bei dem Verstoss gegen eine dieser Regeln zu helfen .\n",
            "\n",
            "\t EN (TRUE):15. assist any third party in engaging in any activity prohibited by these Terms .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Genießen Sie das ganze Jahr über die Sonne und erfrischen Sie sich im Außenpool , während Ihre Kinder sicher in ihren eigenen Schwimmbecken spielen .\n",
            "\n",
            "\t EN (TRUE):Soak up the year ##AT##-##AT## round sunshine as you enjoy a dip in one of the outdoor swimming pools , as children play safely in their own pools .\n",
            "\n",
            "\n",
            "\t EN (Predicted): If you have a <unk> <unk> <unk> , the <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  Karibische Küche gut . Kaum bekannt - bis jetzt !\n",
            "\n",
            "\t EN (TRUE):Can get quite dodgy at night .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Zusätzlich enthält TBarCode / SAPwin eine Menge neuer Strichcode ##AT##-##AT## Symbologien .\n",
            "\n",
            "\t EN (TRUE):In addition TBarCode / SAPwin comes with a bunch of new bar code symbologies .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , and <unk> . </s> \n",
            "\n",
            "DE:  With a unique location in the heart of Peneda / Gerês National Park , this Pousada has a breathking view over the river Cávado and the peaceful Caniçada dam .\n",
            "\n",
            "\t EN (TRUE):Located in the heart of Peneda ##AT##-##AT## Gerês National Park , this guest house boasts panoramic views of the surrounding mountains and is a welcome retreat for nature enthusiasts .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Jeder Wikitraveler kann Artikel verändern , neue Seiten erstellen und sogar Informationen über die Seite selbst überschreiben .\n",
            "\n",
            "\t EN (TRUE):Any Wikitraveller can change articles , rewrite navigation areas , even overwrite information about the site itself .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  Es gibt in der Nähe kein Stadtzentrum , in dem man abends beruhigt spazieren gehen könnte . Zu Fuß braucht man in das Zentrum Bournemouths 25 min .\n",
            "\n",
            "\t EN (TRUE):Also , the possibility of a small corner play area for kids in restuarant so that adults can eat in peace and with peace of mind .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.07027555723961475\n",
            "=====================================================\n",
            ".....Step  5500\n",
            "Actual: Everyone who takes a look round Europe knows that it is pulsating , working , functioning across yesterday &apos;s frontier , almost as self ##AT##-##AT## evidently as if there had never been a division at all . </s> \n",
            "\n",
            "Predicted: The is was the new at at , the the is the and and in and and the , <unk> , and a a ##AT##-##AT## <unk> , the they are a been a <unk> of the the </s> \n",
            "(Train) BLEU (450 elements):  0.32231440311711174\n",
            ".....Step  6000\n",
            "Actual: “ Working together with Advantage CFD we were able to develop and test a new high ##AT##-##AT## <unk> wing much quicker than we would have done by the standard <unk> methods , ” says George Howard ##AT##-##AT## <unk> , team principal for <unk> Racing . </s> \n",
            "\n",
            "Predicted: The The &quot; , the <unk> not have not to be the the the <unk> and ##AT##-##AT## quality ##AT##-##AT## ##AT##-##AT## of and the have be to the the <unk> of , of and <unk> the . . <unk> . and , and the . <unk> </s> \n",
            "(Train) BLEU (560 elements):  0.3004375622108549\n",
            "============= Step  6000  =============\n",
            "\t Loss:  0.900384347975254\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Das Hotel Sempione verfügt über eine ideale , ruhige Lage in einem geschäftigen Viertel mit guter Verkehrsanbindung . Der Bahnhof und eine U ##AT##-##AT## Bahnstation liegen in der Nähe .\n",
            "\n",
            "\t EN (TRUE):Hotel Sempione welcomes you to a busy yet quiet area of Milan , within walking distance of excellent transport links , including the central railway station and the Repubblica metro station .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  18 Denn siehe , er richtet , und sein Richterspruch ist gerecht ; und das Kleinkind , das im Kindesalter stirbt , geht nicht zugrunde ; aber die Menschen trinken Verdammnis für ihre eigene Seele , außer sie demütigen sich und a werden so wie kleine Kinder und glauben daran , daß die Errettung im b sühnenden Blut Christi , des Herrn , des Allmächtigen , und durch dasselbe war und ist und sein wird .\n",
            "\n",
            "\t EN (TRUE):18 For behold he judgeth , and his judgment is just ; and the infant perisheth not that dieth in his infancy ; but men drink a damnation to their own souls except they humble themselves and b become as little children , and believe that c salvation was , and is , and is to come , in and through the d atoning blood of Christ , the Lord Omnipotent .\n",
            "\n",
            "\n",
            "\t EN (Predicted): 10 And I will not be a b b b b <unk> , and the <unk> of the Lord , and the <unk> of the Lord , and the <unk> of the Lord , and the <unk> of the Lord , and the <unk> of the Lord , and the <unk> of the Lord shall be b d . </s> \n",
            "\n",
            "DE:  Leicht und ergonomisch gebaut , mit einer Hand zu bedienen , stellen diese Messgeräte eine wirtschaftliche Lösung dar , wenn bei Verdacht auf Wanddickenverlust schnell geprüft werden soll .\n",
            "\n",
            "\t EN (TRUE):Lightweight and ergonomically designed for easy one ##AT##-##AT## hand operation , these gages provide cost ##AT##-##AT## effective measurement solutions in many applications that require quick inspection of materials suspected of metal wall thinning .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> , the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> of the <unk> . </s> \n",
            "\n",
            "DE:  Wenn eine Speicherung der Daten auf dem Client erfolgen soll , werden Cookys verwendet .\n",
            "\n",
            "\t EN (TRUE):When client data storage is needed , cookies are used .\n",
            "\n",
            "\n",
            "\t EN (Predicted): If you have to be a problem , you can download the user ##AT##-##AT## mail ##AT##-##AT## mail ##AT##-##AT## mail ##AT##-##AT## mail ##AT##-##AT## mail ##AT##-##AT## <unk> . </s> \n",
            "\n",
            "DE:  Die drei GewinnerInnen jeder Kategorie - insgesamt 12 SchülerInnen in Begleitung ihrer koordinierenden Lehrperson - werden zur &quot; Energie ist unsere Zukunft &quot; Preisverleihung nach Brüssel eingeladen .\n",
            "\n",
            "\t EN (TRUE):The top three winners of each category , a total of 12 , together with their coordinating teachers , will be rewarded with a trip to Brussels to attend the “ Energy is our Future ” Awards ceremony .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , and <unk> . </s> \n",
            "\n",
            "DE:  Baustelle zwischen See und Hotel . Altmodische Einrichtung .\n",
            "\n",
            "\t EN (TRUE):Shared lobby with campsite next door , apparently , and hotel check in / out were not handled by lobby staff but by restaurant staff .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Zusätzlich enthält TBarCode / SAPwin eine Menge neuer Strichcode ##AT##-##AT## Symbologien .\n",
            "\n",
            "\t EN (TRUE):In addition TBarCode / SAPwin comes with a bunch of new bar code symbologies .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , and <unk> . </s> \n",
            "\n",
            "DE:  Das Cleddau Bridge Hotel ist der ideale Platz um zu entspannen oder geschäftlich zu reisen .\n",
            "\n",
            "\t EN (TRUE):Cleddau Bridge hotel is the ideal place for those who want a relaxing holiday or who travel for business .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Einst verwunschene Eilande , später Schlupfwinkel von Piraten , welche die goldbeladenen Schiffe der Spanier überfielen , sind diese unwirtlichen Inseln mit dem seltsamen Tierleben heute wohl eines der letzten großen Tierparadiese der Welt .\n",
            "\n",
            "\t EN (TRUE):Apart from its beautiful beaches and unique and varied ecosystems , the Galapagos Islands are home to towering active volcanoes that reach altitudes up to 1,600 meters .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Das Personal war immer hilfsbereit und freundlich .\n",
            "\n",
            "\t EN (TRUE):The location and helpfulness of staff was excellent .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.0658953653252199\n",
            "=====================================================\n",
            ".....Step  6500\n",
            "Actual: The guest reviews are submitted by our customers after their stay at Thon Hotel Maritim . </s> \n",
            "\n",
            "Predicted: The guest reviews are submitted by our customers after their stay at Hotel Hotel <unk> Hotel </s> \n",
            "(Train) BLEU (520 elements):  0.3084956052544234\n",
            ".....Step  7000\n",
            "Actual: After breakfast you can do some exercise in the Mare Nostrum hotel , working out in the fitness suite , or take a dip in the outdoor swimming pool . </s> \n",
            "\n",
            "Predicted: The a , can enjoy you of , the hotel <unk> <unk> , you in of the city centre . a a the dip in the city pool pool . </s> \n",
            "(Train) BLEU (560 elements):  0.3066591431126809\n",
            "============= Step  7000  =============\n",
            "\t Loss:  0.8772332340627909\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Der nordwestliche Teil der Insel besteht aus Granit und Gneis , von Ton überlagert , und bildet eine ca.\n",
            "\n",
            "\t EN (TRUE):A battle between Denmark and Sweden in 1645 led to Swedish control of the island , but it was brief - they left again the same year . In the Roskildepeace of 1658 Bornholm , Skaane , Halland and Blekinge were given to Sweden .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Das Hotel Opera befindet sich in der Nähe des Royal Theatre , Kongens Nytorv , &apos; Stroget &apos; und Nyhavn .\n",
            "\n",
            "\t EN (TRUE):Hotel Opera is situated near The Royal Theatre , Kongens Nytorv , &quot; Strøget &quot; and fascinating Nyhavn .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> Hotel <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Residenz City Lodge befindet sich am mandelförmigen Sweelinckplein im Herzen des schicken Duinoord ##AT##-##AT## Viertels in Den Haag .\n",
            "\n",
            "\t EN (TRUE):In the midst of the bustling city life , an oasis of peace and luxury can be found bordering the beautiful Haagsche Park , across from the main train station .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> Hotel <unk> is a great location in the heart of the city , the hotel offers a wide range of facilities and a great location . </s> \n",
            "\n",
            "DE:  shower was ok but leaked needed updating .\n",
            "\n",
            "\t EN (TRUE):the response to to requests was poor , phone 3 time for milk in the room over 4 hours .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Genießen Sie das ganze Jahr über die Sonne und erfrischen Sie sich im Außenpool , während Ihre Kinder sicher in ihren eigenen Schwimmbecken spielen .\n",
            "\n",
            "\t EN (TRUE):Soak up the year ##AT##-##AT## round sunshine as you enjoy a dip in one of the outdoor swimming pools , as children play safely in their own pools .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a great choice for a variety of the hotel , and the <unk> <unk> , the <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Nachfolgend sehen Sie die Gästebewertungen von Meliá Fernán González Boutique hotel .\n",
            "\n",
            "\t EN (TRUE):The guest reviews are submitted by our customers after their stay at Meliá Fernán González Boutique hotel .\n",
            "\n",
            "\n",
            "\t EN (Predicted): The guest reviews are submitted by our customers after their stay at Hotel <unk> . </s> \n",
            "\n",
            "DE:  Auch ist , so denkt Dr. Gutherz , bereits die erste Seite sehr viel versprechend , da sie eine Definition des klinischen Psychotrauma ##AT##-##AT## Begriffes enthält , der er gänzlich zustimmen kann .\n",
            "\n",
            "\t EN (TRUE):At the rhetorical climax of this summary , Dr Goodheart comes across some sentences expressed with great pathos .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  Das Athens Gate Hotel liegt unterhalb der Akropolis nur 100 m vom neuen Akropolis ##AT##-##AT## Museum entfernt .\n",
            "\n",
            "\t EN (TRUE):The Athens Gate Hotel rests under the Acropolis , just 100 metres from the new Acropolis museum .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> Hotel <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "\n",
            "DE:  Jeder Wikitraveler kann Artikel verändern , neue Seiten erstellen und sogar Informationen über die Seite selbst überschreiben .\n",
            "\n",
            "\t EN (TRUE):Any Wikitraveller can change articles , rewrite navigation areas , even overwrite information about the site itself .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  Es gibt in der Nähe kein Stadtzentrum , in dem man abends beruhigt spazieren gehen könnte . Zu Fuß braucht man in das Zentrum Bournemouths 25 min .\n",
            "\n",
            "\t EN (TRUE):Also , the possibility of a small corner play area for kids in restuarant so that adults can eat in peace and with peace of mind .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a great choice for a relaxing stay in the hotel , the hotel is a great choice for a relaxing stay . </s> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.0754670695994257\n",
            "=====================================================\n",
            ".....Step  7500\n",
            "Actual: Internet via modem is available in the business centre and costs EUR 4 <unk> per hour . </s> \n",
            "\n",
            "Predicted: Wired via modem is available in the hotel and and costs GBP 10 <unk> per hour . </s> \n",
            "(Train) BLEU (440 elements):  0.31163656809585455\n",
            ".....Step  8000\n",
            "Actual: Elisa concentrates on presenting an attractive , sleek and simple to use interface that makes it both easy and visually appealing to watch videos , listen to music , and browse pictures from a dedicated interface . </s> \n",
            "\n",
            "Predicted: The in in the , <unk> , the , the , the the for the it possible in to easy to to the the from and to the , and the to . the <unk> to . </s> \n",
            "(Train) BLEU (490 elements):  0.3121237220963449\n",
            "============= Step  8000  =============\n",
            "\t Loss:  0.8577944709062576\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  34 Diese a Worte sind wahr und treu ; darum übertretet sie nicht , und b nehmt auch nichts davon weg .\n",
            "\n",
            "\t EN (TRUE):34 These sayings are a true and faithful ; wherefore , transgress them not , neither b take therefrom .\n",
            "\n",
            "\n",
            "\t EN (Predicted): 13 And now I say unto you , that ye have a b b <unk> , and the b <unk> of the land of the land . </s> \n",
            "\n",
            "DE:  Google nimmt niemals Geld für die Einbeziehung oder das Ranking von Websites und die Schaltung in den indexbasierten Suchergebnissen ist kostenlos .\n",
            "\n",
            "\t EN (TRUE):Google never accepts money to include or rank sites in our search results , and it costs nothing to appear in our organic search results .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  Das ist viel einfacher ... Nein , streiten Sie nicht mit mir ... es ist einfacher ... ach , wie auch immer !\n",
            "\n",
            "\t EN (TRUE):This is far more easy ... no , don &apos;t argue with me ... it is easier ... ah whatever !\n",
            "\n",
            "\n",
            "\t EN (Predicted): I have to pay for the same time , but it was very good . </s> \n",
            "\n",
            "DE:  Die schlanke , einfache Oberfläche und die gute Performance machen es zum idealen Werkzeug , um dein Netbook ( oder normales Notebook ) in einen e ##AT##-##AT## Book Reader zu verwandeln .\n",
            "\n",
            "\t EN (TRUE):Its low resource use , simple interface and fast performance makes it the ideal tool to turn your netbook ( or regular laptop ) into an e ##AT##-##AT## book reader .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  Nach einigen Wanderwochen erreichten ich und Celina Warschau . Auf dem Weg zum jüdischen Komitee begegnete ich auf der Straße meinem Bruder !\n",
            "\n",
            "\t EN (TRUE):It turned out that Marek had jumped from the window of a train moving to Majdanek .\n",
            "\n",
            "\n",
            "\t EN (Predicted): In the case of the <unk> , the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> . </s> \n",
            "\n",
            "DE:  Wann möchten Sie im Leon &apos; s Place Hotel In Rome übernachten ?\n",
            "\n",
            "\t EN (TRUE):When would you like to stay at the Leon &apos;s Place Hotel In Rome ?\n",
            "\n",
            "\n",
            "\t EN (Predicted): When would you like to stay at the <unk> Hotel ? </s> \n",
            "\n",
            "DE:  Jedes Stück Information kann eigene Eigenschaften und Aktionen besitzen .\n",
            "\n",
            "\t EN (TRUE):Every bit of information and code can be given their own properties and actions .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> can be used to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be \n",
            "\n",
            "DE:  Die Lizenzgeberin haftet auch nach den gesetzlichen Bestimmungen , sofern als Folge eines von ihr zu vertretenden Lieferverzuges der / die Lizenznehmer / in berechtigt ist , geltend zu machen , dass sein Interesse an der weiteren Vertragserfüllung in Fortfall geraten ist .\n",
            "\n",
            "\t EN (TRUE):For any discounts the amount of the bill needs to have been paid in full on the account of the licensor .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  &quot; Die Letzte Droge &quot; wird , wie auch Route 66 , unter einer Creative Commons ##AT##-##AT## Lizenz veröffentlicht - Kopieren , Aufführen und Verändern ist diesmal auch zu kommerziellen Zwecken gestattet und erwünscht !\n",
            "\n",
            "\t EN (TRUE):We will release The Last Drug under a Creative Commons BY SA License , making it the first free HD feature film . All footage , project files , sounds and special effects will be available for those of you that are eager to get hands on experience on the first Open Source feature film project ever or for those that are able to turn it into something different .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a bit ##AT##-##AT## bit , but the <unk> is the <unk> of the <unk> , and the <unk> of the <unk> , the <unk> <unk> , and the <unk> of the <unk> . </s> \n",
            "\n",
            "DE:  Einige der ursprünglichen Charakteristika des Gebäudes - wie beispielsweise die einzigartige denkmalgeschützte Fassade und die bezaubernde Innenausstattung der Bar ##AT##-##AT## Bodega De Blauwe Parde - wurden bis heute bewahrt .\n",
            "\n",
            "\t EN (TRUE):Some unchanged features include the unique frontage , which has listed building status , and also the unique interior of De Blauwe Parade bar ##AT##-##AT## bodega .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> in the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> <unk> , the <unk> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.11759745957804822\n",
            "=====================================================\n",
            ".....Step  8500\n",
            "Actual: Keep my commandments , and assist to bring forth my c work , d according to my commandments , and you shall be blessed . </s> \n",
            "\n",
            "Predicted: <unk> out a , and I the my me the a <unk> , and <unk> to the a , and I shall be b . </s> \n",
            "(Train) BLEU (450 elements):  0.3198134287284041\n",
            ".....Step  9000\n",
            "Actual: Visit the hotel ’ s spa , where you find a swimming pool complete with bar , whirlpool , sauna , steam bath and solarium . </s> \n",
            "\n",
            "Predicted: The the hotel ’ s restaurant facilities the you can a delicious pool , with a , a and and and steam and , a . </s> \n",
            "(Train) BLEU (580 elements):  0.31803690177796734\n",
            "============= Step  9000  =============\n",
            "\t Loss:  0.8650333119034768\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Zum klimatisierten Hotel gehören auch ein Whirpool und eine traumhafte Sonnenterrasse .\n",
            "\n",
            "\t EN (TRUE):Apart from this , the guests can enjoy the facility of an independent air ##AT##-##AT## conditioning system , a jacuzzi and a beautiful sun terrace .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a small hotel , a hotel with a large garden and a large garden . </s> \n",
            "\n",
            "DE:  Das Hotel Opera befindet sich in der Nähe des Royal Theatre , Kongens Nytorv , &apos; Stroget &apos; und Nyhavn .\n",
            "\n",
            "\t EN (TRUE):Hotel Opera is situated near The Royal Theatre , Kongens Nytorv , &quot; Strøget &quot; and fascinating Nyhavn .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> Hotel is a small hotel , the hotel is located in the heart of the city . </s> \n",
            "\n",
            "DE:  Residenz City Lodge befindet sich am mandelförmigen Sweelinckplein im Herzen des schicken Duinoord ##AT##-##AT## Viertels in Den Haag .\n",
            "\n",
            "\t EN (TRUE):In the midst of the bustling city life , an oasis of peace and luxury can be found bordering the beautiful Haagsche Park , across from the main train station .\n",
            "\n",
            "\n",
            "\t EN (Predicted): Situated in the heart of the city , the hotel is located in the heart of the city , the hotel is a perfect place for exploring the city . </s> \n",
            "\n",
            "DE:  Eine Woche später wird Dianne Feinstein , Vorstandsvorsitzende der Inspektoren , als Nachfolgerin Moscones ernannt . Sie ist die erste Bürgermeisterin der Stadt .\n",
            "\n",
            "\t EN (TRUE):Returning by the parallel Stockton or Powell will give you a better feeling of the day to day life of the residents , and are both good for those looking for imported commodities such as tea or herbs .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  aufgerufen wird , fügt Sie die Flash Nachricht &quot; Eintrag gespeichert !\n",
            "\n",
            "\t EN (TRUE):is called , it adds the flash message &quot; Record Saved !\n",
            "\n",
            "\n",
            "\t EN (Predicted): modified is a list of the <unk> <unk> ? </s> \n",
            "\n",
            "DE:  Die Bewohner des Nordens sind ein buntes Völkergemisch aus den verschiedensten Bergstämmen und den Nord ##AT##-##AT## Thais oder kon mueang ; die traditionell in den fruchtbaren Tiefebenen Nordthailands siedeln . In vielerlei Hinsicht halten sich die Nord Thais für die &quot; wahren &quot; Thais , die die Thai ##AT##-##AT## Kultur noch am besten über die Zeit gerettet haben .\n",
            "\n",
            "\t EN (TRUE):From Pratu Chiang Mai market , songthaews also travel to Hang Dong ( 20 baht ) and San Patong , south ##AT##-##AT## west of Chiang Mai .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a <unk> , and the <unk> of the <unk> , the <unk> , the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  Bei der Installation von Adobe Presenter 6 wird das ältere Programm Breeze Presenter 5.1 deinstalliert .\n",
            "\n",
            "\t EN (TRUE):Installing Adobe Presenter 6 will uninstall the earlier Breeze Presenter 5.1 .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Kosten Sie mediterrane Gerichte im preisgekrönten Restaurant Molyvos .\n",
            "\n",
            "\t EN (TRUE):Enjoy award winning Mediterranean cuisine at Molyvos .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> is a perfect place for exploring the hotel . </s> \n",
            "\n",
            "DE:  Zimmerbeschreibung : Our Castle Deluxe Rooms are traditionally themed with rich luxurious fabrics and furnishings , many with excellent views over the Castle grounds .\n",
            "\n",
            "\t EN (TRUE):Room Notes : Our Castle Deluxe Rooms are traditionally themed with rich luxurious fabrics and furnishings , many with excellent views over the Castle grounds .\n",
            "\n",
            "\n",
            "\t EN (Predicted): Description : We are a total of the total of the room and a total of the room stay in the hotel with a total of 120 people . </s> \n",
            "\n",
            "DE:  Die Musikant entspricht dem Folkrock . Dennoch finden sich in den Liedern viele musikalische Elemente aus klassisch- folklorischer Liedern aus Anatolien bis zum Mittelmeer , aus Latein- amerikanischen Märschen bis hin zu klassischen Rockklängen .\n",
            "\n",
            "\t EN (TRUE):Yorum continues to sing hopeful songs in the name of all the world &apos;s revolutionary music .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> is a <unk> ##AT##-##AT## <unk> , which is the most important of the world &apos;s <unk> , the <unk> , the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "(Test) BLEU (100 elements):  0.16686225573885027\n",
            "=====================================================\n",
            ".....Step  9500\n",
            "Actual: <unk> a <unk> de <unk> <unk> , <unk> a <unk> . <unk> , no dia anterior , para me <unk> uma lunch box <unk> <unk> q <unk> <unk> <unk> e a <unk> hora n a <unk> <unk> e o <unk> <unk> q <unk> tomar o <unk> <unk> q <unk> de <unk> . </s> \n",
            "\n",
            "Predicted: I <unk> <unk> <unk> <unk> <unk> . <unk> <unk> <unk> . </s> \n",
            "(Train) BLEU (510 elements):  0.3215774531600357\n",
            ".....Step  10000\n",
            "Actual: From Plaza Isabel La <unk> you can easily reach the Town Hall , the Cathedral and Royal Chapel , and the business and commercial areas of the city . </s> \n",
            "\n",
            "Predicted: The the de , <unk> , will enjoy reach the city of , the <unk> , the <unk> . the the <unk> centre <unk> centres . the city . </s> \n",
            "(Train) BLEU (550 elements):  0.3152351893554751\n",
            "============= Step  10000  =============\n",
            "\t Loss:  0.8593881290853024\n",
            "=====================================================\n",
            "(Test) Translating test sentences ...\n",
            "Processing test data ... \n",
            "DE:  Ideale Lage für Exkursionen in die Stadt und Nähe zur Promenade .\n",
            "\n",
            "\t EN (TRUE):There was plenty of space in the room and a nice garden to sit and have a drink and smoke .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a small city , the hotel is located in the heart of the city . </s> \n",
            "\n",
            "DE:  Dieser bietet doppelten Schutz durch den OnExecution Scan , der Programme noch bevor Sie gestartet werden mit dem Signaturenscanner überprüft , sowie dem Malware ##AT##-##AT## IDS .\n",
            "\n",
            "\t EN (TRUE):It includes the double protection using the OnExecution Scan , which scans programs right before they are started with the signature scanner , as well as the Malware ##AT##-##AT## IDS .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Residenz City Lodge befindet sich am mandelförmigen Sweelinckplein im Herzen des schicken Duinoord ##AT##-##AT## Viertels in Den Haag .\n",
            "\n",
            "\t EN (TRUE):In the midst of the bustling city life , an oasis of peace and luxury can be found bordering the beautiful Haagsche Park , across from the main train station .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a new building in the centre of the city , the hotel is located in the heart of the city . </s> \n",
            "\n",
            "DE:  15. einem Dritten bei dem Verstoss gegen eine dieser Regeln zu helfen .\n",
            "\n",
            "\t EN (TRUE):15. assist any third party in engaging in any activity prohibited by these Terms .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Zitate mit unterschiedlichsten stilistischen Effekten treffen aufeinander : Referenzen auf das narrative Autorenkino ( Hitchcock , Eisenstein , Godard , Brian De Palma ) , poetische oder theoretische Texte ( Tschechow , Duras , Barthes , Žižek , Weibel , Gržinić ) und Verweise auf Massenmedien – B ##AT##-##AT## Filme , TV ##AT##-##AT## Shows , Werbespots , politische Nachrichtensendungen .\n",
            "\n",
            "\t EN (TRUE):Out of all this material , mixed with disnarrative polysemy and an astonishing lack of inhibition , strange “ fictions ” are reconstructed — fragmentary fictions that are constantly interrupted .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Slimline ICE ist in einer Vielzahl von Geschmacksrichtungen sowohl als Eis am Stiel als auch im Becher erhältlich .\n",
            "\n",
            "\t EN (TRUE):Palatinose ™ is a disaccharide derived from beet sugar .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n",
            "\n",
            "DE:  Jedes Stück Information kann eigene Eigenschaften und Aktionen besitzen .\n",
            "\n",
            "\t EN (TRUE):Every bit of information and code can be given their own properties and actions .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> is a new <unk> , which is the ideal for the <unk> . </s> \n",
            "\n",
            "DE:  Das Athens Gate Hotel liegt unterhalb der Akropolis nur 100 m vom neuen Akropolis ##AT##-##AT## Museum entfernt .\n",
            "\n",
            "\t EN (TRUE):The Athens Gate Hotel rests under the Acropolis , just 100 metres from the new Acropolis museum .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . </s> \n",
            "\n",
            "DE:  Softwaretools wie zum Beispiel der NI Analog Waveform Editor , das NI Modulation Toolkit und LabVIEW helfen Anwendern , die Entwicklungszeit von Prüfsystemen zu verringern und gleichzeitig flexibel auf die sich ändernden Anwendungsanforderungen zu reagieren .\n",
            "\n",
            "\t EN (TRUE):Software tools such as the NI Analog Waveform Editor , Modulation Toolkit and LabVIEW , help reduce your test system development time while also having the flexibility to meet your changing application requirements .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> ##AT##-##AT## <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "DE:  William Gross beschreibt die Bedeutung des Manuskripts für die Sammlung der Familie Gross .\n",
            "\n",
            "\t EN (TRUE):William Gross describes the significance of the manuscript to the Gross Family Collection .\n",
            "\n",
            "\n",
            "\t EN (Predicted): <unk> <unk> is a <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , \n",
            "\n",
            "(Test) BLEU (100 elements):  0.11506212922485967\n",
            "=====================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}