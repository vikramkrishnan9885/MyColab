{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2VecEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwUCpIZ9hCQXA05KogLB/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikramkrishnan9885/MyColab/blob/master/Word2VecEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RkiPruBwvk8",
        "colab_type": "text"
      },
      "source": [
        "# WordNet - old fashioned NLP\n",
        "\n",
        "* These approaches mainly can be categorized into two classes:\n",
        "  * approaches that use external resources for representing words and \n",
        "  * approaches that do not. \n",
        "* Example of first is  WordNet — one of the most popular external resource-based approaches for representing words. \n",
        "* Then we will proceed to more localized methods (that is, those that do not rely on external resources), such as **one-hot encoding** and **Term Frequency-Inverse Document Frequency (TF-IDF)**.\n",
        "\n",
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR56VMQsA3ke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "8c549c85-56e3-4005-97cf-acb75c218cd7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZb9cp_gBLJm",
        "colab_type": "text"
      },
      "source": [
        "# WordNet – using an external lexical knowledge base for learning word representations\n",
        "\n",
        "* WordNet is one of the most popular classical approaches or statistical NLP that deals with word representations. \n",
        "* It relies on an external lexical knowledge base that encodes the information about the definition, synonyms, ancestors, descendants, and so forth of a given word.\n",
        "* First, WordNet uses the term synset to denote a group or set of synonyms. \n",
        "* Next, each synset has a definition that explains what the synset represents. \n",
        "* Synonyms contained within a synset are called lemmas.\n",
        "* In WordNet, the word representations are modeled hierarchically, which forms a\n",
        "complex graph between a given synset and the associations to another synset.\n",
        "* These associations can be of two different categories: an is-a relationship or an is-made-of relationship. \n",
        "* First, we will discuss the is-a association.\n",
        "* For a given synset, there exist two categories of relations: \n",
        "  * hypernyms and \n",
        "  * hyponyms.\n",
        "* Hypernyms of a synset are the synsets that carry a general (high-level) meaning of he considered synset. For example, vehicle is a hypernym of the synset car. \n",
        "* Next, hyponyms are synsets that are more specific than the corresponding synset. For example, Toyota car is a hyponym of the synset car. \n",
        "* Now let's discuss the is-made-of relationships for a synset. \n",
        "  * Holonyms of a synset are the group of synsets that represents the whole entity of the considered synset. For example, a holonym of tires is the cars synset. \n",
        "  * Meronyms are an is-made-of category and represent the opposite of holonyms, where meronyms are the parts or substances synset that makes the corresponding synset.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPXh-l4nEmmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "928d4005-3be9-4dc2-d3db-a3f9b5f5040f"
      },
      "source": [
        "word = \"car\"\n",
        "car_syns = wn.synsets(word)\n",
        "car_syns"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('car.n.01'),\n",
              " Synset('car.n.02'),\n",
              " Synset('car.n.03'),\n",
              " Synset('car.n.04'),\n",
              " Synset('cable_car.n.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iiFR3lmEyOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "168193fa-dfdd-4018-ab11-28cf2ad1d737"
      },
      "source": [
        "# The definition of the first two synsets\n",
        "syns_defs = [car_syns[i].definition() for i in range(len(car_syns))]\n",
        "for i in range(len(car_syns)):\n",
        "    print(car_syns[i].name(),': ',syns_defs[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car.n.01 :  a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
            "car.n.02 :  a wheeled vehicle adapted to the rails of railroad\n",
            "car.n.03 :  the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
            "car.n.04 :  where passengers ride up and down\n",
            "cable_car.n.01 :  a conveyance for passengers or freight on a cable railway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoK8cBdAIvQo",
        "colab_type": "text"
      },
      "source": [
        "Lemmas are root forms of words. Consider the verb fly. It can be inflected into many different words—flow, flew, flies, flown, flowing, and so on—and fly is the lemma for all of these seemingly different words. Sometimes, it might be useful to reduce the tokens to their lemmas to keep the dimensionality of the vector representation low. This reduction is called lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4wn4eWjFMkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "e17def7f-3b4e-41d1-c6b8-769b18134420"
      },
      "source": [
        "for i in range(len(car_syns)):\n",
        "  print(car_syns[i].name(),\": \", car_syns[i].lemmas())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car.n.01 :  [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
            "car.n.02 :  [Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
            "car.n.03 :  [Lemma('car.n.03.car'), Lemma('car.n.03.gondola')]\n",
            "car.n.04 :  [Lemma('car.n.04.car'), Lemma('car.n.04.elevator_car')]\n",
            "cable_car.n.01 :  [Lemma('cable_car.n.01.cable_car'), Lemma('cable_car.n.01.car')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_MPJxsFkbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "806ab9a6-42f1-4a79-9ed1-b08a09803544"
      },
      "source": [
        "# Lemmas is a method\n",
        "for i in range(len(car_syns[0].lemmas())):\n",
        "  #print(car_syns[0].lemmas()[i])\n",
        "  print(car_syns[0].lemmas()[i].name())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car\n",
            "auto\n",
            "automobile\n",
            "machine\n",
            "motorcar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdS6YEHZH-DT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "bf4e1ecc-82c1-4b67-f812-8a02c892fec0"
      },
      "source": [
        "for i in range(len(car_syns)):\n",
        "  print(car_syns[i].name(),\": \", car_syns[i].hypernyms())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car.n.01 :  [Synset('motor_vehicle.n.01')]\n",
            "car.n.02 :  [Synset('wheeled_vehicle.n.01')]\n",
            "car.n.03 :  [Synset('compartment.n.02')]\n",
            "car.n.04 :  [Synset('compartment.n.02')]\n",
            "cable_car.n.01 :  [Synset('compartment.n.02')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7voxgK7Hb6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "d096535e-15c0-4ab7-9c8b-aa22a3992faa"
      },
      "source": [
        "# Let us get hypernyms for a Synset (general superclass)\n",
        "syn = car_syns[0]\n",
        "print('Hypernyms of the Synset ',syn.name())\n",
        "print('\\t',syn.hypernyms()[0].name())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hypernyms of the Synset  car.n.01\n",
            "\t motor_vehicle.n.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fabMF7sNHu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "81cfd9f1-741b-48c9-8bc0-36b2890011dd"
      },
      "source": [
        "for i in range(len(car_syns)):\n",
        "  print(car_syns[i].name(),\": \", car_syns[i].hyponyms())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car.n.01 :  [Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03'), Synset('convertible.n.01'), Synset('coupe.n.01'), Synset('cruiser.n.01'), Synset('electric.n.01'), Synset('gas_guzzler.n.01'), Synset('hardtop.n.01'), Synset('hatchback.n.01'), Synset('horseless_carriage.n.01'), Synset('hot_rod.n.01'), Synset('jeep.n.01'), Synset('limousine.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('minivan.n.01'), Synset('model_t.n.01'), Synset('pace_car.n.01'), Synset('racer.n.02'), Synset('roadster.n.01'), Synset('sedan.n.01'), Synset('sport_utility.n.01'), Synset('sports_car.n.01'), Synset('stanley_steamer.n.01'), Synset('stock_car.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('used-car.n.01')]\n",
            "car.n.02 :  [Synset('baggage_car.n.01'), Synset('cabin_car.n.01'), Synset('club_car.n.01'), Synset('freight_car.n.01'), Synset('guard's_van.n.01'), Synset('handcar.n.01'), Synset('mail_car.n.01'), Synset('passenger_car.n.01'), Synset('slip_coach.n.01'), Synset('tender.n.04'), Synset('van.n.03')]\n",
            "car.n.03 :  []\n",
            "car.n.04 :  []\n",
            "cable_car.n.01 :  []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV-yQE_gNNWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "557c0dfc-a451-4fa1-f6ab-30fc49079df9"
      },
      "source": [
        "# Let us get hyponyms for a Synset (specific subclass)\n",
        "syn = car_syns[0]\n",
        "print('Hyponyms of the Synset ',syn.name())\n",
        "print('\\t',[hypo.name() for hypo in syn.hyponyms()[:3]],'\\n')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyponyms of the Synset  car.n.01\n",
            "\t ['ambulance.n.01', 'beach_wagon.n.01', 'bus.n.04'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfvkQejpR_-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "30bc3d77-5bd7-4ab5-e5f1-1d01c2d671fc"
      },
      "source": [
        "# Let's get part-holonyms for the third \"car\"\n",
        "# Synset (specific subclass)\n",
        "syn = car_syns[2]\n",
        "print('\\t',[holo.name() for holo in syn.part_holonyms()],'\\n')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t ['airship.n.01'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o29hCs2Scvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "53be348a-fda8-4c8b-dcb5-90376105272e"
      },
      "source": [
        "# Let us get meronyms for a Synset (specific subclass)\n",
        "# also there is another meronym category called \"substance-meronyms\"\n",
        "syn = car_syns[0]\n",
        "print('Meronyms (Part) of the Synset ',syn.name())\n",
        "print('\\t',[mero.name() for mero in syn.part_meronyms()[:3]],'\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Meronyms (Part) of the Synset  car.n.01\n",
            "\t ['accelerator.n.01', 'air_bag.n.01', 'auto_accessory.n.01'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hquABYBYhsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "1317651d-d265-4b64-aaf4-976d1ee78f43"
      },
      "source": [
        "word1, word2, word3 = 'car','lorry','tree'\n",
        "w1_syns, w2_syns, w3_syns = wn.synsets(word1), wn.synsets(word2), wn.synsets(word3)\n",
        "\n",
        "print('Word Similarity (%s)<->(%s): '%(word1,word2),wn.wup_similarity(w1_syns[0], w2_syns[0]))\n",
        "print('Word Similarity (%s)<->(%s): '%(word1,word3),wn.wup_similarity(w1_syns[0], w3_syns[0]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Similarity (car)<->(lorry):  0.6956521739130435\n",
            "Word Similarity (car)<->(tree):  0.38095238095238093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbRDbYwWSfOH",
        "colab_type": "text"
      },
      "source": [
        "Though WordNet is an amazing resource that anyone can use to learn meanings ofword in the NLP tasks, there are quite a few drawbacks in using WordNet for this.They are as follows:\n",
        " * Missing nuances is a key problem in WordNet. There are both theoreticaland practical reasons why this is not viable for WordNet. From a theoreticalperspective, it is not well-posed or direct to model the definition of the subtledifference between two entities. Practically speaking, defining nuances issubjective. For example, the words want and need have similar meanings, butone of them (need) is more assertive. This is considered to be a nuance.\n",
        " * Next, WordNet is subjective in itself as WordNet was designed by arelatively small community. Therefore, depending on what you are tryingto solve, WordNet might be suitable or you might be able to perform betterwith a loose definition of words.\n",
        " * There also exists the issue of maintaining WordNet, which is labor-intensive.Maintaining and adding new synsets, definitions, lemmas, and so on, can bevery expensive. This adversely affects the scalability of WordNet, as humanlabor is essential to keep WordNet up to date.\n",
        " * Developing WordNet for other languages can be costly. There are also someefforts to build WordNet for other languages and link it with the EnglishWordNet as MultiWordNet (MWN), but they are yet incomplete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "653YMMzrS3HX",
        "colab_type": "text"
      },
      "source": [
        "# Next, we will discuss several word representation techniques that do not rely on external resources\n",
        "\n",
        "## One-hot encoded representation\n",
        "\n",
        "This means that if we have a vocabulary of $V$ size, for each $i_{th}$ word $w_i$, we will represent the word $w_i$ with a $V$-long vector $[0, 0, 0, ..., 0, 1, 0, ..., 0, 0, 0]$ where the $i_{th}$ element is 1 and other elements are zero\n",
        "\n",
        "This representation does not encode the similarity between words in any way and\n",
        "completely ignores the context in which the words are used. \n",
        "\n",
        "This method becomes extremely ineffective for large vocabularies. \n",
        "\n",
        "However, one-hot encoding plays an important role even in the state-of-the-art\n",
        "word embedding learning algorithms. We use one-hot encoding to represent words\n",
        "numerically and feed them into neural networks so that the neural networks can\n",
        "learn better and smaller numerical feature representations of the words.\n",
        "\n",
        "## The TF-IDF method\n",
        "TF-IDF is a frequency-based method that takes into account the frequency with which a word appears in a corpus. This is a word representation in the sense that it represents the importance of a specific word in a given document.\n",
        "\n",
        "Intuitively, the higher the frequency of the word, the more important that word is in the document.\n",
        "\n",
        "For example, in a document about cats, the word cats will appear more. However, just calculating the frequency would not work, because words such as this and is are very frequent but do not carry that much information. TF-IDF takes this into consideration and gives a value of zero for such common words. Again, TF stands for term frequency and IDF stands for inverse document frequency\n",
        "\n",
        "## Co-occurrence matrix\n",
        "Co-occurrence matrices, unlike one-hot-encoded representation, encodes the context information of words, but requires maintaining a $V \\times V$ matrix\n",
        "\n",
        "However, it is not hard to see that maintaining such a co-occurrence matrix comes at a cost as the size of the matrix grows polynomially with the size of the vocabulary. Furthermore, it is not straightforward to incorporate a context window size larger than 1. One option is to have a weighted count, where the weight for a word in the context deteriorates with the distance from the word of interest.\n",
        "\n",
        "\n",
        "# Word2Vec\n",
        "\n",
        "Word2vec is a recently-introduced distributed word representation learning technique that is currently being used as a __feature engineering technique__ for many NLP tasks (for example, machine translation, chatbots, and image caption generators).\n",
        "\n",
        "Essentially, Word2vec learns word representations by looking at the surrounding words (that is, context) in which the word is used. More specifically, we attempt to predict the context, given some words (or vice versa), through a neural network, which leads the neural network to be forced to learn good word embeddings.\n",
        "\n",
        "The Word2vec approach has many advantages over the previously-described methods:\n",
        "* The Word2vec approach is not subjective to the human knowledge of language as in the WordNet-based approach.\n",
        "* Word2vec representation vector size is independent of the vocabulary size unlike one-hot encoded representation or the word co-occurrence matrix.\n",
        "*  Word2vec is a distributed representation. Unlike localist representation, where the representation depends on the activation of a single element of the representation vector (for example, one-hot encoding), the distributed representation depends on the activation pattern of all the elements in the vector. This gives more expressive power to Word2vec than produced by the one-hot encoded representation.\n",
        "\n",
        "We will discuss two Word2vec algorithms:\n",
        "*  the skip-gram and \n",
        "* Continuous Bag-of-Words (CBOW) algorithms.\n",
        "\n",
        "## The skip-gram algorithm\n",
        "\n",
        "### From raw text to structured data\n",
        "First, we need to design a mechanism to extract a dataset that can be fed to our learning model. Such a dataset should be a set of tuples of the format (input, output). \n",
        "\n",
        "The data preparation process should do the following:\n",
        "* Capture the surrounding words of a given word\n",
        "* Perform in an unsupervised manner\n",
        "\n",
        "Once the data is in the (input, output) format, we can use a neural network to learn the word embeddings. First, let's identify the variables we need to learn the word embeddings. To store the word embeddings, we need a $V \\times D$ matrix, where $V$ is the vocabulary size and $D$ is the dimensionality of the word embeddings (that is, the number of elements in the vector that represents a single word). \n",
        "\n",
        "D is a user-defined hyperparameter. The higher D is, the more expressive the word embeddings learned will be. This matrix will be referred to as the embedding space or the embedding layer.\n",
        "\n",
        "Next, we have a softmax layer with weights of size $D \\times V$, a bias of size V. \n",
        "\n",
        "Each word will be represented as a one-hot encoded vector of size $V$ with one\n",
        "element being 1 and all the others being 0. Therefore, an input word and the\n",
        "corresponding output words would each be of size $V$. \n",
        "\n",
        "Let's refer to the $i_{th}$ input as $x_i$, the corresponding embedding of $x_i$ as $z_i$, and the corresponding output as $y_i$.\n",
        "\n",
        "At this point, we have the necessary variables defined. \n",
        "\n",
        "Next, for each input $x_i$, we will look up the embedding vectors from the embedding layer corresponding to the input. This operation provides us with $z_i$, which is a $D$-sized vector (that is, a D-long embedding vector). Afterwards, we calculate the prediction output for $x_i$ using the\n",
        "following transformation:\n",
        "$$\n",
        "\\text{logit}(x_i) = z_i W+b\n",
        "$$\n",
        "$$\n",
        "\\hat{y}_i= \\text{softmax}(\\text{logit}(x_i))\n",
        "$$\n",
        "\n",
        "### Loss func\n",
        "The objective of this loss function from a practical perspective, we want to maximize the probability of predicting a contextual word given a word, while minimizing the probability of \"all\" the noncontextual words, given a word. \n",
        "\n",
        "#### Efficiently approximating the loss function\n",
        "If we try to calculate the loss function in closed form, we will face an inevitable tremendous slowness of our algorithm.\n",
        "\n",
        "We will discuss two popular choices of approximations:\n",
        "* Negative sampling\n",
        "* Hierarchical softmax\n",
        "\n",
        "\n",
        " __Negative sampling of the softmax layer__\n",
        "\n",
        "Here we will discuss our first approach: negative sampling the softmax layer.\n",
        "Negative sampling is an approximation of the Noise-Contrastive Estimation (NCE)\n",
        "method. NCE says that a good model should differentiate data from noise by means\n",
        "of logistic regression.\n",
        "\n",
        "__Hierarchical softmax__\n",
        "\n",
        "Hierarchical softmax is slightly more complex than negative sampling, but serves the same objective as the negative sampling; that is, approximating the softmax without having to calculate activations for all the words in the vocabulary for all the training samples. \n",
        "\n",
        "However, unlike negative sampling, hierarchical softmax uses only the actual data and does not need noise samples\n",
        "\n",
        "_Learning the hierarchy_\n",
        "\n",
        "Though hierarchical softmax is efficient, an important question remains unanswered.\n",
        "How do we determine the decomposition of the tree? More precisely, which word will follow which branch? There are a few options to achieve this:\n",
        "\n",
        "* __Initialize the hierarchy randomly__: This method does have some performance degradations as the random placement cannot be guaranteed to have the best branching possible among words.\n",
        "* __Use WordNet to determine the hierarchy__: WordNet can be utilized to determine a suitable order for the words in the tree. This method has shown to perform significantly better than the random initialization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TT_nLPgthaC",
        "colab_type": "text"
      },
      "source": [
        "### Code\n",
        "\n",
        "#### Data Preproc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlbpONKhvOoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "cdcaa45e-075f-4652-8fbf-336604de6612"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T0yrV4_ttk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOAD DATA\n",
        "\n",
        "def download_data(url, filename):\n",
        "  if not os.path.exists(filename):\n",
        "    print('Downloading file:','\\t',url)\n",
        "    filename, _ = urllib.request.urlretrieve(url,filename)\n",
        "  else:\n",
        "    raise Exception(\"FILE ALREADY EXISTS!\")\n",
        "  return filename"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLoWW_RZu6Fd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "1b5dbdf4-44e8-4150-d42a-ed20afb157dc"
      },
      "source": [
        "# https://www.evanjones.ca/software/wikipedia2text-extracted.txt.bz2 is giving 404\n",
        "# https://github.com/amolnayak311/nlp-with-tensorflow/blob/master/wikipedia2text-extracted.txt.bz2?raw=true is used instead\n",
        "\n",
        "url = 'https://github.com/amolnayak311/nlp-with-tensorflow/blob/master/wikipedia2text-extracted.txt.bz2?raw=true'\n",
        "filename = 'wikipedia2text-extracted.txt.bz2'\n",
        "filename = download_data(url,filename)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading file: \t https://github.com/amolnayak311/nlp-with-tensorflow/blob/master/wikipedia2text-extracted.txt.bz2?raw=true\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu_1NAWuy1b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bz2\n",
        "import math"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXeEAjlHxvFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# READ DATA WITH PREPROCESSING WITH NLTK\n",
        "#\n",
        "# Reads data as it is to a string, \n",
        "# convert to lower-case and \n",
        "# tokenize it using the nltk library. \n",
        "#\n",
        "# This code reads data in 1MB portions as processing the full text at once \n",
        "# slows down the task and returns a list of words. \n",
        "# You will have to download the necessary tokenizer.\n",
        "\n",
        "def read_data(filename):\n",
        "  \"\"\"\n",
        "  Extract the first file enclosed in a zip file as a list of words\n",
        "  and pre-processes it using the nltk python library\n",
        "  \"\"\"\n",
        "\n",
        "  with bz2.BZ2File(filename) as f:\n",
        "\n",
        "    data = []\n",
        "\n",
        "    file_size = os.stat(filename).st_size\n",
        "    # reading 1 MB at a time as the dataset is moderately large\n",
        "    chunk_size = 1024 * 1024 \n",
        "    print('Reading data...')\n",
        "    \n",
        "    for i in range(math.ceil(file_size//chunk_size)+1):\n",
        "      bytes_to_read = min(chunk_size,file_size-(i*chunk_size))\n",
        "      file_string = f.read(bytes_to_read).decode('utf-8')\n",
        "      file_string = file_string.lower()\n",
        "      \n",
        "      # tokenizes a string to words residing in a list\n",
        "      file_string = nltk.word_tokenize(file_string)\n",
        "      data.extend(file_string)\n",
        "  return data"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq0B-IckyvbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "b7047de2-8618-498b-dbef-6cfee2395991"
      },
      "source": [
        "words = read_data(filename)\n",
        "print('Data size %d' % len(words))\n",
        "print('Example words (start): ',words[:10])\n",
        "print('Example words (end): ',words[-10:])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data...\n",
            "Data size 3360286\n",
            "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
            "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0_KHJldz6sr",
        "colab_type": "text"
      },
      "source": [
        "#### Building the Dictionaries\n",
        "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
        "* dictionary: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
        "* reverse_dictionary: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
        "* count: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
        "* data : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
        "\n",
        "It also introduces an additional special token UNK to denote rare words to are too rare to make use of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJMiVjCT0FyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}